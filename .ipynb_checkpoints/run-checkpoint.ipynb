{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "560f2eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f58b0557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-1\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2672/8305 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/P-1/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0075e+00 (1.0075e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4822e-01 (6.2450e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2137e-01 (4.2430e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2198e-02 (3.1035e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4688e-02 (2.4069e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0031e-02 (1.9558e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0053e-02 (1.0053e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0007e-02 (1.0036e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0030e-02 (1.0034e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0022e-02 (1.0031e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0008e-02 (1.0033e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0009e-02 (1.0031e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0015e-02 (1.0015e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0008e-02 (1.0014e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0005e-02 (1.0014e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0006e-02 (1.0014e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0006e-02 (1.0014e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0004e-02 (1.0015e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0012e-02 (1.0012e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0009e-02 (1.0010e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0009e-02 (1.0010e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0009e-02 (1.0010e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0002e-02 (1.0009e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0002e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0011e-02 (1.0007e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0003e-02 (1.0006e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0010e-02 (1.0006e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0002e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0005e-02 (1.0006e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0008e-02 (1.0005e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0009e-02 (1.0005e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0006e-02 (1.0004e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0000e-02 (1.0003e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0006e-02 (1.0002e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0005e-02 (1.0002e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0005e-02 (1.0002e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0000e-02 (1.0008e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [16][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0006e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][40/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [30][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/167]\n",
      "Fill TS Repository [100/167]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries P-1\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5344 - Test samples size: 8305\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/P-1/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/106]\tTotal Loss -2.3010e+00 (-2.3010e+00)\tConsistency Loss 2.2589e+00 (2.2589e+00)\tInconsistency Loss 1.1027e-01 (1.1027e-01)\tEntropy 2.2800e+00 (2.2800e+00)\n",
      "Epoch: [1][100/106]\tTotal Loss -2.3998e+00 (-2.3529e+00)\tConsistency Loss 1.7343e+00 (1.8733e+00)\tInconsistency Loss 1.9491e-01 (1.7308e-01)\tEntropy 2.0671e+00 (2.1131e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/106]\tTotal Loss -2.3798e+00 (-2.3798e+00)\tConsistency Loss 1.7291e+00 (1.7291e+00)\tInconsistency Loss 1.9321e-01 (1.9321e-01)\tEntropy 2.0544e+00 (2.0544e+00)\n",
      "Epoch: [2][100/106]\tTotal Loss -2.3702e+00 (-2.3752e+00)\tConsistency Loss 1.7092e+00 (1.7268e+00)\tInconsistency Loss 2.0452e-01 (1.9594e-01)\tEntropy 2.0397e+00 (2.0510e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/106]\tTotal Loss -2.3858e+00 (-2.3858e+00)\tConsistency Loss 1.7286e+00 (1.7286e+00)\tInconsistency Loss 1.9543e-01 (1.9543e-01)\tEntropy 2.0572e+00 (2.0572e+00)\n",
      "Epoch: [3][100/106]\tTotal Loss -2.3747e+00 (-2.3770e+00)\tConsistency Loss 1.7311e+00 (1.7196e+00)\tInconsistency Loss 1.9661e-01 (1.9742e-01)\tEntropy 2.0529e+00 (2.0483e+00)\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/106]\tTotal Loss -2.3749e+00 (-2.3749e+00)\tConsistency Loss 1.7363e+00 (1.7363e+00)\tInconsistency Loss 1.9365e-01 (1.9365e-01)\tEntropy 2.0556e+00 (2.0556e+00)\n",
      "Epoch: [4][100/106]\tTotal Loss -2.3804e+00 (-2.3767e+00)\tConsistency Loss 1.7340e+00 (1.7194e+00)\tInconsistency Loss 1.9500e-01 (1.9751e-01)\tEntropy 2.0572e+00 (2.0480e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/106]\tTotal Loss -2.3767e+00 (-2.3767e+00)\tConsistency Loss 1.7110e+00 (1.7110e+00)\tInconsistency Loss 1.9939e-01 (1.9939e-01)\tEntropy 2.0439e+00 (2.0439e+00)\n",
      "Epoch: [5][100/106]\tTotal Loss -2.3788e+00 (-2.3766e+00)\tConsistency Loss 1.7137e+00 (1.7199e+00)\tInconsistency Loss 1.9831e-01 (1.9743e-01)\tEntropy 2.0462e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/106]\tTotal Loss -2.3792e+00 (-2.3792e+00)\tConsistency Loss 1.7199e+00 (1.7199e+00)\tInconsistency Loss 1.9744e-01 (1.9744e-01)\tEntropy 2.0496e+00 (2.0496e+00)\n",
      "Epoch: [6][100/106]\tTotal Loss -2.3739e+00 (-2.3771e+00)\tConsistency Loss 1.7132e+00 (1.7184e+00)\tInconsistency Loss 1.9859e-01 (1.9770e-01)\tEntropy 2.0435e+00 (2.0478e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/106]\tTotal Loss -2.3801e+00 (-2.3801e+00)\tConsistency Loss 1.7246e+00 (1.7246e+00)\tInconsistency Loss 1.9562e-01 (1.9562e-01)\tEntropy 2.0524e+00 (2.0524e+00)\n",
      "Epoch: [7][100/106]\tTotal Loss -2.3781e+00 (-2.3769e+00)\tConsistency Loss 1.7266e+00 (1.7195e+00)\tInconsistency Loss 1.9535e-01 (1.9737e-01)\tEntropy 2.0524e+00 (2.0482e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/106]\tTotal Loss -2.3768e+00 (-2.3768e+00)\tConsistency Loss 1.7199e+00 (1.7199e+00)\tInconsistency Loss 1.9715e-01 (1.9715e-01)\tEntropy 2.0484e+00 (2.0484e+00)\n",
      "Epoch: [8][100/106]\tTotal Loss -2.3748e+00 (-2.3772e+00)\tConsistency Loss 1.7122e+00 (1.7186e+00)\tInconsistency Loss 1.9985e-01 (1.9763e-01)\tEntropy 2.0435e+00 (2.0479e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/106]\tTotal Loss -2.3794e+00 (-2.3794e+00)\tConsistency Loss 1.7278e+00 (1.7278e+00)\tInconsistency Loss 1.9619e-01 (1.9619e-01)\tEntropy 2.0536e+00 (2.0536e+00)\n",
      "Epoch: [9][100/106]\tTotal Loss -2.3785e+00 (-2.3769e+00)\tConsistency Loss 1.7146e+00 (1.7194e+00)\tInconsistency Loss 1.9846e-01 (1.9748e-01)\tEntropy 2.0466e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/106]\tTotal Loss -2.3781e+00 (-2.3781e+00)\tConsistency Loss 1.7207e+00 (1.7207e+00)\tInconsistency Loss 1.9723e-01 (1.9723e-01)\tEntropy 2.0494e+00 (2.0494e+00)\n",
      "Epoch: [10][100/106]\tTotal Loss -2.3774e+00 (-2.3771e+00)\tConsistency Loss 1.7179e+00 (1.7184e+00)\tInconsistency Loss 1.9773e-01 (1.9768e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7190e+00 (1.7190e+00)\tInconsistency Loss 1.9747e-01 (1.9747e-01)\tEntropy 2.0481e+00 (2.0481e+00)\n",
      "Epoch: [11][100/106]\tTotal Loss -2.3768e+00 (-2.3771e+00)\tConsistency Loss 1.7191e+00 (1.7183e+00)\tInconsistency Loss 1.9761e-01 (1.9770e-01)\tEntropy 2.0480e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7182e+00)\tInconsistency Loss 1.9766e-01 (1.9766e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [12][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7181e+00)\tInconsistency Loss 1.9776e-01 (1.9772e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "S-1\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2618/7131 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/S-1/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/52]\tLoss 1.0072e+00 (1.0072e+00)\n",
      "Epoch: [1][10/52]\tLoss 3.4804e-01 (6.2463e-01)\n",
      "Epoch: [1][20/52]\tLoss 1.2122e-01 (4.2429e-01)\n",
      "Epoch: [1][30/52]\tLoss 4.2277e-02 (3.1034e-01)\n",
      "Epoch: [1][40/52]\tLoss 1.4757e-02 (2.4069e-01)\n",
      "Epoch: [1][50/52]\tLoss 1.0024e-02 (1.9558e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/52]\tLoss 1.0026e-02 (1.0026e-02)\n",
      "Epoch: [2][10/52]\tLoss 1.0034e-02 (1.0041e-02)\n",
      "Epoch: [2][20/52]\tLoss 1.0014e-02 (1.0032e-02)\n",
      "Epoch: [2][30/52]\tLoss 1.0018e-02 (1.0028e-02)\n",
      "Epoch: [2][40/52]\tLoss 1.0021e-02 (1.0026e-02)\n",
      "Epoch: [2][50/52]\tLoss 1.0078e-02 (1.0026e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/52]\tLoss 1.0009e-02 (1.0009e-02)\n",
      "Epoch: [3][10/52]\tLoss 1.0009e-02 (1.0015e-02)\n",
      "Epoch: [3][20/52]\tLoss 1.0011e-02 (1.0015e-02)\n",
      "Epoch: [3][30/52]\tLoss 1.0014e-02 (1.0014e-02)\n",
      "Epoch: [3][40/52]\tLoss 1.0006e-02 (1.0019e-02)\n",
      "Epoch: [3][50/52]\tLoss 1.0003e-02 (1.0018e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/52]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [4][10/52]\tLoss 1.0018e-02 (1.0014e-02)\n",
      "Epoch: [4][20/52]\tLoss 1.0007e-02 (1.0011e-02)\n",
      "Epoch: [4][30/52]\tLoss 1.0024e-02 (1.0015e-02)\n",
      "Epoch: [4][40/52]\tLoss 1.0007e-02 (1.0013e-02)\n",
      "Epoch: [4][50/52]\tLoss 1.0008e-02 (1.0011e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/52]\tLoss 1.0009e-02 (1.0009e-02)\n",
      "Epoch: [5][10/52]\tLoss 1.0008e-02 (1.0006e-02)\n",
      "Epoch: [5][20/52]\tLoss 1.0004e-02 (1.0006e-02)\n",
      "Epoch: [5][30/52]\tLoss 1.0003e-02 (1.0006e-02)\n",
      "Epoch: [5][40/52]\tLoss 1.0003e-02 (1.0007e-02)\n",
      "Epoch: [5][50/52]\tLoss 1.0003e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/52]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [6][10/52]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [6][20/52]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [6][30/52]\tLoss 1.0002e-02 (1.0005e-02)\n",
      "Epoch: [6][40/52]\tLoss 1.0004e-02 (1.0005e-02)\n",
      "Epoch: [6][50/52]\tLoss 1.0004e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/52]\tLoss 1.0008e-02 (1.0008e-02)\n",
      "Epoch: [7][10/52]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [7][20/52]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [7][30/52]\tLoss 1.0010e-02 (1.0004e-02)\n",
      "Epoch: [7][40/52]\tLoss 1.0003e-02 (1.0005e-02)\n",
      "Epoch: [7][50/52]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/52]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [8][10/52]\tLoss 1.0001e-02 (1.0007e-02)\n",
      "Epoch: [8][20/52]\tLoss 1.0002e-02 (1.0005e-02)\n",
      "Epoch: [8][30/52]\tLoss 1.0008e-02 (1.0004e-02)\n",
      "Epoch: [8][40/52]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [8][50/52]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/52]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [9][10/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][20/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][30/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][40/52]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [9][50/52]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [10][10/52]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [10][20/52]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [10][30/52]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [10][40/52]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [10][50/52]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/52]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [11][10/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][20/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][30/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][40/52]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [11][50/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][10/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][20/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][30/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][40/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][50/52]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][10/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][20/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][30/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][40/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][50/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][10/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [14][20/52]\tLoss 1.0008e-02 (1.0002e-02)\n",
      "Epoch: [14][30/52]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [14][40/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [14][50/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][10/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [15][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][30/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [15][40/52]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [15][50/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][30/52]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [16][40/52]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [16][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [17][10/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [17][20/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [17][30/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [17][40/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [17][50/52]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][10/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [18][20/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [18][30/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [19][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [19][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/52]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [20][10/52]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [20][20/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [20][30/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [20][40/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [20][50/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [21][20/52]\tLoss 9.9988e-03 (1.0002e-02)\n",
      "Epoch: [21][30/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [21][40/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [21][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][20/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][30/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [23][20/52]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [23][30/52]\tLoss 1.0018e-02 (1.0004e-02)\n",
      "Epoch: [23][40/52]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [23][50/52]\tLoss 1.0000e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][10/52]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [25][20/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][30/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][10/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [26][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][10/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][20/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/52]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [28][10/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [28][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [28][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][10/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [29][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [30][10/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [30][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/53]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/143]\n",
      "Fill TS Repository [100/143]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries S-1\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5236 - Test samples size: 7131\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/S-1/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/104]\tTotal Loss -2.3015e+00 (-2.3015e+00)\tConsistency Loss 2.2571e+00 (2.2571e+00)\tInconsistency Loss 1.1042e-01 (1.1042e-01)\tEntropy 2.2793e+00 (2.2793e+00)\n",
      "Epoch: [1][100/104]\tTotal Loss -2.3712e+00 (-2.3617e+00)\tConsistency Loss 1.7177e+00 (1.8221e+00)\tInconsistency Loss 1.9780e-01 (1.8101e-01)\tEntropy 2.0445e+00 (2.0919e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/104]\tTotal Loss -2.3898e+00 (-2.3898e+00)\tConsistency Loss 1.7049e+00 (1.7049e+00)\tInconsistency Loss 1.9517e-01 (1.9517e-01)\tEntropy 2.0473e+00 (2.0473e+00)\n",
      "Epoch: [2][100/104]\tTotal Loss -2.3776e+00 (-2.3750e+00)\tConsistency Loss 1.6857e+00 (1.7271e+00)\tInconsistency Loss 2.0335e-01 (1.9676e-01)\tEntropy 2.0317e+00 (2.0511e+00)\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/104]\tTotal Loss -2.3736e+00 (-2.3736e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9953e-01 (1.9953e-01)\tEntropy 2.0458e+00 (2.0458e+00)\n",
      "Epoch: [3][100/104]\tTotal Loss -2.3811e+00 (-2.3768e+00)\tConsistency Loss 1.6999e+00 (1.7203e+00)\tInconsistency Loss 2.0257e-01 (1.9757e-01)\tEntropy 2.0405e+00 (2.0485e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/104]\tTotal Loss -2.3762e+00 (-2.3762e+00)\tConsistency Loss 1.7076e+00 (1.7076e+00)\tInconsistency Loss 1.9864e-01 (1.9864e-01)\tEntropy 2.0419e+00 (2.0419e+00)\n",
      "Epoch: [4][100/104]\tTotal Loss -2.3769e+00 (-2.3767e+00)\tConsistency Loss 1.7329e+00 (1.7201e+00)\tInconsistency Loss 1.9395e-01 (1.9737e-01)\tEntropy 2.0549e+00 (2.0484e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/104]\tTotal Loss -2.3793e+00 (-2.3793e+00)\tConsistency Loss 1.7275e+00 (1.7275e+00)\tInconsistency Loss 1.9626e-01 (1.9626e-01)\tEntropy 2.0534e+00 (2.0534e+00)\n",
      "Epoch: [5][100/104]\tTotal Loss -2.3765e+00 (-2.3768e+00)\tConsistency Loss 1.7233e+00 (1.7196e+00)\tInconsistency Loss 1.9627e-01 (1.9741e-01)\tEntropy 2.0499e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/104]\tTotal Loss -2.3781e+00 (-2.3781e+00)\tConsistency Loss 1.7258e+00 (1.7258e+00)\tInconsistency Loss 1.9610e-01 (1.9610e-01)\tEntropy 2.0520e+00 (2.0520e+00)\n",
      "Epoch: [6][100/104]\tTotal Loss -2.3764e+00 (-2.3771e+00)\tConsistency Loss 1.7378e+00 (1.7176e+00)\tInconsistency Loss 1.9374e-01 (1.9781e-01)\tEntropy 2.0571e+00 (2.0474e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/104]\tTotal Loss -2.3741e+00 (-2.3741e+00)\tConsistency Loss 1.7428e+00 (1.7428e+00)\tInconsistency Loss 1.9290e-01 (1.9290e-01)\tEntropy 2.0584e+00 (2.0584e+00)\n",
      "Epoch: [7][100/104]\tTotal Loss -2.3750e+00 (-2.3769e+00)\tConsistency Loss 1.7174e+00 (1.7190e+00)\tInconsistency Loss 1.9801e-01 (1.9756e-01)\tEntropy 2.0462e+00 (2.0480e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/104]\tTotal Loss -2.3778e+00 (-2.3778e+00)\tConsistency Loss 1.7157e+00 (1.7157e+00)\tInconsistency Loss 1.9840e-01 (1.9840e-01)\tEntropy 2.0467e+00 (2.0467e+00)\n",
      "Epoch: [8][100/104]\tTotal Loss -2.3773e+00 (-2.3771e+00)\tConsistency Loss 1.7157e+00 (1.7186e+00)\tInconsistency Loss 1.9834e-01 (1.9764e-01)\tEntropy 2.0465e+00 (2.0478e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/104]\tTotal Loss -2.3768e+00 (-2.3768e+00)\tConsistency Loss 1.7162e+00 (1.7162e+00)\tInconsistency Loss 1.9822e-01 (1.9822e-01)\tEntropy 2.0465e+00 (2.0465e+00)\n",
      "Epoch: [9][100/104]\tTotal Loss -2.3772e+00 (-2.3771e+00)\tConsistency Loss 1.7187e+00 (1.7184e+00)\tInconsistency Loss 1.9760e-01 (1.9768e-01)\tEntropy 2.0480e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/104]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7183e+00 (1.7183e+00)\tInconsistency Loss 1.9771e-01 (1.9771e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [10][100/104]\tTotal Loss -2.3771e+00 (-2.3772e+00)\tConsistency Loss 1.7185e+00 (1.7181e+00)\tInconsistency Loss 1.9769e-01 (1.9774e-01)\tEntropy 2.0478e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/104]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7182e+00 (1.7182e+00)\tInconsistency Loss 1.9765e-01 (1.9765e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [11][100/104]\tTotal Loss -2.3773e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7181e+00)\tInconsistency Loss 1.9764e-01 (1.9772e-01)\tEntropy 2.0477e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/104]\tTotal Loss -2.3775e+00 (-2.3775e+00)\tConsistency Loss 1.7189e+00 (1.7189e+00)\tInconsistency Loss 1.9748e-01 (1.9748e-01)\tEntropy 2.0482e+00 (2.0482e+00)\n",
      "Epoch: [12][100/104]\tTotal Loss -2.3771e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9780e-01 (1.9772e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/104]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7175e+00 (1.7175e+00)\tInconsistency Loss 1.9784e-01 (1.9784e-01)\tEntropy 2.0474e+00 (2.0474e+00)\n",
      "Epoch: [13][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7181e+00)\tInconsistency Loss 1.9769e-01 (1.9773e-01)\tEntropy 2.0477e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7180e+00)\tInconsistency Loss 1.9773e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "E-1\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2680/8316 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/E-1/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0044e+00 (1.0044e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4828e-01 (6.2446e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2140e-01 (4.2427e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2235e-02 (3.1034e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4754e-02 (2.4068e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0040e-02 (1.9557e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0025e-02 (1.0025e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0022e-02 (1.0034e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0067e-02 (1.0032e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0015e-02 (1.0028e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0008e-02 (1.0026e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0006e-02 (1.0027e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0011e-02 (1.0011e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0039e-02 (1.0023e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0011e-02 (1.0016e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0016e-02 (1.0014e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0007e-02 (1.0013e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0027e-02 (1.0013e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0023e-02 (1.0023e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0005e-02 (1.0010e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0013e-02 (1.0009e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0004e-02 (1.0008e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0002e-02 (1.0008e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0006e-02 (1.0010e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0002e-02 (1.0010e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0004e-02 (1.0007e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0003e-02 (1.0009e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0002e-02 (1.0009e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0005e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0005e-02 (1.0004e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0108e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0007e-02 (1.0005e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0015e-02 (1.0005e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0000e-02 (1.0003e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0013e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0014e-02 (1.0002e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [16][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0014e-02 (1.0001e-02)\n",
      "Epoch: [20][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0006e-02 (1.0001e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/167]\n",
      "Fill TS Repository [100/167]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries E-1\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5360 - Test samples size: 8316\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/E-1/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/107]\tTotal Loss -2.3053e+00 (-2.3053e+00)\tConsistency Loss 2.2547e+00 (2.2547e+00)\tInconsistency Loss 1.1052e-01 (1.1052e-01)\tEntropy 2.2800e+00 (2.2800e+00)\n",
      "Epoch: [1][100/107]\tTotal Loss -2.3761e+00 (-2.3574e+00)\tConsistency Loss 1.7126e+00 (1.8527e+00)\tInconsistency Loss 2.0046e-01 (1.7556e-01)\tEntropy 2.0443e+00 (2.1051e+00)\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/107]\tTotal Loss -2.3714e+00 (-2.3714e+00)\tConsistency Loss 1.7158e+00 (1.7158e+00)\tInconsistency Loss 1.9751e-01 (1.9751e-01)\tEntropy 2.0436e+00 (2.0436e+00)\n",
      "Epoch: [2][100/107]\tTotal Loss -2.3750e+00 (-2.3762e+00)\tConsistency Loss 1.7089e+00 (1.7236e+00)\tInconsistency Loss 2.0006e-01 (1.9677e-01)\tEntropy 2.0420e+00 (2.0499e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/107]\tTotal Loss -2.3717e+00 (-2.3717e+00)\tConsistency Loss 1.7262e+00 (1.7262e+00)\tInconsistency Loss 1.9617e-01 (1.9617e-01)\tEntropy 2.0490e+00 (2.0490e+00)\n",
      "Epoch: [3][100/107]\tTotal Loss -2.3792e+00 (-2.3770e+00)\tConsistency Loss 1.7176e+00 (1.7198e+00)\tInconsistency Loss 1.9757e-01 (1.9751e-01)\tEntropy 2.0484e+00 (2.0484e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/107]\tTotal Loss -2.3778e+00 (-2.3778e+00)\tConsistency Loss 1.7102e+00 (1.7102e+00)\tInconsistency Loss 1.9912e-01 (1.9912e-01)\tEntropy 2.0440e+00 (2.0440e+00)\n",
      "Epoch: [4][100/107]\tTotal Loss -2.3720e+00 (-2.3768e+00)\tConsistency Loss 1.7224e+00 (1.7195e+00)\tInconsistency Loss 1.9708e-01 (1.9745e-01)\tEntropy 2.0472e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/107]\tTotal Loss -2.3767e+00 (-2.3767e+00)\tConsistency Loss 1.7207e+00 (1.7207e+00)\tInconsistency Loss 1.9712e-01 (1.9712e-01)\tEntropy 2.0487e+00 (2.0487e+00)\n",
      "Epoch: [5][100/107]\tTotal Loss -2.3774e+00 (-2.3771e+00)\tConsistency Loss 1.7249e+00 (1.7187e+00)\tInconsistency Loss 1.9651e-01 (1.9764e-01)\tEntropy 2.0512e+00 (2.0479e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7168e+00 (1.7168e+00)\tInconsistency Loss 1.9842e-01 (1.9842e-01)\tEntropy 2.0469e+00 (2.0469e+00)\n",
      "Epoch: [6][100/107]\tTotal Loss -2.3778e+00 (-2.3771e+00)\tConsistency Loss 1.7196e+00 (1.7186e+00)\tInconsistency Loss 1.9715e-01 (1.9763e-01)\tEntropy 2.0487e+00 (2.0478e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/107]\tTotal Loss -2.3763e+00 (-2.3763e+00)\tConsistency Loss 1.7170e+00 (1.7170e+00)\tInconsistency Loss 1.9823e-01 (1.9823e-01)\tEntropy 2.0466e+00 (2.0466e+00)\n",
      "Epoch: [7][100/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7185e+00 (1.7185e+00)\tInconsistency Loss 1.9758e-01 (1.9765e-01)\tEntropy 2.0478e+00 (2.0478e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7176e+00 (1.7176e+00)\tInconsistency Loss 1.9784e-01 (1.9784e-01)\tEntropy 2.0473e+00 (2.0473e+00)\n",
      "Epoch: [8][100/107]\tTotal Loss -2.3772e+00 (-2.3771e+00)\tConsistency Loss 1.7180e+00 (1.7182e+00)\tInconsistency Loss 1.9774e-01 (1.9770e-01)\tEntropy 2.0476e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [9][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9772e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [10][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [11][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [12][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "E-2\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2680/8332 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/E-2/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0038e+00 (1.0038e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4831e-01 (6.2440e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2142e-01 (4.2424e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2269e-02 (3.1033e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4790e-02 (2.4068e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0035e-02 (1.9557e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0023e-02 (1.0023e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0017e-02 (1.0034e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0039e-02 (1.0033e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0022e-02 (1.0029e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0005e-02 (1.0027e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0009e-02 (1.0026e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0010e-02 (1.0010e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0033e-02 (1.0021e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0012e-02 (1.0016e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0014e-02 (1.0013e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0006e-02 (1.0013e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0015e-02 (1.0012e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0026e-02 (1.0026e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0004e-02 (1.0009e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0016e-02 (1.0009e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0004e-02 (1.0008e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0001e-02 (1.0008e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0005e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0003e-02 (1.0012e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0005e-02 (1.0009e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0003e-02 (1.0009e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0002e-02 (1.0009e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0003e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0061e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0007e-02 (1.0004e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0014e-02 (1.0004e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0006e-02 (1.0002e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0009e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0006e-02 (1.0001e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [16][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][40/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0012e-02 (1.0001e-02)\n",
      "Epoch: [20][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0005e-02 (1.0001e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/167]\n",
      "Fill TS Repository [100/167]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries E-2\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5360 - Test samples size: 8332\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/E-2/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/107]\tTotal Loss -2.3046e+00 (-2.3046e+00)\tConsistency Loss 2.2559e+00 (2.2559e+00)\tInconsistency Loss 1.1043e-01 (1.1043e-01)\tEntropy 2.2802e+00 (2.2802e+00)\n",
      "Epoch: [1][100/107]\tTotal Loss -2.3824e+00 (-2.3587e+00)\tConsistency Loss 1.7467e+00 (1.8539e+00)\tInconsistency Loss 1.9375e-01 (1.7564e-01)\tEntropy 2.0645e+00 (2.1063e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/107]\tTotal Loss -2.3662e+00 (-2.3662e+00)\tConsistency Loss 1.6921e+00 (1.6921e+00)\tInconsistency Loss 2.0712e-01 (2.0712e-01)\tEntropy 2.0292e+00 (2.0292e+00)\n",
      "Epoch: [2][100/107]\tTotal Loss -2.3648e+00 (-2.3763e+00)\tConsistency Loss 1.6976e+00 (1.7240e+00)\tInconsistency Loss 2.0219e-01 (1.9670e-01)\tEntropy 2.0312e+00 (2.0502e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/107]\tTotal Loss -2.3921e+00 (-2.3921e+00)\tConsistency Loss 1.7161e+00 (1.7161e+00)\tInconsistency Loss 1.9871e-01 (1.9871e-01)\tEntropy 2.0541e+00 (2.0541e+00)\n",
      "Epoch: [3][100/107]\tTotal Loss -2.3833e+00 (-2.3767e+00)\tConsistency Loss 1.7221e+00 (1.7211e+00)\tInconsistency Loss 1.9662e-01 (1.9750e-01)\tEntropy 2.0527e+00 (2.0489e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/107]\tTotal Loss -2.3737e+00 (-2.3737e+00)\tConsistency Loss 1.6825e+00 (1.6825e+00)\tInconsistency Loss 2.0571e-01 (2.0571e-01)\tEntropy 2.0281e+00 (2.0281e+00)\n",
      "Epoch: [4][100/107]\tTotal Loss -2.3734e+00 (-2.3768e+00)\tConsistency Loss 1.7239e+00 (1.7183e+00)\tInconsistency Loss 1.9625e-01 (1.9764e-01)\tEntropy 2.0486e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/107]\tTotal Loss -2.3755e+00 (-2.3755e+00)\tConsistency Loss 1.7418e+00 (1.7418e+00)\tInconsistency Loss 1.9407e-01 (1.9407e-01)\tEntropy 2.0587e+00 (2.0587e+00)\n",
      "Epoch: [5][100/107]\tTotal Loss -2.3796e+00 (-2.3767e+00)\tConsistency Loss 1.7386e+00 (1.7193e+00)\tInconsistency Loss 1.9412e-01 (1.9769e-01)\tEntropy 2.0591e+00 (2.0480e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/107]\tTotal Loss -2.3779e+00 (-2.3779e+00)\tConsistency Loss 1.7301e+00 (1.7301e+00)\tInconsistency Loss 1.9564e-01 (1.9564e-01)\tEntropy 2.0540e+00 (2.0540e+00)\n",
      "Epoch: [6][100/107]\tTotal Loss -2.3785e+00 (-2.3767e+00)\tConsistency Loss 1.7121e+00 (1.7198e+00)\tInconsistency Loss 1.9928e-01 (1.9738e-01)\tEntropy 2.0453e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/107]\tTotal Loss -2.3777e+00 (-2.3777e+00)\tConsistency Loss 1.7228e+00 (1.7228e+00)\tInconsistency Loss 1.9644e-01 (1.9644e-01)\tEntropy 2.0503e+00 (2.0503e+00)\n",
      "Epoch: [7][100/107]\tTotal Loss -2.3759e+00 (-2.3771e+00)\tConsistency Loss 1.7176e+00 (1.7187e+00)\tInconsistency Loss 1.9752e-01 (1.9755e-01)\tEntropy 2.0467e+00 (2.0479e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/107]\tTotal Loss -2.3779e+00 (-2.3779e+00)\tConsistency Loss 1.7201e+00 (1.7201e+00)\tInconsistency Loss 1.9688e-01 (1.9688e-01)\tEntropy 2.0490e+00 (2.0490e+00)\n",
      "Epoch: [8][100/107]\tTotal Loss -2.3769e+00 (-2.3771e+00)\tConsistency Loss 1.7195e+00 (1.7185e+00)\tInconsistency Loss 1.9822e-01 (1.9766e-01)\tEntropy 2.0482e+00 (2.0478e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/107]\tTotal Loss -2.3796e+00 (-2.3796e+00)\tConsistency Loss 1.7202e+00 (1.7202e+00)\tInconsistency Loss 1.9732e-01 (1.9732e-01)\tEntropy 2.0499e+00 (2.0499e+00)\n",
      "Epoch: [9][100/107]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7187e+00 (1.7189e+00)\tInconsistency Loss 1.9759e-01 (1.9759e-01)\tEntropy 2.0478e+00 (2.0480e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/107]\tTotal Loss -2.3774e+00 (-2.3774e+00)\tConsistency Loss 1.7175e+00 (1.7175e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0475e+00 (2.0475e+00)\n",
      "Epoch: [10][100/107]\tTotal Loss -2.3777e+00 (-2.3771e+00)\tConsistency Loss 1.7188e+00 (1.7182e+00)\tInconsistency Loss 1.9760e-01 (1.9771e-01)\tEntropy 2.0482e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7177e+00 (1.7177e+00)\tInconsistency Loss 1.9781e-01 (1.9781e-01)\tEntropy 2.0474e+00 (2.0474e+00)\n",
      "Epoch: [11][100/107]\tTotal Loss -2.3771e+00 (-2.3772e+00)\tConsistency Loss 1.7176e+00 (1.7181e+00)\tInconsistency Loss 1.9782e-01 (1.9773e-01)\tEntropy 2.0473e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/107]\tTotal Loss -2.3774e+00 (-2.3774e+00)\tConsistency Loss 1.7184e+00 (1.7184e+00)\tInconsistency Loss 1.9763e-01 (1.9763e-01)\tEntropy 2.0479e+00 (2.0479e+00)\n",
      "Epoch: [12][100/107]\tTotal Loss -2.3771e+00 (-2.3772e+00)\tConsistency Loss 1.7193e+00 (1.7181e+00)\tInconsistency Loss 1.9745e-01 (1.9772e-01)\tEntropy 2.0482e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/107]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7173e+00 (1.7173e+00)\tInconsistency Loss 1.9794e-01 (1.9794e-01)\tEntropy 2.0472e+00 (2.0472e+00)\n",
      "Epoch: [13][100/107]\tTotal Loss -2.3777e+00 (-2.3772e+00)\tConsistency Loss 1.7184e+00 (1.7182e+00)\tInconsistency Loss 1.9768e-01 (1.9772e-01)\tEntropy 2.0481e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7183e+00 (1.7183e+00)\tInconsistency Loss 1.9770e-01 (1.9770e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [14][100/107]\tTotal Loss -2.3760e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7182e+00)\tInconsistency Loss 1.9766e-01 (1.9770e-01)\tEntropy 2.0471e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/107]\tTotal Loss -2.3777e+00 (-2.3777e+00)\tConsistency Loss 1.7195e+00 (1.7195e+00)\tInconsistency Loss 1.9734e-01 (1.9734e-01)\tEntropy 2.0486e+00 (2.0486e+00)\n",
      "Epoch: [15][100/107]\tTotal Loss -2.3755e+00 (-2.3772e+00)\tConsistency Loss 1.7220e+00 (1.7181e+00)\tInconsistency Loss 1.9726e-01 (1.9773e-01)\tEntropy 2.0487e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/107]\tTotal Loss -2.3774e+00 (-2.3774e+00)\tConsistency Loss 1.7137e+00 (1.7137e+00)\tInconsistency Loss 1.9861e-01 (1.9861e-01)\tEntropy 2.0455e+00 (2.0455e+00)\n",
      "Epoch: [16][100/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7173e+00 (1.7181e+00)\tInconsistency Loss 1.9784e-01 (1.9773e-01)\tEntropy 2.0472e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7185e+00 (1.7185e+00)\tInconsistency Loss 1.9741e-01 (1.9741e-01)\tEntropy 2.0478e+00 (2.0478e+00)\n",
      "Epoch: [17][100/107]\tTotal Loss -2.3772e+00 (-2.3771e+00)\tConsistency Loss 1.7180e+00 (1.7182e+00)\tInconsistency Loss 1.9775e-01 (1.9771e-01)\tEntropy 2.0476e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [18][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "E-3\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2680/8107 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/E-3/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0042e+00 (1.0042e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4831e-01 (6.2438e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2147e-01 (4.2423e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2249e-02 (3.1033e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4769e-02 (2.4068e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0024e-02 (1.9557e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0026e-02 (1.0026e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0020e-02 (1.0031e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0044e-02 (1.0027e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0020e-02 (1.0025e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0004e-02 (1.0023e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0006e-02 (1.0022e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0031e-02 (1.0018e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0010e-02 (1.0013e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0009e-02 (1.0011e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0005e-02 (1.0010e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0019e-02 (1.0010e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0021e-02 (1.0021e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0005e-02 (1.0009e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0015e-02 (1.0008e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0005e-02 (1.0007e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0001e-02 (1.0007e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0006e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0003e-02 (1.0011e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0003e-02 (1.0008e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0003e-02 (1.0008e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0002e-02 (1.0008e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0003e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0060e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0007e-02 (1.0004e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0008e-02 (1.0004e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0010e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0007e-02 (1.0001e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [16][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][40/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0015e-02 (1.0001e-02)\n",
      "Epoch: [20][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0005e-02 (1.0001e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/163]\n",
      "Fill TS Repository [100/163]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries E-3\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5360 - Test samples size: 8107\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/E-3/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/107]\tTotal Loss -2.3052e+00 (-2.3052e+00)\tConsistency Loss 2.2554e+00 (2.2554e+00)\tInconsistency Loss 1.1046e-01 (1.1046e-01)\tEntropy 2.2803e+00 (2.2803e+00)\n",
      "Epoch: [1][100/107]\tTotal Loss -2.3762e+00 (-2.3530e+00)\tConsistency Loss 1.7262e+00 (1.8898e+00)\tInconsistency Loss 1.9889e-01 (1.6902e-01)\tEntropy 2.0512e+00 (2.1214e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/107]\tTotal Loss -2.3634e+00 (-2.3634e+00)\tConsistency Loss 1.6936e+00 (1.6936e+00)\tInconsistency Loss 2.0298e-01 (2.0298e-01)\tEntropy 2.0285e+00 (2.0285e+00)\n",
      "Epoch: [2][100/107]\tTotal Loss -2.3700e+00 (-2.3765e+00)\tConsistency Loss 1.6966e+00 (1.7201e+00)\tInconsistency Loss 2.0169e-01 (1.9741e-01)\tEntropy 2.0333e+00 (2.0483e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/107]\tTotal Loss -2.3759e+00 (-2.3759e+00)\tConsistency Loss 1.7031e+00 (1.7031e+00)\tInconsistency Loss 2.0099e-01 (2.0099e-01)\tEntropy 2.0395e+00 (2.0395e+00)\n",
      "Epoch: [3][100/107]\tTotal Loss -2.3802e+00 (-2.3771e+00)\tConsistency Loss 1.7136e+00 (1.7198e+00)\tInconsistency Loss 1.9863e-01 (1.9738e-01)\tEntropy 2.0469e+00 (2.0485e+00)\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/107]\tTotal Loss -2.3709e+00 (-2.3709e+00)\tConsistency Loss 1.6926e+00 (1.6926e+00)\tInconsistency Loss 2.0364e-01 (2.0364e-01)\tEntropy 2.0317e+00 (2.0317e+00)\n",
      "Epoch: [4][100/107]\tTotal Loss -2.3686e+00 (-2.3765e+00)\tConsistency Loss 1.7294e+00 (1.7203e+00)\tInconsistency Loss 1.9530e-01 (1.9727e-01)\tEntropy 2.0490e+00 (2.0484e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/107]\tTotal Loss -2.3765e+00 (-2.3765e+00)\tConsistency Loss 1.7346e+00 (1.7346e+00)\tInconsistency Loss 1.9442e-01 (1.9442e-01)\tEntropy 2.0556e+00 (2.0556e+00)\n",
      "Epoch: [5][100/107]\tTotal Loss -2.3756e+00 (-2.3768e+00)\tConsistency Loss 1.7365e+00 (1.7197e+00)\tInconsistency Loss 1.9434e-01 (1.9752e-01)\tEntropy 2.0561e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/107]\tTotal Loss -2.3818e+00 (-2.3818e+00)\tConsistency Loss 1.7141e+00 (1.7141e+00)\tInconsistency Loss 1.9762e-01 (1.9762e-01)\tEntropy 2.0479e+00 (2.0479e+00)\n",
      "Epoch: [6][100/107]\tTotal Loss -2.3772e+00 (-2.3771e+00)\tConsistency Loss 1.7180e+00 (1.7181e+00)\tInconsistency Loss 1.9776e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7186e+00 (1.7186e+00)\tInconsistency Loss 1.9761e-01 (1.9761e-01)\tEntropy 2.0479e+00 (2.0479e+00)\n",
      "Epoch: [7][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7187e+00 (1.7183e+00)\tInconsistency Loss 1.9759e-01 (1.9769e-01)\tEntropy 2.0480e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7207e+00 (1.7207e+00)\tInconsistency Loss 1.9715e-01 (1.9715e-01)\tEntropy 2.0490e+00 (2.0490e+00)\n",
      "Epoch: [8][100/107]\tTotal Loss -2.3771e+00 (-2.3772e+00)\tConsistency Loss 1.7218e+00 (1.7183e+00)\tInconsistency Loss 1.9693e-01 (1.9769e-01)\tEntropy 2.0495e+00 (2.0478e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7173e+00 (1.7173e+00)\tInconsistency Loss 1.9790e-01 (1.9790e-01)\tEntropy 2.0472e+00 (2.0472e+00)\n",
      "Epoch: [9][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7182e+00)\tInconsistency Loss 1.9771e-01 (1.9771e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7182e+00)\tInconsistency Loss 1.9770e-01 (1.9770e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [10][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7182e+00)\tInconsistency Loss 1.9771e-01 (1.9771e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7182e+00)\tInconsistency Loss 1.9771e-01 (1.9771e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [11][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [12][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "E-4\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2680/8154 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/E-4/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0042e+00 (1.0042e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4827e-01 (6.2438e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2144e-01 (4.2423e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2256e-02 (3.1033e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4775e-02 (2.4068e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0027e-02 (1.9557e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0025e-02 (1.0025e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0014e-02 (1.0033e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0043e-02 (1.0030e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0015e-02 (1.0027e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0005e-02 (1.0025e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0004e-02 (1.0023e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0008e-02 (1.0008e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0026e-02 (1.0017e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0007e-02 (1.0012e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0006e-02 (1.0011e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0005e-02 (1.0010e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0015e-02 (1.0010e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0021e-02 (1.0021e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0005e-02 (1.0009e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0009e-02 (1.0008e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0004e-02 (1.0007e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0001e-02 (1.0007e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0003e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0004e-02 (1.0012e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0003e-02 (1.0008e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0002e-02 (1.0009e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0001e-02 (1.0009e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0003e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0082e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0008e-02 (1.0005e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0010e-02 (1.0004e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0007e-02 (1.0002e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0010e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0006e-02 (1.0001e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [16][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][40/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0011e-02 (1.0001e-02)\n",
      "Epoch: [20][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/164]\n",
      "Fill TS Repository [100/164]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries E-4\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5360 - Test samples size: 8154\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/E-4/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/107]\tTotal Loss -2.3045e+00 (-2.3045e+00)\tConsistency Loss 2.2558e+00 (2.2558e+00)\tInconsistency Loss 1.1048e-01 (1.1048e-01)\tEntropy 2.2802e+00 (2.2802e+00)\n",
      "Epoch: [1][100/107]\tTotal Loss -2.3732e+00 (-2.3565e+00)\tConsistency Loss 1.7335e+00 (1.8733e+00)\tInconsistency Loss 1.9956e-01 (1.7269e-01)\tEntropy 2.0534e+00 (2.1149e+00)\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/107]\tTotal Loss -2.3703e+00 (-2.3703e+00)\tConsistency Loss 1.6935e+00 (1.6935e+00)\tInconsistency Loss 2.0272e-01 (2.0272e-01)\tEntropy 2.0319e+00 (2.0319e+00)\n",
      "Epoch: [2][100/107]\tTotal Loss -2.3761e+00 (-2.3762e+00)\tConsistency Loss 1.7190e+00 (1.7207e+00)\tInconsistency Loss 1.9806e-01 (1.9729e-01)\tEntropy 2.0475e+00 (2.0485e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/107]\tTotal Loss -2.3631e+00 (-2.3631e+00)\tConsistency Loss 1.7194e+00 (1.7194e+00)\tInconsistency Loss 1.9719e-01 (1.9719e-01)\tEntropy 2.0412e+00 (2.0412e+00)\n",
      "Epoch: [3][100/107]\tTotal Loss -2.3768e+00 (-2.3762e+00)\tConsistency Loss 1.7200e+00 (1.7225e+00)\tInconsistency Loss 1.9706e-01 (1.9684e-01)\tEntropy 2.0484e+00 (2.0493e+00)\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/107]\tTotal Loss -2.3812e+00 (-2.3812e+00)\tConsistency Loss 1.7183e+00 (1.7183e+00)\tInconsistency Loss 1.9775e-01 (1.9775e-01)\tEntropy 2.0497e+00 (2.0497e+00)\n",
      "Epoch: [4][100/107]\tTotal Loss -2.3783e+00 (-2.3769e+00)\tConsistency Loss 1.7175e+00 (1.7198e+00)\tInconsistency Loss 1.9770e-01 (1.9739e-01)\tEntropy 2.0479e+00 (2.0483e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/107]\tTotal Loss -2.3764e+00 (-2.3764e+00)\tConsistency Loss 1.7146e+00 (1.7146e+00)\tInconsistency Loss 1.9987e-01 (1.9987e-01)\tEntropy 2.0455e+00 (2.0455e+00)\n",
      "Epoch: [5][100/107]\tTotal Loss -2.3796e+00 (-2.3770e+00)\tConsistency Loss 1.7197e+00 (1.7198e+00)\tInconsistency Loss 1.9667e-01 (1.9734e-01)\tEntropy 2.0496e+00 (2.0484e+00)\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/107]\tTotal Loss -2.3783e+00 (-2.3783e+00)\tConsistency Loss 1.7085e+00 (1.7085e+00)\tInconsistency Loss 2.0049e-01 (2.0049e-01)\tEntropy 2.0434e+00 (2.0434e+00)\n",
      "Epoch: [6][100/107]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7148e+00 (1.7190e+00)\tInconsistency Loss 1.9905e-01 (1.9759e-01)\tEntropy 2.0459e+00 (2.0480e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/107]\tTotal Loss -2.3750e+00 (-2.3750e+00)\tConsistency Loss 1.7231e+00 (1.7231e+00)\tInconsistency Loss 1.9721e-01 (1.9721e-01)\tEntropy 2.0490e+00 (2.0490e+00)\n",
      "Epoch: [7][100/107]\tTotal Loss -2.3771e+00 (-2.3775e+00)\tConsistency Loss 1.7263e+00 (1.7182e+00)\tInconsistency Loss 1.9595e-01 (1.9760e-01)\tEntropy 2.0517e+00 (2.0478e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7079e+00 (1.7079e+00)\tInconsistency Loss 1.9998e-01 (1.9998e-01)\tEntropy 2.0425e+00 (2.0425e+00)\n",
      "Epoch: [8][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7183e+00)\tInconsistency Loss 1.9771e-01 (1.9768e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7182e+00)\tInconsistency Loss 1.9770e-01 (1.9770e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [9][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7182e+00)\tInconsistency Loss 1.9772e-01 (1.9771e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [10][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9772e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [11][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [12][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "E-5\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2680/8094 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/E-5/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0048e+00 (1.0048e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4835e-01 (6.2443e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2141e-01 (4.2425e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2243e-02 (3.1034e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4778e-02 (2.4068e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0040e-02 (1.9557e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0028e-02 (1.0028e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0017e-02 (1.0033e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0044e-02 (1.0030e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0015e-02 (1.0028e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0007e-02 (1.0026e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0007e-02 (1.0026e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0009e-02 (1.0009e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0033e-02 (1.0023e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0010e-02 (1.0017e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0009e-02 (1.0014e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0008e-02 (1.0013e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0015e-02 (1.0012e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0023e-02 (1.0023e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0003e-02 (1.0010e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0012e-02 (1.0009e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0003e-02 (1.0008e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0002e-02 (1.0008e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0005e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0002e-02 (1.0009e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0003e-02 (1.0007e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0002e-02 (1.0008e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0002e-02 (1.0009e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0006e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0087e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0005e-02 (1.0004e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0012e-02 (1.0004e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0006e-02 (1.0002e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0011e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0011e-02 (1.0001e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [16][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][40/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0011e-02 (1.0001e-02)\n",
      "Epoch: [20][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0005e-02 (1.0001e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/162]\n",
      "Fill TS Repository [100/162]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries E-5\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5360 - Test samples size: 8094\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/E-5/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/107]\tTotal Loss -2.3047e+00 (-2.3047e+00)\tConsistency Loss 2.2554e+00 (2.2554e+00)\tInconsistency Loss 1.1047e-01 (1.1047e-01)\tEntropy 2.2801e+00 (2.2801e+00)\n",
      "Epoch: [1][100/107]\tTotal Loss -2.3741e+00 (-2.3538e+00)\tConsistency Loss 1.7281e+00 (1.8859e+00)\tInconsistency Loss 1.9435e-01 (1.6996e-01)\tEntropy 2.0511e+00 (2.1198e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/107]\tTotal Loss -2.3756e+00 (-2.3756e+00)\tConsistency Loss 1.6943e+00 (1.6943e+00)\tInconsistency Loss 2.0072e-01 (2.0072e-01)\tEntropy 2.0350e+00 (2.0350e+00)\n",
      "Epoch: [2][100/107]\tTotal Loss -2.3753e+00 (-2.3770e+00)\tConsistency Loss 1.7108e+00 (1.7190e+00)\tInconsistency Loss 1.9944e-01 (1.9762e-01)\tEntropy 2.0430e+00 (2.0480e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/107]\tTotal Loss -2.3783e+00 (-2.3783e+00)\tConsistency Loss 1.7178e+00 (1.7178e+00)\tInconsistency Loss 1.9922e-01 (1.9922e-01)\tEntropy 2.0481e+00 (2.0481e+00)\n",
      "Epoch: [3][100/107]\tTotal Loss -2.3767e+00 (-2.3777e+00)\tConsistency Loss 1.6860e+00 (1.7172e+00)\tInconsistency Loss 2.0584e-01 (1.9812e-01)\tEntropy 2.0313e+00 (2.0475e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/107]\tTotal Loss -2.3784e+00 (-2.3784e+00)\tConsistency Loss 1.7095e+00 (1.7095e+00)\tInconsistency Loss 1.9803e-01 (1.9803e-01)\tEntropy 2.0439e+00 (2.0439e+00)\n",
      "Epoch: [4][100/107]\tTotal Loss -2.3789e+00 (-2.3763e+00)\tConsistency Loss 1.7198e+00 (1.7219e+00)\tInconsistency Loss 1.9791e-01 (1.9695e-01)\tEntropy 2.0494e+00 (2.0491e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/107]\tTotal Loss -2.3776e+00 (-2.3776e+00)\tConsistency Loss 1.7151e+00 (1.7151e+00)\tInconsistency Loss 1.9874e-01 (1.9874e-01)\tEntropy 2.0464e+00 (2.0464e+00)\n",
      "Epoch: [5][100/107]\tTotal Loss -2.3802e+00 (-2.3772e+00)\tConsistency Loss 1.7364e+00 (1.7197e+00)\tInconsistency Loss 1.9369e-01 (1.9745e-01)\tEntropy 2.0583e+00 (2.0484e+00)\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/107]\tTotal Loss -2.3795e+00 (-2.3795e+00)\tConsistency Loss 1.7145e+00 (1.7145e+00)\tInconsistency Loss 1.9817e-01 (1.9817e-01)\tEntropy 2.0470e+00 (2.0470e+00)\n",
      "Epoch: [6][100/107]\tTotal Loss -2.3754e+00 (-2.3767e+00)\tConsistency Loss 1.7147e+00 (1.7195e+00)\tInconsistency Loss 1.9921e-01 (1.9758e-01)\tEntropy 2.0451e+00 (2.0481e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/107]\tTotal Loss -2.3754e+00 (-2.3754e+00)\tConsistency Loss 1.7203e+00 (1.7203e+00)\tInconsistency Loss 1.9767e-01 (1.9767e-01)\tEntropy 2.0478e+00 (2.0478e+00)\n",
      "Epoch: [7][100/107]\tTotal Loss -2.3739e+00 (-2.3771e+00)\tConsistency Loss 1.7210e+00 (1.7184e+00)\tInconsistency Loss 1.9810e-01 (1.9768e-01)\tEntropy 2.0474e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/107]\tTotal Loss -2.3777e+00 (-2.3777e+00)\tConsistency Loss 1.7195e+00 (1.7195e+00)\tInconsistency Loss 1.9702e-01 (1.9702e-01)\tEntropy 2.0486e+00 (2.0486e+00)\n",
      "Epoch: [8][100/107]\tTotal Loss -2.3776e+00 (-2.3769e+00)\tConsistency Loss 1.7182e+00 (1.7187e+00)\tInconsistency Loss 1.9769e-01 (1.9762e-01)\tEntropy 2.0479e+00 (2.0478e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/107]\tTotal Loss -2.3780e+00 (-2.3780e+00)\tConsistency Loss 1.7205e+00 (1.7205e+00)\tInconsistency Loss 1.9688e-01 (1.9688e-01)\tEntropy 2.0493e+00 (2.0493e+00)\n",
      "Epoch: [9][100/107]\tTotal Loss -2.3771e+00 (-2.3772e+00)\tConsistency Loss 1.7200e+00 (1.7182e+00)\tInconsistency Loss 1.9733e-01 (1.9771e-01)\tEntropy 2.0485e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7167e+00 (1.7167e+00)\tInconsistency Loss 1.9794e-01 (1.9794e-01)\tEntropy 2.0469e+00 (2.0469e+00)\n",
      "Epoch: [10][100/107]\tTotal Loss -2.3775e+00 (-2.3771e+00)\tConsistency Loss 1.7201e+00 (1.7182e+00)\tInconsistency Loss 1.9735e-01 (1.9772e-01)\tEntropy 2.0488e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/107]\tTotal Loss -2.3769e+00 (-2.3769e+00)\tConsistency Loss 1.7170e+00 (1.7170e+00)\tInconsistency Loss 1.9801e-01 (1.9801e-01)\tEntropy 2.0470e+00 (2.0470e+00)\n",
      "Epoch: [11][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7176e+00 (1.7182e+00)\tInconsistency Loss 1.9782e-01 (1.9772e-01)\tEntropy 2.0474e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7184e+00 (1.7184e+00)\tInconsistency Loss 1.9763e-01 (1.9763e-01)\tEntropy 2.0478e+00 (2.0478e+00)\n",
      "Epoch: [12][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7185e+00 (1.7182e+00)\tInconsistency Loss 1.9763e-01 (1.9771e-01)\tEntropy 2.0479e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7176e+00 (1.7176e+00)\tInconsistency Loss 1.9785e-01 (1.9785e-01)\tEntropy 2.0474e+00 (2.0474e+00)\n",
      "Epoch: [13][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "E-6\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2680/8100 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/E-6/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0002e+00 (1.0002e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4874e-01 (6.2387e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2154e-01 (4.2410e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2384e-02 (3.1028e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4775e-02 (2.4066e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0002e-02 (1.9555e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0012e-02 (1.0002e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/162]\n",
      "Fill TS Repository [100/162]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries E-6\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5360 - Test samples size: 8100\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/E-6/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/107]\tTotal Loss -2.3010e+00 (-2.3010e+00)\tConsistency Loss 2.2574e+00 (2.2574e+00)\tInconsistency Loss 1.1059e-01 (1.1059e-01)\tEntropy 2.2792e+00 (2.2792e+00)\n",
      "Epoch: [1][100/107]\tTotal Loss -2.3767e+00 (-2.3579e+00)\tConsistency Loss 1.7454e+00 (1.8471e+00)\tInconsistency Loss 1.9226e-01 (1.7633e-01)\tEntropy 2.0610e+00 (2.1025e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/107]\tTotal Loss -2.3817e+00 (-2.3817e+00)\tConsistency Loss 1.7008e+00 (1.7008e+00)\tInconsistency Loss 2.0119e-01 (2.0119e-01)\tEntropy 2.0413e+00 (2.0413e+00)\n",
      "Epoch: [2][100/107]\tTotal Loss -2.3773e+00 (-2.3775e+00)\tConsistency Loss 1.7040e+00 (1.7176e+00)\tInconsistency Loss 2.0124e-01 (1.9810e-01)\tEntropy 2.0407e+00 (2.0475e+00)\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/107]\tTotal Loss -2.3709e+00 (-2.3709e+00)\tConsistency Loss 1.7410e+00 (1.7410e+00)\tInconsistency Loss 1.9292e-01 (1.9292e-01)\tEntropy 2.0560e+00 (2.0560e+00)\n",
      "Epoch: [3][100/107]\tTotal Loss -2.3807e+00 (-2.3758e+00)\tConsistency Loss 1.7343e+00 (1.7229e+00)\tInconsistency Loss 1.9484e-01 (1.9682e-01)\tEntropy 2.0575e+00 (2.0493e+00)\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/107]\tTotal Loss -2.3725e+00 (-2.3725e+00)\tConsistency Loss 1.7278e+00 (1.7278e+00)\tInconsistency Loss 1.9610e-01 (1.9610e-01)\tEntropy 2.0501e+00 (2.0501e+00)\n",
      "Epoch: [4][100/107]\tTotal Loss -2.3649e+00 (-2.3767e+00)\tConsistency Loss 1.7393e+00 (1.7197e+00)\tInconsistency Loss 1.9602e-01 (1.9742e-01)\tEntropy 2.0521e+00 (2.0482e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/107]\tTotal Loss -2.3798e+00 (-2.3798e+00)\tConsistency Loss 1.7137e+00 (1.7137e+00)\tInconsistency Loss 1.9870e-01 (1.9870e-01)\tEntropy 2.0467e+00 (2.0467e+00)\n",
      "Epoch: [5][100/107]\tTotal Loss -2.3785e+00 (-2.3768e+00)\tConsistency Loss 1.7297e+00 (1.7198e+00)\tInconsistency Loss 1.9565e-01 (1.9724e-01)\tEntropy 2.0541e+00 (2.0483e+00)\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/107]\tTotal Loss -2.3767e+00 (-2.3767e+00)\tConsistency Loss 1.7035e+00 (1.7035e+00)\tInconsistency Loss 2.0042e-01 (2.0042e-01)\tEntropy 2.0401e+00 (2.0401e+00)\n",
      "Epoch: [6][100/107]\tTotal Loss -2.3775e+00 (-2.3771e+00)\tConsistency Loss 1.7168e+00 (1.7185e+00)\tInconsistency Loss 1.9808e-01 (1.9770e-01)\tEntropy 2.0471e+00 (2.0478e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/107]\tTotal Loss -2.3763e+00 (-2.3763e+00)\tConsistency Loss 1.7210e+00 (1.7210e+00)\tInconsistency Loss 1.9706e-01 (1.9706e-01)\tEntropy 2.0486e+00 (2.0486e+00)\n",
      "Epoch: [7][100/107]\tTotal Loss -2.3763e+00 (-2.3770e+00)\tConsistency Loss 1.7199e+00 (1.7188e+00)\tInconsistency Loss 1.9744e-01 (1.9758e-01)\tEntropy 2.0481e+00 (2.0479e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/107]\tTotal Loss -2.3767e+00 (-2.3767e+00)\tConsistency Loss 1.7162e+00 (1.7162e+00)\tInconsistency Loss 1.9815e-01 (1.9815e-01)\tEntropy 2.0464e+00 (2.0464e+00)\n",
      "Epoch: [8][100/107]\tTotal Loss -2.3768e+00 (-2.3772e+00)\tConsistency Loss 1.7167e+00 (1.7181e+00)\tInconsistency Loss 1.9795e-01 (1.9772e-01)\tEntropy 2.0468e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/107]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7191e+00 (1.7191e+00)\tInconsistency Loss 1.9725e-01 (1.9725e-01)\tEntropy 2.0481e+00 (2.0481e+00)\n",
      "Epoch: [9][100/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7180e+00 (1.7183e+00)\tInconsistency Loss 1.9775e-01 (1.9769e-01)\tEntropy 2.0476e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7175e+00 (1.7175e+00)\tInconsistency Loss 1.9783e-01 (1.9783e-01)\tEntropy 2.0474e+00 (2.0474e+00)\n",
      "Epoch: [10][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7181e+00)\tInconsistency Loss 1.9770e-01 (1.9773e-01)\tEntropy 2.0477e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7179e+00 (1.7179e+00)\tInconsistency Loss 1.9776e-01 (1.9776e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [11][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [12][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "E-7\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2569/8110 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/E-7/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/51]\tLoss 1.0024e+00 (1.0024e+00)\n",
      "Epoch: [1][10/51]\tLoss 3.4857e-01 (6.2443e-01)\n",
      "Epoch: [1][20/51]\tLoss 1.2128e-01 (4.2425e-01)\n",
      "Epoch: [1][30/51]\tLoss 4.2377e-02 (3.1033e-01)\n",
      "Epoch: [1][40/51]\tLoss 1.4701e-02 (2.4068e-01)\n",
      "Epoch: [1][50/51]\tLoss 1.0029e-02 (1.9557e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/51]\tLoss 1.0020e-02 (1.0020e-02)\n",
      "Epoch: [2][10/51]\tLoss 1.0024e-02 (1.0033e-02)\n",
      "Epoch: [2][20/51]\tLoss 1.0012e-02 (1.0024e-02)\n",
      "Epoch: [2][30/51]\tLoss 1.0013e-02 (1.0022e-02)\n",
      "Epoch: [2][40/51]\tLoss 1.0009e-02 (1.0021e-02)\n",
      "Epoch: [2][50/51]\tLoss 1.0018e-02 (1.0021e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/51]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [3][10/51]\tLoss 1.0007e-02 (1.0011e-02)\n",
      "Epoch: [3][20/51]\tLoss 1.0012e-02 (1.0012e-02)\n",
      "Epoch: [3][30/51]\tLoss 1.0011e-02 (1.0011e-02)\n",
      "Epoch: [3][40/51]\tLoss 1.0034e-02 (1.0011e-02)\n",
      "Epoch: [3][50/51]\tLoss 1.0006e-02 (1.0011e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/51]\tLoss 1.0011e-02 (1.0011e-02)\n",
      "Epoch: [4][10/51]\tLoss 1.0006e-02 (1.0009e-02)\n",
      "Epoch: [4][20/51]\tLoss 1.0003e-02 (1.0008e-02)\n",
      "Epoch: [4][30/51]\tLoss 1.0003e-02 (1.0008e-02)\n",
      "Epoch: [4][40/51]\tLoss 1.0001e-02 (1.0008e-02)\n",
      "Epoch: [4][50/51]\tLoss 1.0001e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/51]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [5][10/51]\tLoss 1.0045e-02 (1.0009e-02)\n",
      "Epoch: [5][20/51]\tLoss 1.0002e-02 (1.0006e-02)\n",
      "Epoch: [5][30/51]\tLoss 1.0004e-02 (1.0006e-02)\n",
      "Epoch: [5][40/51]\tLoss 1.0003e-02 (1.0006e-02)\n",
      "Epoch: [5][50/51]\tLoss 1.0027e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/51]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [6][10/51]\tLoss 1.0006e-02 (1.0004e-02)\n",
      "Epoch: [6][20/51]\tLoss 1.0041e-02 (1.0006e-02)\n",
      "Epoch: [6][30/51]\tLoss 1.0003e-02 (1.0005e-02)\n",
      "Epoch: [6][40/51]\tLoss 1.0029e-02 (1.0005e-02)\n",
      "Epoch: [6][50/51]\tLoss 1.0004e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/51]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [7][10/51]\tLoss 1.0002e-02 (1.0006e-02)\n",
      "Epoch: [7][20/51]\tLoss 1.0004e-02 (1.0006e-02)\n",
      "Epoch: [7][30/51]\tLoss 1.0002e-02 (1.0006e-02)\n",
      "Epoch: [7][40/51]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "Epoch: [7][50/51]\tLoss 1.0006e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [8][10/51]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][20/51]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][30/51]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][40/51]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [8][50/51]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/51]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][10/51]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][20/51]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [9][30/51]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [9][40/51]\tLoss 1.0007e-02 (1.0002e-02)\n",
      "Epoch: [9][50/51]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/51]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [10][10/51]\tLoss 1.0006e-02 (1.0002e-02)\n",
      "Epoch: [10][20/51]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][30/51]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [10][40/51]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][50/51]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/51]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [11][10/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][20/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][30/51]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][40/51]\tLoss 1.0006e-02 (1.0002e-02)\n",
      "Epoch: [11][50/51]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/51]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [12][10/51]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][20/51]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [12][30/51]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][40/51]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][50/51]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/51]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][10/51]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][20/51]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][30/51]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [13][40/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][50/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/51]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [14][10/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][20/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][30/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][40/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [14][50/51]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/51]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][10/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][20/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][30/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][40/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [15][50/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][10/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][30/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][40/51]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [16][50/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][20/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][30/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][40/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][50/51]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][10/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][20/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/51]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [18][40/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][50/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/51]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][10/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][40/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][50/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/51]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][10/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][20/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][30/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][40/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][50/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/51]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][10/51]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [21][20/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][30/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][40/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][50/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][10/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][20/51]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [22][30/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][40/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][50/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/51]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][20/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][30/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][40/51]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [23][50/51]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][10/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][20/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][30/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][40/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][50/51]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][10/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][20/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][30/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][40/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][50/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/51]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][20/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][30/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][40/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][50/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][10/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][20/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][30/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][40/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][50/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/51]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][10/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][20/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][30/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][50/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/51]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][10/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][20/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][30/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][40/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][50/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][10/51]\tLoss 9.9999e-03 (1.0001e-02)\n",
      "Epoch: [30][20/51]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][30/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][50/51]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/52]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/163]\n",
      "Fill TS Repository [100/163]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries E-7\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5138 - Test samples size: 8110\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/E-7/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/102]\tTotal Loss -2.3005e+00 (-2.3005e+00)\tConsistency Loss 2.2587e+00 (2.2587e+00)\tInconsistency Loss 1.1035e-01 (1.1035e-01)\tEntropy 2.2796e+00 (2.2796e+00)\n",
      "Epoch: [1][100/102]\tTotal Loss -2.3495e+00 (-2.3602e+00)\tConsistency Loss 1.7264e+00 (1.8306e+00)\tInconsistency Loss 2.0013e-01 (1.7901e-01)\tEntropy 2.0379e+00 (2.0954e+00)\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/102]\tTotal Loss -2.3892e+00 (-2.3892e+00)\tConsistency Loss 1.7175e+00 (1.7175e+00)\tInconsistency Loss 1.9743e-01 (1.9743e-01)\tEntropy 2.0533e+00 (2.0533e+00)\n",
      "Epoch: [2][100/102]\tTotal Loss -2.3728e+00 (-2.3754e+00)\tConsistency Loss 1.6929e+00 (1.7254e+00)\tInconsistency Loss 2.0258e-01 (1.9653e-01)\tEntropy 2.0328e+00 (2.0504e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/102]\tTotal Loss -2.3768e+00 (-2.3768e+00)\tConsistency Loss 1.7088e+00 (1.7088e+00)\tInconsistency Loss 2.0122e-01 (2.0122e-01)\tEntropy 2.0428e+00 (2.0428e+00)\n",
      "Epoch: [3][100/102]\tTotal Loss -2.3788e+00 (-2.3773e+00)\tConsistency Loss 1.7277e+00 (1.7190e+00)\tInconsistency Loss 1.9572e-01 (1.9745e-01)\tEntropy 2.0532e+00 (2.0482e+00)\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/102]\tTotal Loss -2.3685e+00 (-2.3685e+00)\tConsistency Loss 1.7142e+00 (1.7142e+00)\tInconsistency Loss 1.9923e-01 (1.9923e-01)\tEntropy 2.0414e+00 (2.0414e+00)\n",
      "Epoch: [4][100/102]\tTotal Loss -2.3747e+00 (-2.3768e+00)\tConsistency Loss 1.7130e+00 (1.7195e+00)\tInconsistency Loss 1.9985e-01 (1.9756e-01)\tEntropy 2.0439e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/102]\tTotal Loss -2.3714e+00 (-2.3714e+00)\tConsistency Loss 1.7102e+00 (1.7102e+00)\tInconsistency Loss 1.9989e-01 (1.9989e-01)\tEntropy 2.0408e+00 (2.0408e+00)\n",
      "Epoch: [5][100/102]\tTotal Loss -2.3762e+00 (-2.3767e+00)\tConsistency Loss 1.7316e+00 (1.7199e+00)\tInconsistency Loss 1.9526e-01 (1.9740e-01)\tEntropy 2.0539e+00 (2.0483e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/102]\tTotal Loss -2.3775e+00 (-2.3775e+00)\tConsistency Loss 1.7213e+00 (1.7213e+00)\tInconsistency Loss 1.9627e-01 (1.9627e-01)\tEntropy 2.0494e+00 (2.0494e+00)\n",
      "Epoch: [6][100/102]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7197e+00 (1.7187e+00)\tInconsistency Loss 1.9770e-01 (1.9763e-01)\tEntropy 2.0483e+00 (2.0479e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/102]\tTotal Loss -2.3774e+00 (-2.3774e+00)\tConsistency Loss 1.7182e+00 (1.7182e+00)\tInconsistency Loss 1.9754e-01 (1.9754e-01)\tEntropy 2.0478e+00 (2.0478e+00)\n",
      "Epoch: [7][100/102]\tTotal Loss -2.3770e+00 (-2.3771e+00)\tConsistency Loss 1.7201e+00 (1.7183e+00)\tInconsistency Loss 1.9733e-01 (1.9769e-01)\tEntropy 2.0486e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/102]\tTotal Loss -2.3775e+00 (-2.3775e+00)\tConsistency Loss 1.7208e+00 (1.7208e+00)\tInconsistency Loss 1.9714e-01 (1.9714e-01)\tEntropy 2.0492e+00 (2.0492e+00)\n",
      "Epoch: [8][100/102]\tTotal Loss -2.3752e+00 (-2.3772e+00)\tConsistency Loss 1.7167e+00 (1.7183e+00)\tInconsistency Loss 1.9833e-01 (1.9770e-01)\tEntropy 2.0459e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/102]\tTotal Loss -2.3783e+00 (-2.3783e+00)\tConsistency Loss 1.7148e+00 (1.7148e+00)\tInconsistency Loss 1.9811e-01 (1.9811e-01)\tEntropy 2.0466e+00 (2.0466e+00)\n",
      "Epoch: [9][100/102]\tTotal Loss -2.3772e+00 (-2.3771e+00)\tConsistency Loss 1.7176e+00 (1.7182e+00)\tInconsistency Loss 1.9787e-01 (1.9769e-01)\tEntropy 2.0474e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/102]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7166e+00 (1.7166e+00)\tInconsistency Loss 1.9812e-01 (1.9812e-01)\tEntropy 2.0469e+00 (2.0469e+00)\n",
      "Epoch: [10][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [11][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [12][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/102]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "E-8\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2680/8332 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/E-8/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0048e+00 (1.0048e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4828e-01 (6.2443e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2142e-01 (4.2424e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2256e-02 (3.1033e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4813e-02 (2.4068e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0033e-02 (1.9557e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0023e-02 (1.0023e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0018e-02 (1.0035e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0032e-02 (1.0031e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0018e-02 (1.0028e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0004e-02 (1.0025e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0006e-02 (1.0025e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0010e-02 (1.0010e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0028e-02 (1.0020e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0007e-02 (1.0014e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0010e-02 (1.0012e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0004e-02 (1.0012e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0015e-02 (1.0011e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0021e-02 (1.0021e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0002e-02 (1.0009e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0013e-02 (1.0009e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0004e-02 (1.0007e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0001e-02 (1.0007e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0003e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0003e-02 (1.0009e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0003e-02 (1.0007e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0003e-02 (1.0007e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0002e-02 (1.0008e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0002e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0072e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0007e-02 (1.0004e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0010e-02 (1.0004e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0007e-02 (1.0002e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0010e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0007e-02 (1.0001e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [16][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [19][40/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0008e-02 (1.0001e-02)\n",
      "Epoch: [20][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/167]\n",
      "Fill TS Repository [100/167]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries E-8\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5360 - Test samples size: 8332\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/E-8/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/107]\tTotal Loss -2.3041e+00 (-2.3041e+00)\tConsistency Loss 2.2564e+00 (2.2564e+00)\tInconsistency Loss 1.1044e-01 (1.1044e-01)\tEntropy 2.2802e+00 (2.2802e+00)\n",
      "Epoch: [1][100/107]\tTotal Loss -2.3852e+00 (-2.3585e+00)\tConsistency Loss 1.7003e+00 (1.8520e+00)\tInconsistency Loss 2.0277e-01 (1.7522e-01)\tEntropy 2.0427e+00 (2.1053e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/107]\tTotal Loss -2.3607e+00 (-2.3607e+00)\tConsistency Loss 1.7015e+00 (1.7015e+00)\tInconsistency Loss 2.0044e-01 (2.0044e-01)\tEntropy 2.0311e+00 (2.0311e+00)\n",
      "Epoch: [2][100/107]\tTotal Loss -2.3792e+00 (-2.3755e+00)\tConsistency Loss 1.6929e+00 (1.7255e+00)\tInconsistency Loss 2.0146e-01 (1.9609e-01)\tEntropy 2.0360e+00 (2.0505e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/107]\tTotal Loss -2.3747e+00 (-2.3747e+00)\tConsistency Loss 1.7118e+00 (1.7118e+00)\tInconsistency Loss 1.9686e-01 (1.9686e-01)\tEntropy 2.0432e+00 (2.0432e+00)\n",
      "Epoch: [3][100/107]\tTotal Loss -2.3864e+00 (-2.3761e+00)\tConsistency Loss 1.7303e+00 (1.7244e+00)\tInconsistency Loss 1.9456e-01 (1.9652e-01)\tEntropy 2.0584e+00 (2.0502e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/107]\tTotal Loss -2.3781e+00 (-2.3781e+00)\tConsistency Loss 1.6937e+00 (1.6937e+00)\tInconsistency Loss 2.0267e-01 (2.0267e-01)\tEntropy 2.0359e+00 (2.0359e+00)\n",
      "Epoch: [4][100/107]\tTotal Loss -2.3730e+00 (-2.3769e+00)\tConsistency Loss 1.7296e+00 (1.7188e+00)\tInconsistency Loss 1.9627e-01 (1.9761e-01)\tEntropy 2.0513e+00 (2.0478e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/107]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7223e+00 (1.7223e+00)\tInconsistency Loss 1.9742e-01 (1.9742e-01)\tEntropy 2.0498e+00 (2.0498e+00)\n",
      "Epoch: [5][100/107]\tTotal Loss -2.3772e+00 (-2.3770e+00)\tConsistency Loss 1.7157e+00 (1.7184e+00)\tInconsistency Loss 1.9825e-01 (1.9773e-01)\tEntropy 2.0464e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/107]\tTotal Loss -2.3777e+00 (-2.3777e+00)\tConsistency Loss 1.7267e+00 (1.7267e+00)\tInconsistency Loss 1.9589e-01 (1.9589e-01)\tEntropy 2.0522e+00 (2.0522e+00)\n",
      "Epoch: [6][100/107]\tTotal Loss -2.3782e+00 (-2.3771e+00)\tConsistency Loss 1.7187e+00 (1.7192e+00)\tInconsistency Loss 1.9757e-01 (1.9751e-01)\tEntropy 2.0484e+00 (2.0481e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/107]\tTotal Loss -2.3767e+00 (-2.3767e+00)\tConsistency Loss 1.7175e+00 (1.7175e+00)\tInconsistency Loss 1.9794e-01 (1.9794e-01)\tEntropy 2.0471e+00 (2.0471e+00)\n",
      "Epoch: [7][100/107]\tTotal Loss -2.3773e+00 (-2.3771e+00)\tConsistency Loss 1.7189e+00 (1.7183e+00)\tInconsistency Loss 1.9755e-01 (1.9769e-01)\tEntropy 2.0481e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/107]\tTotal Loss -2.3769e+00 (-2.3769e+00)\tConsistency Loss 1.7174e+00 (1.7174e+00)\tInconsistency Loss 1.9790e-01 (1.9790e-01)\tEntropy 2.0472e+00 (2.0472e+00)\n",
      "Epoch: [8][100/107]\tTotal Loss -2.3773e+00 (-2.3772e+00)\tConsistency Loss 1.7185e+00 (1.7181e+00)\tInconsistency Loss 1.9770e-01 (1.9773e-01)\tEntropy 2.0479e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7186e+00 (1.7186e+00)\tInconsistency Loss 1.9764e-01 (1.9764e-01)\tEntropy 2.0479e+00 (2.0479e+00)\n",
      "Epoch: [9][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7179e+00 (1.7181e+00)\tInconsistency Loss 1.9777e-01 (1.9772e-01)\tEntropy 2.0475e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7182e+00)\tInconsistency Loss 1.9770e-01 (1.9770e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [10][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [11][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [12][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "E-9\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2680/8102 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/E-9/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0034e+00 (1.0034e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4830e-01 (6.2432e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2147e-01 (4.2422e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2266e-02 (3.1032e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4782e-02 (2.4068e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0035e-02 (1.9557e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0027e-02 (1.0027e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0022e-02 (1.0035e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0027e-02 (1.0029e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0016e-02 (1.0026e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0004e-02 (1.0024e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0009e-02 (1.0024e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0031e-02 (1.0023e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0013e-02 (1.0016e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0013e-02 (1.0013e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0005e-02 (1.0012e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0013e-02 (1.0012e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0018e-02 (1.0018e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0004e-02 (1.0008e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0014e-02 (1.0008e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0004e-02 (1.0007e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0001e-02 (1.0007e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0006e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0002e-02 (1.0010e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0004e-02 (1.0007e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0003e-02 (1.0008e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0001e-02 (1.0008e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0003e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0066e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0006e-02 (1.0004e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0006e-02 (1.0004e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0006e-02 (1.0002e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0005e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0008e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0010e-02 (1.0001e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [16][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][40/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0009e-02 (1.0001e-02)\n",
      "Epoch: [20][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0005e-02 (1.0001e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/163]\n",
      "Fill TS Repository [100/163]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries E-9\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5360 - Test samples size: 8102\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/E-9/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/107]\tTotal Loss -2.3046e+00 (-2.3046e+00)\tConsistency Loss 2.2560e+00 (2.2560e+00)\tInconsistency Loss 1.1044e-01 (1.1044e-01)\tEntropy 2.2803e+00 (2.2803e+00)\n",
      "Epoch: [1][100/107]\tTotal Loss -2.3861e+00 (-2.3595e+00)\tConsistency Loss 1.7302e+00 (1.8409e+00)\tInconsistency Loss 1.9543e-01 (1.7741e-01)\tEntropy 2.0581e+00 (2.1002e+00)\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/107]\tTotal Loss -2.3583e+00 (-2.3583e+00)\tConsistency Loss 1.7152e+00 (1.7152e+00)\tInconsistency Loss 2.0028e-01 (2.0028e-01)\tEntropy 2.0368e+00 (2.0368e+00)\n",
      "Epoch: [2][100/107]\tTotal Loss -2.3684e+00 (-2.3763e+00)\tConsistency Loss 1.6897e+00 (1.7215e+00)\tInconsistency Loss 2.0246e-01 (1.9723e-01)\tEntropy 2.0290e+00 (2.0489e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/107]\tTotal Loss -2.3855e+00 (-2.3855e+00)\tConsistency Loss 1.7147e+00 (1.7147e+00)\tInconsistency Loss 1.9711e-01 (1.9711e-01)\tEntropy 2.0501e+00 (2.0501e+00)\n",
      "Epoch: [3][100/107]\tTotal Loss -2.3829e+00 (-2.3768e+00)\tConsistency Loss 1.7096e+00 (1.7201e+00)\tInconsistency Loss 1.9893e-01 (1.9734e-01)\tEntropy 2.0463e+00 (2.0485e+00)\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/107]\tTotal Loss -2.3779e+00 (-2.3779e+00)\tConsistency Loss 1.7109e+00 (1.7109e+00)\tInconsistency Loss 1.9850e-01 (1.9850e-01)\tEntropy 2.0444e+00 (2.0444e+00)\n",
      "Epoch: [4][100/107]\tTotal Loss -2.3779e+00 (-2.3764e+00)\tConsistency Loss 1.7183e+00 (1.7203e+00)\tInconsistency Loss 1.9710e-01 (1.9740e-01)\tEntropy 2.0481e+00 (2.0484e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/107]\tTotal Loss -2.3742e+00 (-2.3742e+00)\tConsistency Loss 1.7159e+00 (1.7159e+00)\tInconsistency Loss 1.9867e-01 (1.9867e-01)\tEntropy 2.0450e+00 (2.0450e+00)\n",
      "Epoch: [5][100/107]\tTotal Loss -2.3772e+00 (-2.3769e+00)\tConsistency Loss 1.7142e+00 (1.7189e+00)\tInconsistency Loss 1.9849e-01 (1.9759e-01)\tEntropy 2.0457e+00 (2.0479e+00)\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/107]\tTotal Loss -2.3776e+00 (-2.3776e+00)\tConsistency Loss 1.7232e+00 (1.7232e+00)\tInconsistency Loss 1.9666e-01 (1.9666e-01)\tEntropy 2.0504e+00 (2.0504e+00)\n",
      "Epoch: [6][100/107]\tTotal Loss -2.3793e+00 (-2.3772e+00)\tConsistency Loss 1.7219e+00 (1.7184e+00)\tInconsistency Loss 1.9698e-01 (1.9765e-01)\tEntropy 2.0506e+00 (2.0478e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/107]\tTotal Loss -2.3778e+00 (-2.3778e+00)\tConsistency Loss 1.7167e+00 (1.7167e+00)\tInconsistency Loss 1.9813e-01 (1.9813e-01)\tEntropy 2.0473e+00 (2.0473e+00)\n",
      "Epoch: [7][100/107]\tTotal Loss -2.3770e+00 (-2.3771e+00)\tConsistency Loss 1.7176e+00 (1.7184e+00)\tInconsistency Loss 1.9787e-01 (1.9769e-01)\tEntropy 2.0473e+00 (2.0478e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/107]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7197e+00 (1.7197e+00)\tInconsistency Loss 1.9741e-01 (1.9741e-01)\tEntropy 2.0485e+00 (2.0485e+00)\n",
      "Epoch: [8][100/107]\tTotal Loss -2.3770e+00 (-2.3771e+00)\tConsistency Loss 1.7187e+00 (1.7183e+00)\tInconsistency Loss 1.9757e-01 (1.9770e-01)\tEntropy 2.0479e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/107]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7167e+00 (1.7167e+00)\tInconsistency Loss 1.9796e-01 (1.9796e-01)\tEntropy 2.0470e+00 (2.0470e+00)\n",
      "Epoch: [9][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [10][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [11][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [12][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "E-10\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2680/8305 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/E-10/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0044e+00 (1.0044e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4828e-01 (6.2444e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2142e-01 (4.2426e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2246e-02 (3.1034e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4762e-02 (2.4068e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0033e-02 (1.9557e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0025e-02 (1.0025e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0024e-02 (1.0036e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0046e-02 (1.0034e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0027e-02 (1.0031e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0007e-02 (1.0028e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0008e-02 (1.0028e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0010e-02 (1.0010e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0042e-02 (1.0024e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0018e-02 (1.0018e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0015e-02 (1.0015e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0007e-02 (1.0014e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0014e-02 (1.0013e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0024e-02 (1.0024e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0007e-02 (1.0010e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0015e-02 (1.0010e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0005e-02 (1.0009e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0001e-02 (1.0009e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0006e-02 (1.0010e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0003e-02 (1.0012e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0004e-02 (1.0009e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0004e-02 (1.0010e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0002e-02 (1.0010e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0005e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0005e-02 (1.0004e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0002e-02 (1.0005e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0053e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0007e-02 (1.0005e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0009e-02 (1.0005e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0005e-02 (1.0003e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0007e-02 (1.0002e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0013e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0011e-02 (1.0001e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [16][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0011e-02 (1.0001e-02)\n",
      "Epoch: [20][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/167]\n",
      "Fill TS Repository [100/167]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries E-10\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5360 - Test samples size: 8305\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/E-10/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/107]\tTotal Loss -2.3053e+00 (-2.3053e+00)\tConsistency Loss 2.2551e+00 (2.2551e+00)\tInconsistency Loss 1.1048e-01 (1.1048e-01)\tEntropy 2.2802e+00 (2.2802e+00)\n",
      "Epoch: [1][100/107]\tTotal Loss -2.3757e+00 (-2.3556e+00)\tConsistency Loss 1.7264e+00 (1.8684e+00)\tInconsistency Loss 1.9589e-01 (1.7295e-01)\tEntropy 2.0510e+00 (2.1120e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/107]\tTotal Loss -2.3661e+00 (-2.3661e+00)\tConsistency Loss 1.7100e+00 (1.7100e+00)\tInconsistency Loss 1.9976e-01 (1.9976e-01)\tEntropy 2.0380e+00 (2.0380e+00)\n",
      "Epoch: [2][100/107]\tTotal Loss -2.3779e+00 (-2.3761e+00)\tConsistency Loss 1.7030e+00 (1.7236e+00)\tInconsistency Loss 1.9921e-01 (1.9655e-01)\tEntropy 2.0405e+00 (2.0498e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/107]\tTotal Loss -2.3741e+00 (-2.3741e+00)\tConsistency Loss 1.7216e+00 (1.7216e+00)\tInconsistency Loss 1.9924e-01 (1.9924e-01)\tEntropy 2.0478e+00 (2.0478e+00)\n",
      "Epoch: [3][100/107]\tTotal Loss -2.3747e+00 (-2.3770e+00)\tConsistency Loss 1.7139e+00 (1.7194e+00)\tInconsistency Loss 1.9872e-01 (1.9749e-01)\tEntropy 2.0443e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/107]\tTotal Loss -2.3722e+00 (-2.3722e+00)\tConsistency Loss 1.7231e+00 (1.7231e+00)\tInconsistency Loss 1.9653e-01 (1.9653e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [4][100/107]\tTotal Loss -2.3774e+00 (-2.3766e+00)\tConsistency Loss 1.7157e+00 (1.7210e+00)\tInconsistency Loss 1.9823e-01 (1.9719e-01)\tEntropy 2.0466e+00 (2.0488e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/107]\tTotal Loss -2.3759e+00 (-2.3759e+00)\tConsistency Loss 1.7191e+00 (1.7191e+00)\tInconsistency Loss 1.9740e-01 (1.9740e-01)\tEntropy 2.0475e+00 (2.0475e+00)\n",
      "Epoch: [5][100/107]\tTotal Loss -2.3767e+00 (-2.3769e+00)\tConsistency Loss 1.7296e+00 (1.7191e+00)\tInconsistency Loss 1.9522e-01 (1.9758e-01)\tEntropy 2.0532e+00 (2.0480e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7112e+00 (1.7112e+00)\tInconsistency Loss 1.9916e-01 (1.9916e-01)\tEntropy 2.0441e+00 (2.0441e+00)\n",
      "Epoch: [6][100/107]\tTotal Loss -2.3812e+00 (-2.3774e+00)\tConsistency Loss 1.7037e+00 (1.7178e+00)\tInconsistency Loss 2.0109e-01 (1.9778e-01)\tEntropy 2.0424e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7205e+00 (1.7205e+00)\tInconsistency Loss 1.9721e-01 (1.9721e-01)\tEntropy 2.0488e+00 (2.0488e+00)\n",
      "Epoch: [7][100/107]\tTotal Loss -2.3775e+00 (-2.3772e+00)\tConsistency Loss 1.7179e+00 (1.7181e+00)\tInconsistency Loss 1.9770e-01 (1.9772e-01)\tEntropy 2.0477e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/107]\tTotal Loss -2.3766e+00 (-2.3766e+00)\tConsistency Loss 1.7125e+00 (1.7125e+00)\tInconsistency Loss 1.9823e-01 (1.9823e-01)\tEntropy 2.0445e+00 (2.0445e+00)\n",
      "Epoch: [8][100/107]\tTotal Loss -2.3772e+00 (-2.3771e+00)\tConsistency Loss 1.7179e+00 (1.7191e+00)\tInconsistency Loss 1.9776e-01 (1.9752e-01)\tEntropy 2.0476e+00 (2.0481e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7183e+00 (1.7183e+00)\tInconsistency Loss 1.9769e-01 (1.9769e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [9][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0476e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [10][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [11][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [12][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "E-11\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2680/8314 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/E-11/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0042e+00 (1.0042e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4829e-01 (6.2447e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2138e-01 (4.2426e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2252e-02 (3.1034e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4763e-02 (2.4068e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0041e-02 (1.9557e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0023e-02 (1.0023e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0014e-02 (1.0033e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0058e-02 (1.0031e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0018e-02 (1.0028e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0007e-02 (1.0026e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0006e-02 (1.0026e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0010e-02 (1.0010e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0038e-02 (1.0022e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0012e-02 (1.0016e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0010e-02 (1.0014e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0006e-02 (1.0013e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0016e-02 (1.0012e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0017e-02 (1.0017e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0005e-02 (1.0009e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0016e-02 (1.0009e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0004e-02 (1.0008e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0001e-02 (1.0008e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0007e-02 (1.0010e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0002e-02 (1.0010e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0004e-02 (1.0007e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0002e-02 (1.0008e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0003e-02 (1.0009e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0004e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0078e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0005e-02 (1.0004e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0011e-02 (1.0004e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0005e-02 (1.0002e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0007e-02 (1.0002e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0015e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0006e-02 (1.0001e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [16][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0012e-02 (1.0001e-02)\n",
      "Epoch: [20][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0005e-02 (1.0001e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/167]\n",
      "Fill TS Repository [100/167]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries E-11\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5360 - Test samples size: 8314\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/E-11/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/107]\tTotal Loss -2.3050e+00 (-2.3050e+00)\tConsistency Loss 2.2551e+00 (2.2551e+00)\tInconsistency Loss 1.1052e-01 (1.1052e-01)\tEntropy 2.2801e+00 (2.2801e+00)\n",
      "Epoch: [1][100/107]\tTotal Loss -2.3975e+00 (-2.3607e+00)\tConsistency Loss 1.7241e+00 (1.8366e+00)\tInconsistency Loss 1.9121e-01 (1.7850e-01)\tEntropy 2.0608e+00 (2.0987e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/107]\tTotal Loss -2.3880e+00 (-2.3880e+00)\tConsistency Loss 1.7489e+00 (1.7489e+00)\tInconsistency Loss 1.8878e-01 (1.8878e-01)\tEntropy 2.0685e+00 (2.0685e+00)\n",
      "Epoch: [2][100/107]\tTotal Loss -2.3779e+00 (-2.3753e+00)\tConsistency Loss 1.7277e+00 (1.7263e+00)\tInconsistency Loss 1.9565e-01 (1.9618e-01)\tEntropy 2.0528e+00 (2.0508e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/107]\tTotal Loss -2.3775e+00 (-2.3775e+00)\tConsistency Loss 1.7264e+00 (1.7264e+00)\tInconsistency Loss 1.9606e-01 (1.9606e-01)\tEntropy 2.0519e+00 (2.0519e+00)\n",
      "Epoch: [3][100/107]\tTotal Loss -2.3688e+00 (-2.3766e+00)\tConsistency Loss 1.6706e+00 (1.7234e+00)\tInconsistency Loss 2.0835e-01 (1.9677e-01)\tEntropy 2.0197e+00 (2.0500e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/107]\tTotal Loss -2.3731e+00 (-2.3731e+00)\tConsistency Loss 1.7262e+00 (1.7262e+00)\tInconsistency Loss 1.9389e-01 (1.9389e-01)\tEntropy 2.0496e+00 (2.0496e+00)\n",
      "Epoch: [4][100/107]\tTotal Loss -2.3743e+00 (-2.3765e+00)\tConsistency Loss 1.7450e+00 (1.7227e+00)\tInconsistency Loss 1.9193e-01 (1.9690e-01)\tEntropy 2.0596e+00 (2.0496e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/107]\tTotal Loss -2.3692e+00 (-2.3692e+00)\tConsistency Loss 1.7086e+00 (1.7086e+00)\tInconsistency Loss 2.0010e-01 (2.0010e-01)\tEntropy 2.0389e+00 (2.0389e+00)\n",
      "Epoch: [5][100/107]\tTotal Loss -2.3795e+00 (-2.3772e+00)\tConsistency Loss 1.7158e+00 (1.7179e+00)\tInconsistency Loss 1.9840e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7208e+00 (1.7208e+00)\tInconsistency Loss 1.9618e-01 (1.9618e-01)\tEntropy 2.0490e+00 (2.0490e+00)\n",
      "Epoch: [6][100/107]\tTotal Loss -2.3788e+00 (-2.3766e+00)\tConsistency Loss 1.7211e+00 (1.7209e+00)\tInconsistency Loss 1.9710e-01 (1.9725e-01)\tEntropy 2.0499e+00 (2.0488e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/107]\tTotal Loss -2.3763e+00 (-2.3763e+00)\tConsistency Loss 1.7183e+00 (1.7183e+00)\tInconsistency Loss 1.9787e-01 (1.9787e-01)\tEntropy 2.0473e+00 (2.0473e+00)\n",
      "Epoch: [7][100/107]\tTotal Loss -2.3779e+00 (-2.3771e+00)\tConsistency Loss 1.7201e+00 (1.7185e+00)\tInconsistency Loss 1.9738e-01 (1.9767e-01)\tEntropy 2.0490e+00 (2.0478e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/107]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7172e+00 (1.7172e+00)\tInconsistency Loss 1.9764e-01 (1.9764e-01)\tEntropy 2.0472e+00 (2.0472e+00)\n",
      "Epoch: [8][100/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7172e+00 (1.7185e+00)\tInconsistency Loss 1.9794e-01 (1.9764e-01)\tEntropy 2.0471e+00 (2.0478e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7189e+00 (1.7189e+00)\tInconsistency Loss 1.9757e-01 (1.9757e-01)\tEntropy 2.0481e+00 (2.0481e+00)\n",
      "Epoch: [9][100/107]\tTotal Loss -2.3771e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7183e+00)\tInconsistency Loss 1.9771e-01 (1.9770e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7184e+00 (1.7184e+00)\tInconsistency Loss 1.9766e-01 (1.9766e-01)\tEntropy 2.0478e+00 (2.0478e+00)\n",
      "Epoch: [10][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7182e+00)\tInconsistency Loss 1.9772e-01 (1.9771e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7182e+00)\tInconsistency Loss 1.9771e-01 (1.9771e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [11][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [12][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "E-12\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2680/8312 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/E-12/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0042e+00 (1.0042e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4832e-01 (6.2444e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2138e-01 (4.2425e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2255e-02 (3.1033e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4789e-02 (2.4068e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0031e-02 (1.9557e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0021e-02 (1.0021e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0017e-02 (1.0035e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0047e-02 (1.0031e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0019e-02 (1.0029e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0006e-02 (1.0026e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0006e-02 (1.0026e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0009e-02 (1.0009e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0037e-02 (1.0024e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0012e-02 (1.0017e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0016e-02 (1.0014e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0006e-02 (1.0013e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0023e-02 (1.0012e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0023e-02 (1.0023e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0005e-02 (1.0010e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0013e-02 (1.0009e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0004e-02 (1.0008e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0001e-02 (1.0008e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0005e-02 (1.0010e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0002e-02 (1.0011e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0003e-02 (1.0008e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0003e-02 (1.0009e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0002e-02 (1.0009e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0006e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0096e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0007e-02 (1.0005e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0014e-02 (1.0004e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0009e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0005e-02 (1.0001e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [16][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][40/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0011e-02 (1.0001e-02)\n",
      "Epoch: [20][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/167]\n",
      "Fill TS Repository [100/167]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries E-12\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5360 - Test samples size: 8312\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/E-12/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/107]\tTotal Loss -2.3047e+00 (-2.3047e+00)\tConsistency Loss 2.2557e+00 (2.2557e+00)\tInconsistency Loss 1.1048e-01 (1.1048e-01)\tEntropy 2.2802e+00 (2.2802e+00)\n",
      "Epoch: [1][100/107]\tTotal Loss -2.3647e+00 (-2.3564e+00)\tConsistency Loss 1.7455e+00 (1.8718e+00)\tInconsistency Loss 1.9352e-01 (1.7235e-01)\tEntropy 2.0551e+00 (2.1141e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/107]\tTotal Loss -2.3769e+00 (-2.3769e+00)\tConsistency Loss 1.6942e+00 (1.6942e+00)\tInconsistency Loss 2.0368e-01 (2.0368e-01)\tEntropy 2.0356e+00 (2.0356e+00)\n",
      "Epoch: [2][100/107]\tTotal Loss -2.3754e+00 (-2.3767e+00)\tConsistency Loss 1.7064e+00 (1.7204e+00)\tInconsistency Loss 1.9912e-01 (1.9725e-01)\tEntropy 2.0409e+00 (2.0486e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/107]\tTotal Loss -2.3758e+00 (-2.3758e+00)\tConsistency Loss 1.7161e+00 (1.7161e+00)\tInconsistency Loss 1.9865e-01 (1.9865e-01)\tEntropy 2.0460e+00 (2.0460e+00)\n",
      "Epoch: [3][100/107]\tTotal Loss -2.3858e+00 (-2.3768e+00)\tConsistency Loss 1.7186e+00 (1.7207e+00)\tInconsistency Loss 1.9763e-01 (1.9706e-01)\tEntropy 2.0522e+00 (2.0488e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/107]\tTotal Loss -2.3792e+00 (-2.3792e+00)\tConsistency Loss 1.7146e+00 (1.7146e+00)\tInconsistency Loss 1.9770e-01 (1.9770e-01)\tEntropy 2.0469e+00 (2.0469e+00)\n",
      "Epoch: [4][100/107]\tTotal Loss -2.3706e+00 (-2.3764e+00)\tConsistency Loss 1.7198e+00 (1.7209e+00)\tInconsistency Loss 1.9841e-01 (1.9721e-01)\tEntropy 2.0452e+00 (2.0487e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/107]\tTotal Loss -2.3754e+00 (-2.3754e+00)\tConsistency Loss 1.7265e+00 (1.7265e+00)\tInconsistency Loss 1.9639e-01 (1.9639e-01)\tEntropy 2.0509e+00 (2.0509e+00)\n",
      "Epoch: [5][100/107]\tTotal Loss -2.3775e+00 (-2.3776e+00)\tConsistency Loss 1.7356e+00 (1.7173e+00)\tInconsistency Loss 1.9379e-01 (1.9795e-01)\tEntropy 2.0566e+00 (2.0475e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/107]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7234e+00 (1.7234e+00)\tInconsistency Loss 1.9811e-01 (1.9811e-01)\tEntropy 2.0504e+00 (2.0504e+00)\n",
      "Epoch: [6][100/107]\tTotal Loss -2.3817e+00 (-2.3772e+00)\tConsistency Loss 1.7146e+00 (1.7184e+00)\tInconsistency Loss 1.9903e-01 (1.9772e-01)\tEntropy 2.0482e+00 (2.0478e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/107]\tTotal Loss -2.3781e+00 (-2.3781e+00)\tConsistency Loss 1.7179e+00 (1.7179e+00)\tInconsistency Loss 1.9707e-01 (1.9707e-01)\tEntropy 2.0480e+00 (2.0480e+00)\n",
      "Epoch: [7][100/107]\tTotal Loss -2.3747e+00 (-2.3772e+00)\tConsistency Loss 1.7379e+00 (1.7191e+00)\tInconsistency Loss 1.9348e-01 (1.9751e-01)\tEntropy 2.0563e+00 (2.0481e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/107]\tTotal Loss -2.3737e+00 (-2.3737e+00)\tConsistency Loss 1.7122e+00 (1.7122e+00)\tInconsistency Loss 1.9873e-01 (1.9873e-01)\tEntropy 2.0429e+00 (2.0429e+00)\n",
      "Epoch: [8][100/107]\tTotal Loss -2.3772e+00 (-2.3773e+00)\tConsistency Loss 1.7263e+00 (1.7178e+00)\tInconsistency Loss 1.9594e-01 (1.9772e-01)\tEntropy 2.0518e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/107]\tTotal Loss -2.3782e+00 (-2.3782e+00)\tConsistency Loss 1.7041e+00 (1.7041e+00)\tInconsistency Loss 2.0099e-01 (2.0099e-01)\tEntropy 2.0411e+00 (2.0411e+00)\n",
      "Epoch: [9][100/107]\tTotal Loss -2.3757e+00 (-2.3770e+00)\tConsistency Loss 1.7269e+00 (1.7187e+00)\tInconsistency Loss 1.9591e-01 (1.9764e-01)\tEntropy 2.0513e+00 (2.0479e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/107]\tTotal Loss -2.3761e+00 (-2.3761e+00)\tConsistency Loss 1.7223e+00 (1.7223e+00)\tInconsistency Loss 1.9687e-01 (1.9687e-01)\tEntropy 2.0492e+00 (2.0492e+00)\n",
      "Epoch: [10][100/107]\tTotal Loss -2.3772e+00 (-2.3770e+00)\tConsistency Loss 1.7193e+00 (1.7189e+00)\tInconsistency Loss 1.9759e-01 (1.9761e-01)\tEntropy 2.0482e+00 (2.0479e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/107]\tTotal Loss -2.3774e+00 (-2.3774e+00)\tConsistency Loss 1.7177e+00 (1.7177e+00)\tInconsistency Loss 1.9789e-01 (1.9789e-01)\tEntropy 2.0475e+00 (2.0475e+00)\n",
      "Epoch: [11][100/107]\tTotal Loss -2.3768e+00 (-2.3772e+00)\tConsistency Loss 1.7188e+00 (1.7182e+00)\tInconsistency Loss 1.9769e-01 (1.9771e-01)\tEntropy 2.0478e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/107]\tTotal Loss -2.3769e+00 (-2.3769e+00)\tConsistency Loss 1.7184e+00 (1.7184e+00)\tInconsistency Loss 1.9776e-01 (1.9776e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [12][100/107]\tTotal Loss -2.3769e+00 (-2.3771e+00)\tConsistency Loss 1.7184e+00 (1.7182e+00)\tInconsistency Loss 1.9777e-01 (1.9771e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/107]\tTotal Loss -2.3774e+00 (-2.3774e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9766e-01 (1.9766e-01)\tEntropy 2.0478e+00 (2.0478e+00)\n",
      "Epoch: [13][100/107]\tTotal Loss -2.3772e+00 (-2.3771e+00)\tConsistency Loss 1.7178e+00 (1.7182e+00)\tInconsistency Loss 1.9780e-01 (1.9772e-01)\tEntropy 2.0475e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7183e+00 (1.7183e+00)\tInconsistency Loss 1.9768e-01 (1.9768e-01)\tEntropy 2.0478e+00 (2.0478e+00)\n",
      "Epoch: [14][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "E-13\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2680/8440 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/E-13/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0044e+00 (1.0044e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4821e-01 (6.2435e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2149e-01 (4.2422e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2240e-02 (3.1033e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4761e-02 (2.4068e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0012e-02 (1.9557e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0042e-02 (1.0042e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0013e-02 (1.0029e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0034e-02 (1.0028e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0015e-02 (1.0024e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0006e-02 (1.0022e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0006e-02 (1.0020e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0014e-02 (1.0014e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0021e-02 (1.0017e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0013e-02 (1.0013e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0009e-02 (1.0012e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0011e-02 (1.0012e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0012e-02 (1.0011e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0008e-02 (1.0008e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0006e-02 (1.0008e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0003e-02 (1.0007e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0003e-02 (1.0007e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0005e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0003e-02 (1.0010e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0002e-02 (1.0008e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0004e-02 (1.0007e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0003e-02 (1.0007e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0016e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0005e-02 (1.0006e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0020e-02 (1.0005e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0006e-02 (1.0005e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0020e-02 (1.0004e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0013e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0034e-02 (1.0002e-02)\n",
      "Epoch: [20][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0005e-02 (1.0001e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [30][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/169]\n",
      "Fill TS Repository [100/169]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries E-13\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5360 - Test samples size: 8440\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/E-13/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/107]\tTotal Loss -2.3042e+00 (-2.3042e+00)\tConsistency Loss 2.2553e+00 (2.2553e+00)\tInconsistency Loss 1.1057e-01 (1.1057e-01)\tEntropy 2.2798e+00 (2.2798e+00)\n",
      "Epoch: [1][100/107]\tTotal Loss -2.3318e+00 (-2.3605e+00)\tConsistency Loss 1.7106e+00 (1.8340e+00)\tInconsistency Loss 2.0613e-01 (1.7916e-01)\tEntropy 2.0212e+00 (2.0973e+00)\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/107]\tTotal Loss -2.3454e+00 (-2.3454e+00)\tConsistency Loss 1.7451e+00 (1.7451e+00)\tInconsistency Loss 1.9781e-01 (1.9781e-01)\tEntropy 2.0452e+00 (2.0452e+00)\n",
      "Epoch: [2][100/107]\tTotal Loss -2.3682e+00 (-2.3752e+00)\tConsistency Loss 1.6691e+00 (1.7280e+00)\tInconsistency Loss 2.0727e-01 (1.9617e-01)\tEntropy 2.0186e+00 (2.0516e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/107]\tTotal Loss -2.3678e+00 (-2.3678e+00)\tConsistency Loss 1.6892e+00 (1.6892e+00)\tInconsistency Loss 2.0427e-01 (2.0427e-01)\tEntropy 2.0285e+00 (2.0285e+00)\n",
      "Epoch: [3][100/107]\tTotal Loss -2.3946e+00 (-2.3772e+00)\tConsistency Loss 1.7184e+00 (1.7210e+00)\tInconsistency Loss 1.9997e-01 (1.9730e-01)\tEntropy 2.0565e+00 (2.0491e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/107]\tTotal Loss -2.3700e+00 (-2.3700e+00)\tConsistency Loss 1.7123e+00 (1.7123e+00)\tInconsistency Loss 1.9966e-01 (1.9966e-01)\tEntropy 2.0412e+00 (2.0412e+00)\n",
      "Epoch: [4][100/107]\tTotal Loss -2.3657e+00 (-2.3761e+00)\tConsistency Loss 1.7384e+00 (1.7229e+00)\tInconsistency Loss 1.9313e-01 (1.9667e-01)\tEntropy 2.0521e+00 (2.0495e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/107]\tTotal Loss -2.3660e+00 (-2.3660e+00)\tConsistency Loss 1.7226e+00 (1.7226e+00)\tInconsistency Loss 1.9873e-01 (1.9873e-01)\tEntropy 2.0443e+00 (2.0443e+00)\n",
      "Epoch: [5][100/107]\tTotal Loss -2.3817e+00 (-2.3769e+00)\tConsistency Loss 1.7714e+00 (1.7200e+00)\tInconsistency Loss 1.8639e-01 (1.9754e-01)\tEntropy 2.0766e+00 (2.0484e+00)\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/107]\tTotal Loss -2.3778e+00 (-2.3778e+00)\tConsistency Loss 1.6918e+00 (1.6918e+00)\tInconsistency Loss 2.0198e-01 (2.0198e-01)\tEntropy 2.0348e+00 (2.0348e+00)\n",
      "Epoch: [6][100/107]\tTotal Loss -2.3791e+00 (-2.3762e+00)\tConsistency Loss 1.7184e+00 (1.7205e+00)\tInconsistency Loss 1.9722e-01 (1.9748e-01)\tEntropy 2.0487e+00 (2.0483e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/107]\tTotal Loss -2.3760e+00 (-2.3760e+00)\tConsistency Loss 1.7179e+00 (1.7179e+00)\tInconsistency Loss 1.9762e-01 (1.9762e-01)\tEntropy 2.0470e+00 (2.0470e+00)\n",
      "Epoch: [7][100/107]\tTotal Loss -2.3777e+00 (-2.3770e+00)\tConsistency Loss 1.7206e+00 (1.7187e+00)\tInconsistency Loss 1.9740e-01 (1.9763e-01)\tEntropy 2.0491e+00 (2.0479e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/107]\tTotal Loss -2.3787e+00 (-2.3787e+00)\tConsistency Loss 1.7132e+00 (1.7132e+00)\tInconsistency Loss 1.9869e-01 (1.9869e-01)\tEntropy 2.0460e+00 (2.0460e+00)\n",
      "Epoch: [8][100/107]\tTotal Loss -2.3771e+00 (-2.3770e+00)\tConsistency Loss 1.7190e+00 (1.7186e+00)\tInconsistency Loss 1.9746e-01 (1.9761e-01)\tEntropy 2.0481e+00 (2.0478e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/107]\tTotal Loss -2.3774e+00 (-2.3774e+00)\tConsistency Loss 1.7176e+00 (1.7176e+00)\tInconsistency Loss 1.9791e-01 (1.9791e-01)\tEntropy 2.0475e+00 (2.0475e+00)\n",
      "Epoch: [9][100/107]\tTotal Loss -2.3773e+00 (-2.3772e+00)\tConsistency Loss 1.7186e+00 (1.7180e+00)\tInconsistency Loss 1.9760e-01 (1.9775e-01)\tEntropy 2.0479e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/107]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7171e+00 (1.7171e+00)\tInconsistency Loss 1.9797e-01 (1.9797e-01)\tEntropy 2.0470e+00 (2.0470e+00)\n",
      "Epoch: [10][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7188e+00 (1.7183e+00)\tInconsistency Loss 1.9757e-01 (1.9770e-01)\tEntropy 2.0480e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/107]\tTotal Loss -2.3774e+00 (-2.3774e+00)\tConsistency Loss 1.7179e+00 (1.7179e+00)\tInconsistency Loss 1.9779e-01 (1.9779e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [11][100/107]\tTotal Loss -2.3771e+00 (-2.3772e+00)\tConsistency Loss 1.7171e+00 (1.7182e+00)\tInconsistency Loss 1.9790e-01 (1.9772e-01)\tEntropy 2.0471e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/107]\tTotal Loss -2.3769e+00 (-2.3769e+00)\tConsistency Loss 1.7198e+00 (1.7198e+00)\tInconsistency Loss 1.9743e-01 (1.9743e-01)\tEntropy 2.0484e+00 (2.0484e+00)\n",
      "Epoch: [12][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "A-1\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2680/8440 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/A-1/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0000e+00 (1.0000e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4867e-01 (6.2381e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2158e-01 (4.2409e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2337e-02 (3.1029e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4762e-02 (2.4066e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0000e-02 (1.9555e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0293e-02 (1.0009e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0000e-02 (1.0007e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0000e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0321e-02 (1.0029e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0000e-02 (1.0015e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0000e-02 (1.0010e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0000e-02 (1.0008e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0000e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/169]\n",
      "Fill TS Repository [100/169]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries A-1\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5360 - Test samples size: 8440\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/A-1/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/107]\tTotal Loss -2.3009e+00 (-2.3009e+00)\tConsistency Loss 2.2555e+00 (2.2555e+00)\tInconsistency Loss 1.1075e-01 (1.1075e-01)\tEntropy 2.2782e+00 (2.2782e+00)\n",
      "Epoch: [1][100/107]\tTotal Loss -2.3617e+00 (-2.3563e+00)\tConsistency Loss 1.7337e+00 (1.8684e+00)\tInconsistency Loss 1.9962e-01 (1.7338e-01)\tEntropy 2.0477e+00 (2.1123e+00)\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/107]\tTotal Loss -2.3653e+00 (-2.3653e+00)\tConsistency Loss 1.7766e+00 (1.7766e+00)\tInconsistency Loss 1.9114e-01 (1.9114e-01)\tEntropy 2.0710e+00 (2.0710e+00)\n",
      "Epoch: [2][100/107]\tTotal Loss -2.3781e+00 (-2.3770e+00)\tConsistency Loss 1.7376e+00 (1.7233e+00)\tInconsistency Loss 1.9362e-01 (1.9657e-01)\tEntropy 2.0579e+00 (2.0501e+00)\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/107]\tTotal Loss -2.3741e+00 (-2.3741e+00)\tConsistency Loss 1.6924e+00 (1.6924e+00)\tInconsistency Loss 2.0416e-01 (2.0416e-01)\tEntropy 2.0333e+00 (2.0333e+00)\n",
      "Epoch: [3][100/107]\tTotal Loss -2.3823e+00 (-2.3774e+00)\tConsistency Loss 1.7158e+00 (1.7176e+00)\tInconsistency Loss 1.9830e-01 (1.9784e-01)\tEntropy 2.0491e+00 (2.0475e+00)\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/107]\tTotal Loss -2.3630e+00 (-2.3630e+00)\tConsistency Loss 1.7355e+00 (1.7355e+00)\tInconsistency Loss 1.9500e-01 (1.9500e-01)\tEntropy 2.0493e+00 (2.0493e+00)\n",
      "Epoch: [4][100/107]\tTotal Loss -2.3778e+00 (-2.3759e+00)\tConsistency Loss 1.7240e+00 (1.7240e+00)\tInconsistency Loss 1.9646e-01 (1.9679e-01)\tEntropy 2.0509e+00 (2.0499e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/107]\tTotal Loss -2.3737e+00 (-2.3737e+00)\tConsistency Loss 1.7198e+00 (1.7198e+00)\tInconsistency Loss 1.9830e-01 (1.9830e-01)\tEntropy 2.0468e+00 (2.0468e+00)\n",
      "Epoch: [5][100/107]\tTotal Loss -2.3733e+00 (-2.3770e+00)\tConsistency Loss 1.7317e+00 (1.7185e+00)\tInconsistency Loss 1.9499e-01 (1.9769e-01)\tEntropy 2.0525e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/107]\tTotal Loss -2.3799e+00 (-2.3799e+00)\tConsistency Loss 1.7121e+00 (1.7121e+00)\tInconsistency Loss 1.9894e-01 (1.9894e-01)\tEntropy 2.0460e+00 (2.0460e+00)\n",
      "Epoch: [6][100/107]\tTotal Loss -2.3773e+00 (-2.3770e+00)\tConsistency Loss 1.7204e+00 (1.7187e+00)\tInconsistency Loss 1.9723e-01 (1.9763e-01)\tEntropy 2.0488e+00 (2.0478e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/107]\tTotal Loss -2.3780e+00 (-2.3780e+00)\tConsistency Loss 1.7145e+00 (1.7145e+00)\tInconsistency Loss 1.9868e-01 (1.9868e-01)\tEntropy 2.0462e+00 (2.0462e+00)\n",
      "Epoch: [7][100/107]\tTotal Loss -2.3755e+00 (-2.3770e+00)\tConsistency Loss 1.7167e+00 (1.7187e+00)\tInconsistency Loss 1.9824e-01 (1.9759e-01)\tEntropy 2.0461e+00 (2.0478e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/107]\tTotal Loss -2.3767e+00 (-2.3767e+00)\tConsistency Loss 1.7215e+00 (1.7215e+00)\tInconsistency Loss 1.9696e-01 (1.9696e-01)\tEntropy 2.0491e+00 (2.0491e+00)\n",
      "Epoch: [8][100/107]\tTotal Loss -2.3753e+00 (-2.3772e+00)\tConsistency Loss 1.7168e+00 (1.7181e+00)\tInconsistency Loss 1.9814e-01 (1.9772e-01)\tEntropy 2.0461e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/107]\tTotal Loss -2.3755e+00 (-2.3755e+00)\tConsistency Loss 1.7210e+00 (1.7210e+00)\tInconsistency Loss 1.9735e-01 (1.9735e-01)\tEntropy 2.0483e+00 (2.0483e+00)\n",
      "Epoch: [9][100/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7180e+00 (1.7183e+00)\tInconsistency Loss 1.9774e-01 (1.9769e-01)\tEntropy 2.0475e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/107]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7183e+00 (1.7183e+00)\tInconsistency Loss 1.9765e-01 (1.9765e-01)\tEntropy 2.0478e+00 (2.0478e+00)\n",
      "Epoch: [10][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7183e+00 (1.7181e+00)\tInconsistency Loss 1.9768e-01 (1.9772e-01)\tEntropy 2.0477e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9776e-01 (1.9776e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [11][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [12][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "D-1\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2649/8309 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/D-1/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/52]\tLoss 1.0029e+00 (1.0029e+00)\n",
      "Epoch: [1][10/52]\tLoss 3.4879e-01 (6.2449e-01)\n",
      "Epoch: [1][20/52]\tLoss 1.2129e-01 (4.2424e-01)\n",
      "Epoch: [1][30/52]\tLoss 4.2295e-02 (3.1033e-01)\n",
      "Epoch: [1][40/52]\tLoss 1.4723e-02 (2.4068e-01)\n",
      "Epoch: [1][50/52]\tLoss 1.0016e-02 (1.9557e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/52]\tLoss 1.0036e-02 (1.0036e-02)\n",
      "Epoch: [2][10/52]\tLoss 1.0006e-02 (1.0025e-02)\n",
      "Epoch: [2][20/52]\tLoss 1.0026e-02 (1.0026e-02)\n",
      "Epoch: [2][30/52]\tLoss 1.0017e-02 (1.0026e-02)\n",
      "Epoch: [2][40/52]\tLoss 1.0017e-02 (1.0024e-02)\n",
      "Epoch: [2][50/52]\tLoss 1.0015e-02 (1.0024e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/52]\tLoss 1.0013e-02 (1.0013e-02)\n",
      "Epoch: [3][10/52]\tLoss 1.0003e-02 (1.0015e-02)\n",
      "Epoch: [3][20/52]\tLoss 1.0019e-02 (1.0016e-02)\n",
      "Epoch: [3][30/52]\tLoss 1.0011e-02 (1.0015e-02)\n",
      "Epoch: [3][40/52]\tLoss 1.0015e-02 (1.0014e-02)\n",
      "Epoch: [3][50/52]\tLoss 1.0010e-02 (1.0013e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/52]\tLoss 1.0020e-02 (1.0020e-02)\n",
      "Epoch: [4][10/52]\tLoss 1.0006e-02 (1.0010e-02)\n",
      "Epoch: [4][20/52]\tLoss 1.0005e-02 (1.0009e-02)\n",
      "Epoch: [4][30/52]\tLoss 1.0010e-02 (1.0011e-02)\n",
      "Epoch: [4][40/52]\tLoss 1.0015e-02 (1.0010e-02)\n",
      "Epoch: [4][50/52]\tLoss 1.0001e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/52]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [5][10/52]\tLoss 1.0002e-02 (1.0006e-02)\n",
      "Epoch: [5][20/52]\tLoss 1.0002e-02 (1.0007e-02)\n",
      "Epoch: [5][30/52]\tLoss 1.0012e-02 (1.0007e-02)\n",
      "Epoch: [5][40/52]\tLoss 1.0001e-02 (1.0007e-02)\n",
      "Epoch: [5][50/52]\tLoss 1.0015e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [6][10/52]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [6][20/52]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "Epoch: [6][30/52]\tLoss 1.0002e-02 (1.0006e-02)\n",
      "Epoch: [6][40/52]\tLoss 1.0002e-02 (1.0006e-02)\n",
      "Epoch: [6][50/52]\tLoss 1.0002e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/52]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [7][10/52]\tLoss 1.0003e-02 (1.0006e-02)\n",
      "Epoch: [7][20/52]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "Epoch: [7][30/52]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [7][40/52]\tLoss 1.0002e-02 (1.0005e-02)\n",
      "Epoch: [7][50/52]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/52]\tLoss 1.0014e-02 (1.0014e-02)\n",
      "Epoch: [8][10/52]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [8][20/52]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [8][30/52]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [8][40/52]\tLoss 1.0012e-02 (1.0003e-02)\n",
      "Epoch: [8][50/52]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][10/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][20/52]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [9][30/52]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [9][40/52]\tLoss 1.0017e-02 (1.0003e-02)\n",
      "Epoch: [9][50/52]\tLoss 1.0011e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][10/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][20/52]\tLoss 1.0013e-02 (1.0002e-02)\n",
      "Epoch: [10][30/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][40/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][50/52]\tLoss 1.0066e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/52]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [11][10/52]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [11][20/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][30/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][40/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][50/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][10/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][20/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][30/52]\tLoss 1.0011e-02 (1.0002e-02)\n",
      "Epoch: [12][40/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][50/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][20/52]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [13][30/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][40/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [14][10/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [14][20/52]\tLoss 1.0024e-02 (1.0003e-02)\n",
      "Epoch: [14][30/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [14][40/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [14][50/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][10/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [15][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][40/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [15][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [16][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [17][10/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][40/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [17][50/52]\tLoss 1.0010e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/52]\tLoss 1.0007e-02 (1.0001e-02)\n",
      "Epoch: [18][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [19][30/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [19][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][10/52]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [20][20/52]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [20][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/52]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [21][10/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][20/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][50/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][20/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][10/52]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [24][20/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/52]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [26][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][30/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][50/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][20/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][20/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][20/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][30/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][40/52]\tLoss 1.0005e-02 (1.0001e-02)\n",
      "Epoch: [30][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/53]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/167]\n",
      "Fill TS Repository [100/167]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries D-1\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5298 - Test samples size: 8309\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/D-1/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/105]\tTotal Loss -2.3003e+00 (-2.3003e+00)\tConsistency Loss 2.2602e+00 (2.2602e+00)\tInconsistency Loss 1.1018e-01 (1.1018e-01)\tEntropy 2.2802e+00 (2.2802e+00)\n",
      "Epoch: [1][100/105]\tTotal Loss -2.3772e+00 (-2.3661e+00)\tConsistency Loss 1.7125e+00 (1.8142e+00)\tInconsistency Loss 1.9759e-01 (1.8174e-01)\tEntropy 2.0449e+00 (2.0901e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/105]\tTotal Loss -2.4063e+00 (-2.4063e+00)\tConsistency Loss 1.7969e+00 (1.7969e+00)\tInconsistency Loss 1.8135e-01 (1.8135e-01)\tEntropy 2.1016e+00 (2.1016e+00)\n",
      "Epoch: [2][100/105]\tTotal Loss -2.3788e+00 (-2.3743e+00)\tConsistency Loss 1.7389e+00 (1.7320e+00)\tInconsistency Loss 1.9622e-01 (1.9497e-01)\tEntropy 2.0588e+00 (2.0532e+00)\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/105]\tTotal Loss -2.3809e+00 (-2.3809e+00)\tConsistency Loss 1.7113e+00 (1.7113e+00)\tInconsistency Loss 2.0024e-01 (2.0024e-01)\tEntropy 2.0461e+00 (2.0461e+00)\n",
      "Epoch: [3][100/105]\tTotal Loss -2.3502e+00 (-2.3757e+00)\tConsistency Loss 1.6894e+00 (1.7219e+00)\tInconsistency Loss 2.0493e-01 (1.9718e-01)\tEntropy 2.0198e+00 (2.0488e+00)\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/105]\tTotal Loss -2.3798e+00 (-2.3798e+00)\tConsistency Loss 1.7215e+00 (1.7215e+00)\tInconsistency Loss 1.9310e-01 (1.9310e-01)\tEntropy 2.0507e+00 (2.0507e+00)\n",
      "Epoch: [4][100/105]\tTotal Loss -2.3781e+00 (-2.3762e+00)\tConsistency Loss 1.7244e+00 (1.7232e+00)\tInconsistency Loss 1.9607e-01 (1.9716e-01)\tEntropy 2.0512e+00 (2.0497e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/105]\tTotal Loss -2.3748e+00 (-2.3748e+00)\tConsistency Loss 1.7076e+00 (1.7076e+00)\tInconsistency Loss 2.0082e-01 (2.0082e-01)\tEntropy 2.0412e+00 (2.0412e+00)\n",
      "Epoch: [5][100/105]\tTotal Loss -2.3786e+00 (-2.3771e+00)\tConsistency Loss 1.7160e+00 (1.7192e+00)\tInconsistency Loss 1.9718e-01 (1.9736e-01)\tEntropy 2.0473e+00 (2.0481e+00)\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/105]\tTotal Loss -2.3791e+00 (-2.3791e+00)\tConsistency Loss 1.7231e+00 (1.7231e+00)\tInconsistency Loss 1.9527e-01 (1.9527e-01)\tEntropy 2.0511e+00 (2.0511e+00)\n",
      "Epoch: [6][100/105]\tTotal Loss -2.3764e+00 (-2.3767e+00)\tConsistency Loss 1.7238e+00 (1.7205e+00)\tInconsistency Loss 1.9618e-01 (1.9737e-01)\tEntropy 2.0501e+00 (2.0486e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/105]\tTotal Loss -2.3787e+00 (-2.3787e+00)\tConsistency Loss 1.7200e+00 (1.7200e+00)\tInconsistency Loss 1.9726e-01 (1.9726e-01)\tEntropy 2.0493e+00 (2.0493e+00)\n",
      "Epoch: [7][100/105]\tTotal Loss -2.3771e+00 (-2.3768e+00)\tConsistency Loss 1.7174e+00 (1.7192e+00)\tInconsistency Loss 1.9784e-01 (1.9758e-01)\tEntropy 2.0472e+00 (2.0480e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/105]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7196e+00 (1.7196e+00)\tInconsistency Loss 1.9741e-01 (1.9741e-01)\tEntropy 2.0483e+00 (2.0483e+00)\n",
      "Epoch: [8][100/105]\tTotal Loss -2.3772e+00 (-2.3771e+00)\tConsistency Loss 1.7177e+00 (1.7183e+00)\tInconsistency Loss 1.9781e-01 (1.9769e-01)\tEntropy 2.0475e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/105]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7197e+00 (1.7197e+00)\tInconsistency Loss 1.9737e-01 (1.9737e-01)\tEntropy 2.0485e+00 (2.0485e+00)\n",
      "Epoch: [9][100/105]\tTotal Loss -2.3769e+00 (-2.3772e+00)\tConsistency Loss 1.7189e+00 (1.7182e+00)\tInconsistency Loss 1.9770e-01 (1.9770e-01)\tEntropy 2.0479e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/105]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9775e-01 (1.9775e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [10][100/105]\tTotal Loss -2.3771e+00 (-2.3772e+00)\tConsistency Loss 1.7183e+00 (1.7181e+00)\tInconsistency Loss 1.9768e-01 (1.9773e-01)\tEntropy 2.0477e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/105]\tTotal Loss -2.3774e+00 (-2.3774e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9768e-01 (1.9768e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [11][100/105]\tTotal Loss -2.3771e+00 (-2.3772e+00)\tConsistency Loss 1.7185e+00 (1.7181e+00)\tInconsistency Loss 1.9769e-01 (1.9771e-01)\tEntropy 2.0478e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/105]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7178e+00 (1.7178e+00)\tInconsistency Loss 1.9783e-01 (1.9783e-01)\tEntropy 2.0474e+00 (2.0474e+00)\n",
      "Epoch: [12][100/105]\tTotal Loss -2.3778e+00 (-2.3772e+00)\tConsistency Loss 1.7189e+00 (1.7181e+00)\tInconsistency Loss 1.9756e-01 (1.9772e-01)\tEntropy 2.0484e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/105]\tTotal Loss -2.3766e+00 (-2.3766e+00)\tConsistency Loss 1.7188e+00 (1.7188e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [13][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "P-3\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2655/8293 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/P-3/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0069e+00 (1.0069e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4838e-01 (6.2469e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2112e-01 (4.2431e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2280e-02 (3.1035e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4690e-02 (2.4069e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0041e-02 (1.9558e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0018e-02 (1.0018e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0016e-02 (1.0030e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0038e-02 (1.0027e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0038e-02 (1.0030e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0016e-02 (1.0029e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0016e-02 (1.0028e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0034e-02 (1.0034e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0009e-02 (1.0019e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0017e-02 (1.0017e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0014e-02 (1.0017e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0017e-02 (1.0016e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0011e-02 (1.0016e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0008e-02 (1.0008e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0013e-02 (1.0014e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0017e-02 (1.0012e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0007e-02 (1.0011e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0089e-02 (1.0014e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0006e-02 (1.0013e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0010e-02 (1.0006e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0007e-02 (1.0008e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0036e-02 (1.0010e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0005e-02 (1.0010e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0019e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0004e-02 (1.0009e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0006e-02 (1.0008e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0005e-02 (1.0007e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0025e-02 (1.0007e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0015e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0008e-02 (1.0008e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0009e-02 (1.0004e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0003e-02 (1.0005e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0004e-02 (1.0006e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0002e-02 (1.0005e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0002e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0004e-02 (1.0005e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0007e-02 (1.0006e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0001e-02 (1.0006e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0002e-02 (1.0005e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0003e-02 (1.0005e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0003e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0018e-02 (1.0003e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [16][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [17][20/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [19][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0009e-02 (1.0001e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][20/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0005e-02 (1.0001e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0001e-02 (1.0009e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0004e-02 (1.0007e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0000e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/166]\n",
      "Fill TS Repository [100/166]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries P-3\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5310 - Test samples size: 8293\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/P-3/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/106]\tTotal Loss -2.3059e+00 (-2.3059e+00)\tConsistency Loss 2.2533e+00 (2.2533e+00)\tInconsistency Loss 1.1122e-01 (1.1122e-01)\tEntropy 2.2796e+00 (2.2796e+00)\n",
      "Epoch: [1][100/106]\tTotal Loss -2.3635e+00 (-2.3608e+00)\tConsistency Loss 1.7368e+00 (1.8316e+00)\tInconsistency Loss 1.9620e-01 (1.7926e-01)\tEntropy 2.0501e+00 (2.0962e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/106]\tTotal Loss -2.3679e+00 (-2.3679e+00)\tConsistency Loss 1.7506e+00 (1.7506e+00)\tInconsistency Loss 1.9583e-01 (1.9583e-01)\tEntropy 2.0592e+00 (2.0592e+00)\n",
      "Epoch: [2][100/106]\tTotal Loss -2.3787e+00 (-2.3765e+00)\tConsistency Loss 1.7203e+00 (1.7221e+00)\tInconsistency Loss 1.9744e-01 (1.9705e-01)\tEntropy 2.0495e+00 (2.0493e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/106]\tTotal Loss -2.3738e+00 (-2.3738e+00)\tConsistency Loss 1.7305e+00 (1.7305e+00)\tInconsistency Loss 1.9507e-01 (1.9507e-01)\tEntropy 2.0522e+00 (2.0522e+00)\n",
      "Epoch: [3][100/106]\tTotal Loss -2.3777e+00 (-2.3770e+00)\tConsistency Loss 1.7138e+00 (1.7191e+00)\tInconsistency Loss 1.9809e-01 (1.9752e-01)\tEntropy 2.0458e+00 (2.0481e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/106]\tTotal Loss -2.3793e+00 (-2.3793e+00)\tConsistency Loss 1.7185e+00 (1.7185e+00)\tInconsistency Loss 1.9894e-01 (1.9894e-01)\tEntropy 2.0489e+00 (2.0489e+00)\n",
      "Epoch: [4][100/106]\tTotal Loss -2.3796e+00 (-2.3763e+00)\tConsistency Loss 1.7219e+00 (1.7207e+00)\tInconsistency Loss 1.9719e-01 (1.9726e-01)\tEntropy 2.0507e+00 (2.0485e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/106]\tTotal Loss -2.3802e+00 (-2.3802e+00)\tConsistency Loss 1.7215e+00 (1.7215e+00)\tInconsistency Loss 1.9666e-01 (1.9666e-01)\tEntropy 2.0509e+00 (2.0509e+00)\n",
      "Epoch: [5][100/106]\tTotal Loss -2.3746e+00 (-2.3770e+00)\tConsistency Loss 1.7252e+00 (1.7186e+00)\tInconsistency Loss 1.9578e-01 (1.9770e-01)\tEntropy 2.0499e+00 (2.0478e+00)\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/106]\tTotal Loss -2.3775e+00 (-2.3775e+00)\tConsistency Loss 1.7189e+00 (1.7189e+00)\tInconsistency Loss 1.9758e-01 (1.9758e-01)\tEntropy 2.0482e+00 (2.0482e+00)\n",
      "Epoch: [6][100/106]\tTotal Loss -2.3765e+00 (-2.3770e+00)\tConsistency Loss 1.7195e+00 (1.7184e+00)\tInconsistency Loss 1.9764e-01 (1.9770e-01)\tEntropy 2.0480e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/106]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7202e+00 (1.7202e+00)\tInconsistency Loss 1.9728e-01 (1.9728e-01)\tEntropy 2.0488e+00 (2.0488e+00)\n",
      "Epoch: [7][100/106]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7187e+00 (1.7181e+00)\tInconsistency Loss 1.9765e-01 (1.9773e-01)\tEntropy 2.0479e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/106]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9776e-01 (1.9776e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [8][100/106]\tTotal Loss -2.3770e+00 (-2.3772e+00)\tConsistency Loss 1.7171e+00 (1.7181e+00)\tInconsistency Loss 1.9792e-01 (1.9771e-01)\tEntropy 2.0470e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7200e+00 (1.7200e+00)\tInconsistency Loss 1.9732e-01 (1.9732e-01)\tEntropy 2.0486e+00 (2.0486e+00)\n",
      "Epoch: [9][100/106]\tTotal Loss -2.3775e+00 (-2.3772e+00)\tConsistency Loss 1.7157e+00 (1.7181e+00)\tInconsistency Loss 1.9816e-01 (1.9773e-01)\tEntropy 2.0466e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/106]\tTotal Loss -2.3775e+00 (-2.3775e+00)\tConsistency Loss 1.7204e+00 (1.7204e+00)\tInconsistency Loss 1.9730e-01 (1.9730e-01)\tEntropy 2.0489e+00 (2.0489e+00)\n",
      "Epoch: [10][100/106]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7188e+00 (1.7182e+00)\tInconsistency Loss 1.9765e-01 (1.9771e-01)\tEntropy 2.0479e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/106]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7173e+00 (1.7173e+00)\tInconsistency Loss 1.9792e-01 (1.9792e-01)\tEntropy 2.0471e+00 (2.0471e+00)\n",
      "Epoch: [11][100/106]\tTotal Loss -2.3776e+00 (-2.3772e+00)\tConsistency Loss 1.7191e+00 (1.7181e+00)\tInconsistency Loss 1.9747e-01 (1.9773e-01)\tEntropy 2.0484e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/106]\tTotal Loss -2.3775e+00 (-2.3775e+00)\tConsistency Loss 1.7176e+00 (1.7176e+00)\tInconsistency Loss 1.9783e-01 (1.9783e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [12][100/106]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7180e+00 (1.7182e+00)\tInconsistency Loss 1.9777e-01 (1.9772e-01)\tEntropy 2.0476e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/106]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7183e+00 (1.7183e+00)\tInconsistency Loss 1.9770e-01 (1.9770e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [13][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "D-2\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2680/8395 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/D-2/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0056e+00 (1.0056e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4829e-01 (6.2436e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2139e-01 (4.2424e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2208e-02 (3.1033e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4724e-02 (2.4068e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0028e-02 (1.9557e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0016e-02 (1.0016e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0014e-02 (1.0022e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0037e-02 (1.0021e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0009e-02 (1.0020e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0008e-02 (1.0019e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0005e-02 (1.0018e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0008e-02 (1.0008e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0032e-02 (1.0018e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0012e-02 (1.0012e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0018e-02 (1.0011e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0005e-02 (1.0012e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0022e-02 (1.0011e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0009e-02 (1.0009e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0004e-02 (1.0007e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0004e-02 (1.0008e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0005e-02 (1.0007e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0002e-02 (1.0007e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0004e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0002e-02 (1.0010e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0005e-02 (1.0007e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0001e-02 (1.0009e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0006e-02 (1.0008e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0130e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0003e-02 (1.0005e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0012e-02 (1.0004e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0009e-02 (1.0004e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0000e-02 (1.0003e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0008e-02 (1.0008e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0006e-02 (1.0002e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0005e-02 (1.0002e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0010e-02 (1.0003e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0000e-02 (1.0004e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0000e-02 (1.0003e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0013e-02 (1.0001e-02)\n",
      "Epoch: [20][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0005e-02 (1.0001e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/168]\n",
      "Fill TS Repository [100/168]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries D-2\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5360 - Test samples size: 8395\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/D-2/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/107]\tTotal Loss -2.3030e+00 (-2.3030e+00)\tConsistency Loss 2.2569e+00 (2.2569e+00)\tInconsistency Loss 1.1058e-01 (1.1058e-01)\tEntropy 2.2800e+00 (2.2800e+00)\n",
      "Epoch: [1][100/107]\tTotal Loss -2.3742e+00 (-2.3596e+00)\tConsistency Loss 1.7037e+00 (1.8413e+00)\tInconsistency Loss 2.0024e-01 (1.7788e-01)\tEntropy 2.0390e+00 (2.1004e+00)\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/107]\tTotal Loss -2.3611e+00 (-2.3611e+00)\tConsistency Loss 1.7453e+00 (1.7453e+00)\tInconsistency Loss 1.9196e-01 (1.9196e-01)\tEntropy 2.0532e+00 (2.0532e+00)\n",
      "Epoch: [2][100/107]\tTotal Loss -2.3642e+00 (-2.3762e+00)\tConsistency Loss 1.7108e+00 (1.7248e+00)\tInconsistency Loss 2.0028e-01 (1.9633e-01)\tEntropy 2.0375e+00 (2.0505e+00)\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/107]\tTotal Loss -2.3534e+00 (-2.3534e+00)\tConsistency Loss 1.6779e+00 (1.6779e+00)\tInconsistency Loss 2.0717e-01 (2.0717e-01)\tEntropy 2.0157e+00 (2.0157e+00)\n",
      "Epoch: [3][100/107]\tTotal Loss -2.3790e+00 (-2.3769e+00)\tConsistency Loss 1.6987e+00 (1.7195e+00)\tInconsistency Loss 2.0075e-01 (1.9744e-01)\tEntropy 2.0388e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/107]\tTotal Loss -2.3667e+00 (-2.3667e+00)\tConsistency Loss 1.7282e+00 (1.7282e+00)\tInconsistency Loss 1.9728e-01 (1.9728e-01)\tEntropy 2.0474e+00 (2.0474e+00)\n",
      "Epoch: [4][100/107]\tTotal Loss -2.3639e+00 (-2.3771e+00)\tConsistency Loss 1.7488e+00 (1.7193e+00)\tInconsistency Loss 1.9406e-01 (1.9736e-01)\tEntropy 2.0564e+00 (2.0482e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/107]\tTotal Loss -2.3678e+00 (-2.3678e+00)\tConsistency Loss 1.7004e+00 (1.7004e+00)\tInconsistency Loss 2.0195e-01 (2.0195e-01)\tEntropy 2.0341e+00 (2.0341e+00)\n",
      "Epoch: [5][100/107]\tTotal Loss -2.3767e+00 (-2.3763e+00)\tConsistency Loss 1.7445e+00 (1.7205e+00)\tInconsistency Loss 1.9246e-01 (1.9734e-01)\tEntropy 2.0606e+00 (2.0484e+00)\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/107]\tTotal Loss -2.3751e+00 (-2.3751e+00)\tConsistency Loss 1.7002e+00 (1.7002e+00)\tInconsistency Loss 2.0175e-01 (2.0175e-01)\tEntropy 2.0376e+00 (2.0376e+00)\n",
      "Epoch: [6][100/107]\tTotal Loss -2.3789e+00 (-2.3770e+00)\tConsistency Loss 1.7150e+00 (1.7187e+00)\tInconsistency Loss 1.9811e-01 (1.9766e-01)\tEntropy 2.0470e+00 (2.0478e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/107]\tTotal Loss -2.3779e+00 (-2.3779e+00)\tConsistency Loss 1.7207e+00 (1.7207e+00)\tInconsistency Loss 1.9703e-01 (1.9703e-01)\tEntropy 2.0493e+00 (2.0493e+00)\n",
      "Epoch: [7][100/107]\tTotal Loss -2.3775e+00 (-2.3770e+00)\tConsistency Loss 1.7216e+00 (1.7185e+00)\tInconsistency Loss 1.9706e-01 (1.9764e-01)\tEntropy 2.0495e+00 (2.0478e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/107]\tTotal Loss -2.3777e+00 (-2.3777e+00)\tConsistency Loss 1.7163e+00 (1.7163e+00)\tInconsistency Loss 1.9794e-01 (1.9794e-01)\tEntropy 2.0470e+00 (2.0470e+00)\n",
      "Epoch: [8][100/107]\tTotal Loss -2.3772e+00 (-2.3771e+00)\tConsistency Loss 1.7178e+00 (1.7183e+00)\tInconsistency Loss 1.9777e-01 (1.9770e-01)\tEntropy 2.0475e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/107]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7183e+00 (1.7183e+00)\tInconsistency Loss 1.9769e-01 (1.9769e-01)\tEntropy 2.0478e+00 (2.0478e+00)\n",
      "Epoch: [9][100/107]\tTotal Loss -2.3770e+00 (-2.3771e+00)\tConsistency Loss 1.7195e+00 (1.7182e+00)\tInconsistency Loss 1.9746e-01 (1.9772e-01)\tEntropy 2.0482e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/107]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7171e+00 (1.7171e+00)\tInconsistency Loss 1.9791e-01 (1.9791e-01)\tEntropy 2.0472e+00 (2.0472e+00)\n",
      "Epoch: [10][100/107]\tTotal Loss -2.3774e+00 (-2.3771e+00)\tConsistency Loss 1.7186e+00 (1.7182e+00)\tInconsistency Loss 1.9757e-01 (1.9772e-01)\tEntropy 2.0480e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/107]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9777e-01 (1.9777e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [11][100/107]\tTotal Loss -2.3774e+00 (-2.3771e+00)\tConsistency Loss 1.7198e+00 (1.7182e+00)\tInconsistency Loss 1.9731e-01 (1.9772e-01)\tEntropy 2.0486e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7173e+00 (1.7173e+00)\tInconsistency Loss 1.9792e-01 (1.9792e-01)\tEntropy 2.0473e+00 (2.0473e+00)\n",
      "Epoch: [12][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7181e+00)\tInconsistency Loss 1.9775e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "D-3\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2680/8440 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/D-3/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0009e+00 (1.0009e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4860e-01 (6.2399e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2148e-01 (4.2414e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2360e-02 (3.1029e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4760e-02 (2.4066e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0014e-02 (1.9556e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0004e-02 (1.0008e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0002e-02 (1.0006e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0013e-02 (1.0006e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0002e-02 (1.0006e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0002e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0004e-02 (1.0005e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0015e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0005e-02 (1.0001e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0002e-02 (1.0000e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][40/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [20][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/169]\n",
      "Fill TS Repository [100/169]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries D-3\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5360 - Test samples size: 8440\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/D-3/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/107]\tTotal Loss -2.3013e+00 (-2.3013e+00)\tConsistency Loss 2.2587e+00 (2.2587e+00)\tInconsistency Loss 1.1047e-01 (1.1047e-01)\tEntropy 2.2800e+00 (2.2800e+00)\n",
      "Epoch: [1][100/107]\tTotal Loss -2.3577e+00 (-2.3577e+00)\tConsistency Loss 1.6772e+00 (1.8402e+00)\tInconsistency Loss 2.1047e-01 (1.7801e-01)\tEntropy 2.0175e+00 (2.0989e+00)\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/107]\tTotal Loss -2.3645e+00 (-2.3645e+00)\tConsistency Loss 1.7125e+00 (1.7125e+00)\tInconsistency Loss 1.9636e-01 (1.9636e-01)\tEntropy 2.0385e+00 (2.0385e+00)\n",
      "Epoch: [2][100/107]\tTotal Loss -2.3685e+00 (-2.3764e+00)\tConsistency Loss 1.7000e+00 (1.7249e+00)\tInconsistency Loss 2.0403e-01 (1.9623e-01)\tEntropy 2.0342e+00 (2.0506e+00)\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/107]\tTotal Loss -2.3872e+00 (-2.3872e+00)\tConsistency Loss 1.7167e+00 (1.7167e+00)\tInconsistency Loss 1.9450e-01 (1.9450e-01)\tEntropy 2.0520e+00 (2.0520e+00)\n",
      "Epoch: [3][100/107]\tTotal Loss -2.3728e+00 (-2.3766e+00)\tConsistency Loss 1.7271e+00 (1.7207e+00)\tInconsistency Loss 1.9814e-01 (1.9739e-01)\tEntropy 2.0500e+00 (2.0487e+00)\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/107]\tTotal Loss -2.3932e+00 (-2.3932e+00)\tConsistency Loss 1.7162e+00 (1.7162e+00)\tInconsistency Loss 1.9649e-01 (1.9649e-01)\tEntropy 2.0547e+00 (2.0547e+00)\n",
      "Epoch: [4][100/107]\tTotal Loss -2.3662e+00 (-2.3764e+00)\tConsistency Loss 1.7454e+00 (1.7201e+00)\tInconsistency Loss 1.9161e-01 (1.9735e-01)\tEntropy 2.0558e+00 (2.0483e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/107]\tTotal Loss -2.3733e+00 (-2.3733e+00)\tConsistency Loss 1.7578e+00 (1.7578e+00)\tInconsistency Loss 1.8873e-01 (1.8873e-01)\tEntropy 2.0655e+00 (2.0655e+00)\n",
      "Epoch: [5][100/107]\tTotal Loss -2.3804e+00 (-2.3769e+00)\tConsistency Loss 1.7361e+00 (1.7194e+00)\tInconsistency Loss 1.9427e-01 (1.9745e-01)\tEntropy 2.0582e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/107]\tTotal Loss -2.3731e+00 (-2.3731e+00)\tConsistency Loss 1.7171e+00 (1.7171e+00)\tInconsistency Loss 1.9933e-01 (1.9933e-01)\tEntropy 2.0451e+00 (2.0451e+00)\n",
      "Epoch: [6][100/107]\tTotal Loss -2.3723e+00 (-2.3770e+00)\tConsistency Loss 1.7090e+00 (1.7189e+00)\tInconsistency Loss 2.0029e-01 (1.9765e-01)\tEntropy 2.0406e+00 (2.0479e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/107]\tTotal Loss -2.3778e+00 (-2.3778e+00)\tConsistency Loss 1.7244e+00 (1.7244e+00)\tInconsistency Loss 1.9634e-01 (1.9634e-01)\tEntropy 2.0511e+00 (2.0511e+00)\n",
      "Epoch: [7][100/107]\tTotal Loss -2.3781e+00 (-2.3771e+00)\tConsistency Loss 1.7301e+00 (1.7193e+00)\tInconsistency Loss 1.9550e-01 (1.9750e-01)\tEntropy 2.0541e+00 (2.0482e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/107]\tTotal Loss -2.3769e+00 (-2.3769e+00)\tConsistency Loss 1.7128e+00 (1.7128e+00)\tInconsistency Loss 1.9834e-01 (1.9834e-01)\tEntropy 2.0449e+00 (2.0449e+00)\n",
      "Epoch: [8][100/107]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7173e+00 (1.7185e+00)\tInconsistency Loss 1.9791e-01 (1.9767e-01)\tEntropy 2.0472e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7183e+00 (1.7183e+00)\tInconsistency Loss 1.9767e-01 (1.9767e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [9][100/107]\tTotal Loss -2.3772e+00 (-2.3771e+00)\tConsistency Loss 1.7184e+00 (1.7183e+00)\tInconsistency Loss 1.9766e-01 (1.9769e-01)\tEntropy 2.0478e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7179e+00 (1.7179e+00)\tInconsistency Loss 1.9776e-01 (1.9776e-01)\tEntropy 2.0475e+00 (2.0475e+00)\n",
      "Epoch: [10][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7181e+00)\tInconsistency Loss 1.9771e-01 (1.9772e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [11][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [12][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "D-4\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2633/8273 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/D-4/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/52]\tLoss 1.0009e+00 (1.0009e+00)\n",
      "Epoch: [1][10/52]\tLoss 3.4854e-01 (6.2401e-01)\n",
      "Epoch: [1][20/52]\tLoss 1.2147e-01 (4.2414e-01)\n",
      "Epoch: [1][30/52]\tLoss 4.2343e-02 (3.1029e-01)\n",
      "Epoch: [1][40/52]\tLoss 1.4767e-02 (2.4066e-01)\n",
      "Epoch: [1][50/52]\tLoss 1.0003e-02 (1.9556e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [2][10/52]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [2][20/52]\tLoss 1.0003e-02 (1.0005e-02)\n",
      "Epoch: [2][30/52]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "Epoch: [2][40/52]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "Epoch: [2][50/52]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [3][10/52]\tLoss 1.0005e-02 (1.0004e-02)\n",
      "Epoch: [3][20/52]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [3][30/52]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [3][40/52]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [3][50/52]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [4][10/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [4][20/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [4][30/52]\tLoss 1.0005e-02 (1.0002e-02)\n",
      "Epoch: [4][40/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [4][50/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [5][10/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [5][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [5][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [5][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [5][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [6][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [6][20/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [6][30/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [6][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [6][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][10/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [7][20/52]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [7][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [7][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [7][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [8][10/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [8][20/52]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [8][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [8][40/52]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [8][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [9][20/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [9][30/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [9][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [9][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][10/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][20/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][30/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][40/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][50/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][10/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][20/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][30/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][40/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][50/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][10/52]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [12][20/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][30/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][40/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][50/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][10/52]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [13][20/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][30/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [13][40/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][50/52]\tLoss 1.0003e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][10/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][20/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][30/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][40/52]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [14][50/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][10/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][20/52]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [15][30/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][40/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][50/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][10/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][20/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][30/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][40/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][50/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][20/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][30/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][40/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][50/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][10/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][20/52]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [18][30/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][40/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][50/52]\tLoss 1.0002e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][10/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][20/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][30/52]\tLoss 1.0003e-02 (1.0000e-02)\n",
      "Epoch: [19][40/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][50/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][10/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][20/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][30/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][40/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][50/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][10/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][20/52]\tLoss 1.0002e-02 (1.0000e-02)\n",
      "Epoch: [21][30/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][40/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][50/52]\tLoss 1.0002e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][20/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][30/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][40/52]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [22][50/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][20/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][30/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][40/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][50/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][10/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][20/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][30/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][40/52]\tLoss 1.0003e-02 (1.0000e-02)\n",
      "Epoch: [24][50/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][20/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][30/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][40/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][50/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [26][10/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][20/52]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [26][30/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][40/52]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [26][50/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][10/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][20/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][30/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][40/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][50/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][10/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][20/52]\tLoss 1.0002e-02 (1.0000e-02)\n",
      "Epoch: [28][30/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][40/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][50/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][10/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][20/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][30/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][40/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][50/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/52]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [30][20/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][30/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][40/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][50/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/53]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/166]\n",
      "Fill TS Repository [100/166]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries D-4\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5266 - Test samples size: 8273\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/D-4/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/105]\tTotal Loss -2.3138e+00 (-2.3138e+00)\tConsistency Loss 2.2448e+00 (2.2448e+00)\tInconsistency Loss 1.1060e-01 (1.1060e-01)\tEntropy 2.2793e+00 (2.2793e+00)\n",
      "Epoch: [1][100/105]\tTotal Loss -2.3582e+00 (-2.3540e+00)\tConsistency Loss 1.6963e+00 (1.8838e+00)\tInconsistency Loss 2.1763e-01 (1.7087e-01)\tEntropy 2.0272e+00 (2.1189e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/105]\tTotal Loss -2.3743e+00 (-2.3743e+00)\tConsistency Loss 1.7058e+00 (1.7058e+00)\tInconsistency Loss 2.0022e-01 (2.0022e-01)\tEntropy 2.0401e+00 (2.0401e+00)\n",
      "Epoch: [2][100/105]\tTotal Loss -2.3823e+00 (-2.3763e+00)\tConsistency Loss 1.7279e+00 (1.7231e+00)\tInconsistency Loss 1.9743e-01 (1.9655e-01)\tEntropy 2.0551e+00 (2.0497e+00)\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/105]\tTotal Loss -2.3735e+00 (-2.3735e+00)\tConsistency Loss 1.7080e+00 (1.7080e+00)\tInconsistency Loss 2.0105e-01 (2.0105e-01)\tEntropy 2.0407e+00 (2.0407e+00)\n",
      "Epoch: [3][100/105]\tTotal Loss -2.3776e+00 (-2.3771e+00)\tConsistency Loss 1.7411e+00 (1.7187e+00)\tInconsistency Loss 1.9328e-01 (1.9761e-01)\tEntropy 2.0594e+00 (2.0479e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/105]\tTotal Loss -2.3751e+00 (-2.3751e+00)\tConsistency Loss 1.7402e+00 (1.7402e+00)\tInconsistency Loss 1.9402e-01 (1.9402e-01)\tEntropy 2.0576e+00 (2.0576e+00)\n",
      "Epoch: [4][100/105]\tTotal Loss -2.3776e+00 (-2.3763e+00)\tConsistency Loss 1.6998e+00 (1.7221e+00)\tInconsistency Loss 2.0102e-01 (1.9712e-01)\tEntropy 2.0387e+00 (2.0492e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/105]\tTotal Loss -2.3782e+00 (-2.3782e+00)\tConsistency Loss 1.7001e+00 (1.7001e+00)\tInconsistency Loss 2.0092e-01 (2.0092e-01)\tEntropy 2.0392e+00 (2.0392e+00)\n",
      "Epoch: [5][100/105]\tTotal Loss -2.3752e+00 (-2.3765e+00)\tConsistency Loss 1.7212e+00 (1.7210e+00)\tInconsistency Loss 1.9624e-01 (1.9720e-01)\tEntropy 2.0482e+00 (2.0488e+00)\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/105]\tTotal Loss -2.3757e+00 (-2.3757e+00)\tConsistency Loss 1.7261e+00 (1.7261e+00)\tInconsistency Loss 1.9514e-01 (1.9514e-01)\tEntropy 2.0509e+00 (2.0509e+00)\n",
      "Epoch: [6][100/105]\tTotal Loss -2.3781e+00 (-2.3772e+00)\tConsistency Loss 1.7205e+00 (1.7185e+00)\tInconsistency Loss 1.9656e-01 (1.9757e-01)\tEntropy 2.0493e+00 (2.0479e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/105]\tTotal Loss -2.3723e+00 (-2.3723e+00)\tConsistency Loss 1.7262e+00 (1.7262e+00)\tInconsistency Loss 1.9579e-01 (1.9579e-01)\tEntropy 2.0492e+00 (2.0492e+00)\n",
      "Epoch: [7][100/105]\tTotal Loss -2.3769e+00 (-2.3768e+00)\tConsistency Loss 1.7215e+00 (1.7197e+00)\tInconsistency Loss 1.9699e-01 (1.9751e-01)\tEntropy 2.0492e+00 (2.0482e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/105]\tTotal Loss -2.3752e+00 (-2.3752e+00)\tConsistency Loss 1.7192e+00 (1.7192e+00)\tInconsistency Loss 1.9749e-01 (1.9749e-01)\tEntropy 2.0472e+00 (2.0472e+00)\n",
      "Epoch: [8][100/105]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7303e+00 (1.7197e+00)\tInconsistency Loss 1.9513e-01 (1.9743e-01)\tEntropy 2.0537e+00 (2.0483e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/105]\tTotal Loss -2.3803e+00 (-2.3803e+00)\tConsistency Loss 1.7054e+00 (1.7054e+00)\tInconsistency Loss 2.0051e-01 (2.0051e-01)\tEntropy 2.0428e+00 (2.0428e+00)\n",
      "Epoch: [9][100/105]\tTotal Loss -2.3761e+00 (-2.3770e+00)\tConsistency Loss 1.7165e+00 (1.7184e+00)\tInconsistency Loss 1.9820e-01 (1.9767e-01)\tEntropy 2.0463e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/105]\tTotal Loss -2.3775e+00 (-2.3775e+00)\tConsistency Loss 1.7227e+00 (1.7227e+00)\tInconsistency Loss 1.9661e-01 (1.9661e-01)\tEntropy 2.0501e+00 (2.0501e+00)\n",
      "Epoch: [10][100/105]\tTotal Loss -2.3772e+00 (-2.3771e+00)\tConsistency Loss 1.7184e+00 (1.7185e+00)\tInconsistency Loss 1.9765e-01 (1.9765e-01)\tEntropy 2.0478e+00 (2.0478e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7173e+00 (1.7173e+00)\tInconsistency Loss 1.9791e-01 (1.9791e-01)\tEntropy 2.0472e+00 (2.0472e+00)\n",
      "Epoch: [11][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7182e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [12][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/105]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "A-2\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2448/7714 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/A-2/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/48]\tLoss 1.0036e+00 (1.0036e+00)\n",
      "Epoch: [1][10/48]\tLoss 3.4826e-01 (6.2426e-01)\n",
      "Epoch: [1][20/48]\tLoss 1.2147e-01 (4.2420e-01)\n",
      "Epoch: [1][30/48]\tLoss 4.2338e-02 (3.1032e-01)\n",
      "Epoch: [1][40/48]\tLoss 1.4743e-02 (2.4068e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/48]\tLoss 1.0015e-02 (1.0015e-02)\n",
      "Epoch: [2][10/48]\tLoss 1.0007e-02 (1.0020e-02)\n",
      "Epoch: [2][20/48]\tLoss 1.0030e-02 (1.0019e-02)\n",
      "Epoch: [2][30/48]\tLoss 1.0009e-02 (1.0022e-02)\n",
      "Epoch: [2][40/48]\tLoss 1.0005e-02 (1.0019e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/48]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [3][10/48]\tLoss 1.0005e-02 (1.0008e-02)\n",
      "Epoch: [3][20/48]\tLoss 1.0004e-02 (1.0008e-02)\n",
      "Epoch: [3][30/48]\tLoss 1.0045e-02 (1.0012e-02)\n",
      "Epoch: [3][40/48]\tLoss 1.0005e-02 (1.0011e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [4][10/48]\tLoss 1.0137e-02 (1.0017e-02)\n",
      "Epoch: [4][20/48]\tLoss 1.0007e-02 (1.0012e-02)\n",
      "Epoch: [4][30/48]\tLoss 1.0002e-02 (1.0010e-02)\n",
      "Epoch: [4][40/48]\tLoss 1.0002e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/48]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [5][10/48]\tLoss 1.0014e-02 (1.0006e-02)\n",
      "Epoch: [5][20/48]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [5][30/48]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [5][40/48]\tLoss 1.0009e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/48]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [6][10/48]\tLoss 1.0008e-02 (1.0004e-02)\n",
      "Epoch: [6][20/48]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [6][30/48]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [6][40/48]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/48]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [7][10/48]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [7][20/48]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [7][30/48]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [7][40/48]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/48]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][10/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [8][20/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [8][30/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [8][40/48]\tLoss 1.0007e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [9][10/48]\tLoss 1.0005e-02 (1.0002e-02)\n",
      "Epoch: [9][20/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][30/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][40/48]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [10][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [10][20/48]\tLoss 1.0006e-02 (1.0002e-02)\n",
      "Epoch: [10][30/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][40/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][20/48]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [11][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][40/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/48]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [12][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][20/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [13][30/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [13][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [14][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][20/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [15][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][40/48]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [16][30/48]\tLoss 1.0019e-02 (1.0002e-02)\n",
      "Epoch: [16][40/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][30/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [17][40/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/48]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [18][10/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [18][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [18][40/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [19][40/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][10/48]\tLoss 1.0000e-02 (1.0005e-02)\n",
      "Epoch: [20][20/48]\tLoss 1.0000e-02 (1.0003e-02)\n",
      "Epoch: [20][30/48]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [20][40/48]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][20/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][30/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][10/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][40/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/48]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [23][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][30/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][30/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [24][40/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/48]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/48]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [25][20/48]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [25][30/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][40/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][10/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][20/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][30/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/48]\tLoss 1.0009e-02 (1.0009e-02)\n",
      "Epoch: [27][10/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][30/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][20/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][30/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][40/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/48]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][10/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][40/48]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][10/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][20/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/49]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/155]\n",
      "Fill TS Repository [100/155]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries A-2\u001b[0m\n",
      "\u001b[32m-- Train samples size: 4896 - Test samples size: 7714\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/A-2/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][ 0/97]\tTotal Loss -2.3011e+00 (-2.3011e+00)\tConsistency Loss 2.2574e+00 (2.2574e+00)\tInconsistency Loss 1.1042e-01 (1.1042e-01)\tEntropy 2.2793e+00 (2.2793e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][ 0/97]\tTotal Loss -2.3745e+00 (-2.3745e+00)\tConsistency Loss 1.7466e+00 (1.7466e+00)\tInconsistency Loss 1.9756e-01 (1.9756e-01)\tEntropy 2.0606e+00 (2.0606e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][ 0/97]\tTotal Loss -2.3658e+00 (-2.3658e+00)\tConsistency Loss 1.7435e+00 (1.7435e+00)\tInconsistency Loss 1.9480e-01 (1.9480e-01)\tEntropy 2.0547e+00 (2.0547e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][ 0/97]\tTotal Loss -2.3735e+00 (-2.3735e+00)\tConsistency Loss 1.7301e+00 (1.7301e+00)\tInconsistency Loss 1.9634e-01 (1.9634e-01)\tEntropy 2.0518e+00 (2.0518e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][ 0/97]\tTotal Loss -2.3766e+00 (-2.3766e+00)\tConsistency Loss 1.7186e+00 (1.7186e+00)\tInconsistency Loss 1.9754e-01 (1.9754e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][ 0/97]\tTotal Loss -2.3767e+00 (-2.3767e+00)\tConsistency Loss 1.7175e+00 (1.7175e+00)\tInconsistency Loss 1.9820e-01 (1.9820e-01)\tEntropy 2.0471e+00 (2.0471e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][ 0/97]\tTotal Loss -2.3766e+00 (-2.3766e+00)\tConsistency Loss 1.7193e+00 (1.7193e+00)\tInconsistency Loss 1.9771e-01 (1.9771e-01)\tEntropy 2.0480e+00 (2.0480e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][ 0/97]\tTotal Loss -2.3778e+00 (-2.3778e+00)\tConsistency Loss 1.7186e+00 (1.7186e+00)\tInconsistency Loss 1.9754e-01 (1.9754e-01)\tEntropy 2.0482e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][ 0/97]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7179e+00 (1.7179e+00)\tInconsistency Loss 1.9780e-01 (1.9780e-01)\tEntropy 2.0475e+00 (2.0475e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][ 0/97]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "A-3\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2536/8005 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/A-3/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/50]\tLoss 1.0082e+00 (1.0082e+00)\n",
      "Epoch: [1][10/50]\tLoss 3.4810e-01 (6.2455e-01)\n",
      "Epoch: [1][20/50]\tLoss 1.2115e-01 (4.2428e-01)\n",
      "Epoch: [1][30/50]\tLoss 4.2216e-02 (3.1034e-01)\n",
      "Epoch: [1][40/50]\tLoss 1.4753e-02 (2.4068e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/50]\tLoss 1.0014e-02 (1.0014e-02)\n",
      "Epoch: [2][10/50]\tLoss 1.0029e-02 (1.0054e-02)\n",
      "Epoch: [2][20/50]\tLoss 1.0104e-02 (1.0052e-02)\n",
      "Epoch: [2][30/50]\tLoss 1.0014e-02 (1.0042e-02)\n",
      "Epoch: [2][40/50]\tLoss 1.0025e-02 (1.0037e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/50]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [3][10/50]\tLoss 1.0017e-02 (1.0016e-02)\n",
      "Epoch: [3][20/50]\tLoss 1.0013e-02 (1.0016e-02)\n",
      "Epoch: [3][30/50]\tLoss 1.0009e-02 (1.0017e-02)\n",
      "Epoch: [3][40/50]\tLoss 1.0015e-02 (1.0018e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/50]\tLoss 1.0079e-02 (1.0079e-02)\n",
      "Epoch: [4][10/50]\tLoss 1.0004e-02 (1.0014e-02)\n",
      "Epoch: [4][20/50]\tLoss 1.0006e-02 (1.0013e-02)\n",
      "Epoch: [4][30/50]\tLoss 1.0005e-02 (1.0013e-02)\n",
      "Epoch: [4][40/50]\tLoss 1.0006e-02 (1.0013e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/50]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [5][10/50]\tLoss 1.0012e-02 (1.0008e-02)\n",
      "Epoch: [5][20/50]\tLoss 1.0009e-02 (1.0007e-02)\n",
      "Epoch: [5][30/50]\tLoss 1.0004e-02 (1.0009e-02)\n",
      "Epoch: [5][40/50]\tLoss 1.0004e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/50]\tLoss 1.0012e-02 (1.0012e-02)\n",
      "Epoch: [6][10/50]\tLoss 1.0003e-02 (1.0006e-02)\n",
      "Epoch: [6][20/50]\tLoss 1.0002e-02 (1.0006e-02)\n",
      "Epoch: [6][30/50]\tLoss 1.0003e-02 (1.0006e-02)\n",
      "Epoch: [6][40/50]\tLoss 1.0003e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/50]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [7][10/50]\tLoss 1.0006e-02 (1.0005e-02)\n",
      "Epoch: [7][20/50]\tLoss 1.0003e-02 (1.0005e-02)\n",
      "Epoch: [7][30/50]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "Epoch: [7][40/50]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/50]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [8][10/50]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [8][20/50]\tLoss 1.0005e-02 (1.0003e-02)\n",
      "Epoch: [8][30/50]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [8][40/50]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/50]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [9][10/50]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [9][20/50]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [9][30/50]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [9][40/50]\tLoss 1.0006e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/50]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][10/50]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][20/50]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [10][30/50]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [10][40/50]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][10/50]\tLoss 1.0004e-02 (1.0011e-02)\n",
      "Epoch: [11][20/50]\tLoss 1.0002e-02 (1.0007e-02)\n",
      "Epoch: [11][30/50]\tLoss 1.0002e-02 (1.0005e-02)\n",
      "Epoch: [11][40/50]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/50]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [12][10/50]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][20/50]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][30/50]\tLoss 1.0009e-02 (1.0002e-02)\n",
      "Epoch: [12][40/50]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/50]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][10/50]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][20/50]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [13][30/50]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][40/50]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][10/50]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [14][20/50]\tLoss 1.0000e-02 (1.0003e-02)\n",
      "Epoch: [14][30/50]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [14][40/50]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/50]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [15][10/50]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [15][20/50]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [15][30/50]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [15][40/50]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][10/50]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [16][20/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][30/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][40/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/50]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [17][20/50]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [17][30/50]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [17][40/50]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][10/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][20/50]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [18][30/50]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [18][40/50]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/50]\tLoss 1.0051e-02 (1.0051e-02)\n",
      "Epoch: [19][10/50]\tLoss 1.0001e-02 (1.0006e-02)\n",
      "Epoch: [19][20/50]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [19][30/50]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [19][40/50]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][10/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][20/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][30/50]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [20][40/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/50]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][10/50]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][20/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][30/50]\tLoss 1.0006e-02 (1.0001e-02)\n",
      "Epoch: [21][40/50]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][10/50]\tLoss 1.0007e-02 (1.0002e-02)\n",
      "Epoch: [22][20/50]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [22][30/50]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [22][40/50]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][10/50]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [23][20/50]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [23][30/50]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [23][40/50]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][10/50]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [24][20/50]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [24][30/50]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [24][40/50]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/50]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][20/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][30/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][40/50]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/50]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [26][10/50]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][20/50]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [26][30/50]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [26][40/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][10/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][20/50]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [27][30/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][40/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][10/50]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [28][20/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][30/50]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [28][40/50]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][10/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][20/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][30/50]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [29][40/50]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/50]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [30][10/50]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [30][20/50]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [30][30/50]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][40/50]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/51]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/161]\n",
      "Fill TS Repository [100/161]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries A-3\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5072 - Test samples size: 8005\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/A-3/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/101]\tTotal Loss -2.2997e+00 (-2.2997e+00)\tConsistency Loss 2.2579e+00 (2.2579e+00)\tInconsistency Loss 1.1051e-01 (1.1051e-01)\tEntropy 2.2788e+00 (2.2788e+00)\n",
      "Epoch: [1][100/101]\tTotal Loss -2.3417e+00 (-2.3659e+00)\tConsistency Loss 1.6802e+00 (1.8038e+00)\tInconsistency Loss 2.1365e-01 (1.8347e-01)\tEntropy 2.0109e+00 (2.0848e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/101]\tTotal Loss -2.3228e+00 (-2.3228e+00)\tConsistency Loss 1.7317e+00 (1.7317e+00)\tInconsistency Loss 1.9386e-01 (1.9386e-01)\tEntropy 2.0272e+00 (2.0272e+00)\n",
      "Epoch: [2][100/101]\tTotal Loss -2.3883e+00 (-2.3763e+00)\tConsistency Loss 1.7367e+00 (1.7263e+00)\tInconsistency Loss 1.9383e-01 (1.9636e-01)\tEntropy 2.0625e+00 (2.0513e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/101]\tTotal Loss -2.4071e+00 (-2.4071e+00)\tConsistency Loss 1.6862e+00 (1.6862e+00)\tInconsistency Loss 1.9994e-01 (1.9994e-01)\tEntropy 2.0467e+00 (2.0467e+00)\n",
      "Epoch: [3][100/101]\tTotal Loss -2.3601e+00 (-2.3769e+00)\tConsistency Loss 1.7044e+00 (1.7193e+00)\tInconsistency Loss 2.0410e-01 (1.9757e-01)\tEntropy 2.0322e+00 (2.0481e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/101]\tTotal Loss -2.3638e+00 (-2.3638e+00)\tConsistency Loss 1.6816e+00 (1.6816e+00)\tInconsistency Loss 2.0639e-01 (2.0639e-01)\tEntropy 2.0227e+00 (2.0227e+00)\n",
      "Epoch: [4][100/101]\tTotal Loss -2.3909e+00 (-2.3771e+00)\tConsistency Loss 1.7363e+00 (1.7213e+00)\tInconsistency Loss 1.9375e-01 (1.9740e-01)\tEntropy 2.0636e+00 (2.0492e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/101]\tTotal Loss -2.4012e+00 (-2.4012e+00)\tConsistency Loss 1.7119e+00 (1.7119e+00)\tInconsistency Loss 1.9552e-01 (1.9552e-01)\tEntropy 2.0565e+00 (2.0565e+00)\n",
      "Epoch: [5][100/101]\tTotal Loss -2.3641e+00 (-2.3757e+00)\tConsistency Loss 1.7184e+00 (1.7246e+00)\tInconsistency Loss 2.0026e-01 (1.9699e-01)\tEntropy 2.0412e+00 (2.0502e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/101]\tTotal Loss -2.3492e+00 (-2.3492e+00)\tConsistency Loss 1.7079e+00 (1.7079e+00)\tInconsistency Loss 2.0636e-01 (2.0636e-01)\tEntropy 2.0285e+00 (2.0285e+00)\n",
      "Epoch: [6][100/101]\tTotal Loss -2.3636e+00 (-2.3763e+00)\tConsistency Loss 1.7273e+00 (1.7202e+00)\tInconsistency Loss 1.9825e-01 (1.9751e-01)\tEntropy 2.0454e+00 (2.0483e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/101]\tTotal Loss -2.3783e+00 (-2.3783e+00)\tConsistency Loss 1.7283e+00 (1.7283e+00)\tInconsistency Loss 1.9472e-01 (1.9472e-01)\tEntropy 2.0533e+00 (2.0533e+00)\n",
      "Epoch: [7][100/101]\tTotal Loss -2.3825e+00 (-2.3766e+00)\tConsistency Loss 1.7204e+00 (1.7211e+00)\tInconsistency Loss 1.9646e-01 (1.9715e-01)\tEntropy 2.0514e+00 (2.0489e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/101]\tTotal Loss -2.3846e+00 (-2.3846e+00)\tConsistency Loss 1.7226e+00 (1.7226e+00)\tInconsistency Loss 1.9643e-01 (1.9643e-01)\tEntropy 2.0536e+00 (2.0536e+00)\n",
      "Epoch: [8][100/101]\tTotal Loss -2.3824e+00 (-2.3770e+00)\tConsistency Loss 1.7295e+00 (1.7190e+00)\tInconsistency Loss 1.9466e-01 (1.9765e-01)\tEntropy 2.0560e+00 (2.0480e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/101]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7160e+00 (1.7160e+00)\tInconsistency Loss 1.9853e-01 (1.9853e-01)\tEntropy 2.0465e+00 (2.0465e+00)\n",
      "Epoch: [9][100/101]\tTotal Loss -2.3786e+00 (-2.3767e+00)\tConsistency Loss 1.7121e+00 (1.7193e+00)\tInconsistency Loss 1.9824e-01 (1.9758e-01)\tEntropy 2.0454e+00 (2.0480e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/101]\tTotal Loss -2.3763e+00 (-2.3763e+00)\tConsistency Loss 1.7102e+00 (1.7102e+00)\tInconsistency Loss 1.9952e-01 (1.9952e-01)\tEntropy 2.0433e+00 (2.0433e+00)\n",
      "Epoch: [10][100/101]\tTotal Loss -2.3780e+00 (-2.3771e+00)\tConsistency Loss 1.7246e+00 (1.7180e+00)\tInconsistency Loss 1.9612e-01 (1.9781e-01)\tEntropy 2.0513e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/101]\tTotal Loss -2.3787e+00 (-2.3787e+00)\tConsistency Loss 1.7232e+00 (1.7232e+00)\tInconsistency Loss 1.9666e-01 (1.9666e-01)\tEntropy 2.0509e+00 (2.0509e+00)\n",
      "Epoch: [11][100/101]\tTotal Loss -2.3759e+00 (-2.3769e+00)\tConsistency Loss 1.7194e+00 (1.7191e+00)\tInconsistency Loss 1.9739e-01 (1.9754e-01)\tEntropy 2.0477e+00 (2.0480e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/101]\tTotal Loss -2.3757e+00 (-2.3757e+00)\tConsistency Loss 1.7187e+00 (1.7187e+00)\tInconsistency Loss 1.9782e-01 (1.9782e-01)\tEntropy 2.0472e+00 (2.0472e+00)\n",
      "Epoch: [12][100/101]\tTotal Loss -2.3768e+00 (-2.3771e+00)\tConsistency Loss 1.7182e+00 (1.7182e+00)\tInconsistency Loss 1.9788e-01 (1.9771e-01)\tEntropy 2.0475e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/101]\tTotal Loss -2.3766e+00 (-2.3766e+00)\tConsistency Loss 1.7186e+00 (1.7186e+00)\tInconsistency Loss 1.9783e-01 (1.9783e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/101]\tTotal Loss -2.3770e+00 (-2.3771e+00)\tConsistency Loss 1.7177e+00 (1.7183e+00)\tInconsistency Loss 1.9782e-01 (1.9769e-01)\tEntropy 2.0473e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/101]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7184e+00 (1.7184e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [14][100/101]\tTotal Loss -2.3770e+00 (-2.3772e+00)\tConsistency Loss 1.7190e+00 (1.7180e+00)\tInconsistency Loss 1.9763e-01 (1.9774e-01)\tEntropy 2.0480e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/101]\tTotal Loss -2.3774e+00 (-2.3774e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9771e-01 (1.9771e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [15][100/101]\tTotal Loss -2.3774e+00 (-2.3772e+00)\tConsistency Loss 1.7185e+00 (1.7181e+00)\tInconsistency Loss 1.9756e-01 (1.9773e-01)\tEntropy 2.0479e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/101]\tTotal Loss -2.3775e+00 (-2.3775e+00)\tConsistency Loss 1.7186e+00 (1.7186e+00)\tInconsistency Loss 1.9760e-01 (1.9760e-01)\tEntropy 2.0481e+00 (2.0481e+00)\n",
      "Epoch: [16][100/101]\tTotal Loss -2.3774e+00 (-2.3772e+00)\tConsistency Loss 1.7186e+00 (1.7181e+00)\tInconsistency Loss 1.9764e-01 (1.9772e-01)\tEntropy 2.0480e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/101]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7179e+00 (1.7179e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0475e+00 (2.0475e+00)\n",
      "Epoch: [17][100/101]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7178e+00 (1.7181e+00)\tInconsistency Loss 1.9780e-01 (1.9772e-01)\tEntropy 2.0475e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7179e+00 (1.7179e+00)\tInconsistency Loss 1.9777e-01 (1.9777e-01)\tEntropy 2.0475e+00 (2.0475e+00)\n",
      "Epoch: [18][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7179e+00 (1.7181e+00)\tInconsistency Loss 1.9778e-01 (1.9773e-01)\tEntropy 2.0475e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7179e+00 (1.7179e+00)\tInconsistency Loss 1.9778e-01 (1.9778e-01)\tEntropy 2.0475e+00 (2.0475e+00)\n",
      "Epoch: [19][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/101]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "A-4\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2490/7880 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/A-4/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/49]\tLoss 1.0027e+00 (1.0027e+00)\n",
      "Epoch: [1][10/49]\tLoss 3.4824e-01 (6.2440e-01)\n",
      "Epoch: [1][20/49]\tLoss 1.2125e-01 (4.2423e-01)\n",
      "Epoch: [1][30/49]\tLoss 4.2311e-02 (3.1032e-01)\n",
      "Epoch: [1][40/49]\tLoss 1.4723e-02 (2.4068e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/49]\tLoss 1.0072e-02 (1.0072e-02)\n",
      "Epoch: [2][10/49]\tLoss 1.0039e-02 (1.0024e-02)\n",
      "Epoch: [2][20/49]\tLoss 1.0042e-02 (1.0025e-02)\n",
      "Epoch: [2][30/49]\tLoss 1.0018e-02 (1.0022e-02)\n",
      "Epoch: [2][40/49]\tLoss 1.0015e-02 (1.0023e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/49]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [3][10/49]\tLoss 1.0007e-02 (1.0015e-02)\n",
      "Epoch: [3][20/49]\tLoss 1.0012e-02 (1.0015e-02)\n",
      "Epoch: [3][30/49]\tLoss 1.0007e-02 (1.0013e-02)\n",
      "Epoch: [3][40/49]\tLoss 1.0005e-02 (1.0012e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/49]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [4][10/49]\tLoss 1.0001e-02 (1.0008e-02)\n",
      "Epoch: [4][20/49]\tLoss 1.0004e-02 (1.0007e-02)\n",
      "Epoch: [4][30/49]\tLoss 1.0005e-02 (1.0008e-02)\n",
      "Epoch: [4][40/49]\tLoss 1.0009e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/49]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [5][10/49]\tLoss 1.0001e-02 (1.0006e-02)\n",
      "Epoch: [5][20/49]\tLoss 1.0009e-02 (1.0006e-02)\n",
      "Epoch: [5][30/49]\tLoss 1.0005e-02 (1.0006e-02)\n",
      "Epoch: [5][40/49]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/49]\tLoss 1.0009e-02 (1.0009e-02)\n",
      "Epoch: [6][10/49]\tLoss 1.0001e-02 (1.0006e-02)\n",
      "Epoch: [6][20/49]\tLoss 1.0001e-02 (1.0006e-02)\n",
      "Epoch: [6][30/49]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "Epoch: [6][40/49]\tLoss 1.0003e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/49]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [7][10/49]\tLoss 1.0003e-02 (1.0005e-02)\n",
      "Epoch: [7][20/49]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [7][30/49]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [7][40/49]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/49]\tLoss 1.0009e-02 (1.0009e-02)\n",
      "Epoch: [8][10/49]\tLoss 1.0005e-02 (1.0004e-02)\n",
      "Epoch: [8][20/49]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][30/49]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [8][40/49]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/49]\tLoss 1.0008e-02 (1.0008e-02)\n",
      "Epoch: [9][10/49]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][20/49]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [9][30/49]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [9][40/49]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [10][10/49]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][20/49]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [10][30/49]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [10][40/49]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/49]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][10/49]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][20/49]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][30/49]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][40/49]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][10/49]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [12][20/49]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][30/49]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][40/49]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/49]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][10/49]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][20/49]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][30/49]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][40/49]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/49]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][10/49]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [14][20/49]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [14][30/49]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [14][40/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/49]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [15][10/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][20/49]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [15][30/49]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [15][40/49]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/49]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [16][10/49]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [16][20/49]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [16][30/49]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [16][40/49]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][20/49]\tLoss 1.0000e-02 (1.0005e-02)\n",
      "Epoch: [17][30/49]\tLoss 1.0000e-02 (1.0004e-02)\n",
      "Epoch: [17][40/49]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/49]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][10/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][20/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][40/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/49]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [19][20/49]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [19][30/49]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [19][40/49]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/49]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][10/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][20/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][30/49]\tLoss 1.0007e-02 (1.0001e-02)\n",
      "Epoch: [20][40/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/49]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][10/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][20/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][30/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][40/49]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/49]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][20/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][30/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][40/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/49]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][20/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][30/49]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [23][40/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][10/49]\tLoss 1.0005e-02 (1.0001e-02)\n",
      "Epoch: [24][20/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][30/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][40/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/49]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][20/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][30/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][40/49]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/49]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][20/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][30/49]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [26][40/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/49]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][20/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][30/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][40/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/49]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [28][10/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][20/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][30/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][40/49]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][10/49]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [29][20/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][30/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][40/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/49]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][20/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][30/49]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/49]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/50]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/158]\n",
      "Fill TS Repository [100/158]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries A-4\u001b[0m\n",
      "\u001b[32m-- Train samples size: 4980 - Test samples size: 7880\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/A-4/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][ 0/99]\tTotal Loss -2.3040e+00 (-2.3040e+00)\tConsistency Loss 2.2558e+00 (2.2558e+00)\tInconsistency Loss 1.1033e-01 (1.1033e-01)\tEntropy 2.2799e+00 (2.2799e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][ 0/99]\tTotal Loss -2.3716e+00 (-2.3716e+00)\tConsistency Loss 1.7255e+00 (1.7255e+00)\tInconsistency Loss 1.9682e-01 (1.9682e-01)\tEntropy 2.0486e+00 (2.0486e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7197e+00 (1.7197e+00)\tInconsistency Loss 1.9735e-01 (1.9735e-01)\tEntropy 2.0484e+00 (2.0484e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][ 0/99]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7205e+00 (1.7205e+00)\tInconsistency Loss 1.9725e-01 (1.9725e-01)\tEntropy 2.0488e+00 (2.0488e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][ 0/99]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7127e+00 (1.7127e+00)\tInconsistency Loss 1.9889e-01 (1.9889e-01)\tEntropy 2.0450e+00 (2.0450e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][ 0/99]\tTotal Loss -2.3767e+00 (-2.3767e+00)\tConsistency Loss 1.7192e+00 (1.7192e+00)\tInconsistency Loss 1.9766e-01 (1.9766e-01)\tEntropy 2.0479e+00 (2.0479e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][ 0/99]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7172e+00 (1.7172e+00)\tInconsistency Loss 1.9795e-01 (1.9795e-01)\tEntropy 2.0471e+00 (2.0471e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7175e+00 (1.7175e+00)\tInconsistency Loss 1.9784e-01 (1.9784e-01)\tEntropy 2.0474e+00 (2.0474e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7191e+00 (1.7191e+00)\tInconsistency Loss 1.9751e-01 (1.9751e-01)\tEntropy 2.0481e+00 (2.0481e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][ 0/99]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "G-1\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2620/8269 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/G-1/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/52]\tLoss 1.0038e+00 (1.0038e+00)\n",
      "Epoch: [1][10/52]\tLoss 3.4820e-01 (6.2440e-01)\n",
      "Epoch: [1][20/52]\tLoss 1.2126e-01 (4.2424e-01)\n",
      "Epoch: [1][30/52]\tLoss 4.2365e-02 (3.1032e-01)\n",
      "Epoch: [1][40/52]\tLoss 1.4791e-02 (2.4068e-01)\n",
      "Epoch: [1][50/52]\tLoss 1.0025e-02 (1.9557e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/52]\tLoss 1.0025e-02 (1.0025e-02)\n",
      "Epoch: [2][10/52]\tLoss 1.0011e-02 (1.0029e-02)\n",
      "Epoch: [2][20/52]\tLoss 1.0049e-02 (1.0034e-02)\n",
      "Epoch: [2][30/52]\tLoss 1.0030e-02 (1.0029e-02)\n",
      "Epoch: [2][40/52]\tLoss 1.0004e-02 (1.0025e-02)\n",
      "Epoch: [2][50/52]\tLoss 1.0020e-02 (1.0024e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/52]\tLoss 1.0018e-02 (1.0018e-02)\n",
      "Epoch: [3][10/52]\tLoss 1.0011e-02 (1.0014e-02)\n",
      "Epoch: [3][20/52]\tLoss 1.0010e-02 (1.0013e-02)\n",
      "Epoch: [3][30/52]\tLoss 1.0016e-02 (1.0013e-02)\n",
      "Epoch: [3][40/52]\tLoss 1.0005e-02 (1.0012e-02)\n",
      "Epoch: [3][50/52]\tLoss 1.0006e-02 (1.0013e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/52]\tLoss 1.0008e-02 (1.0008e-02)\n",
      "Epoch: [4][10/52]\tLoss 1.0009e-02 (1.0013e-02)\n",
      "Epoch: [4][20/52]\tLoss 1.0006e-02 (1.0011e-02)\n",
      "Epoch: [4][30/52]\tLoss 1.0005e-02 (1.0010e-02)\n",
      "Epoch: [4][40/52]\tLoss 1.0006e-02 (1.0009e-02)\n",
      "Epoch: [4][50/52]\tLoss 1.0027e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/52]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [5][10/52]\tLoss 1.0005e-02 (1.0006e-02)\n",
      "Epoch: [5][20/52]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [5][30/52]\tLoss 1.0003e-02 (1.0006e-02)\n",
      "Epoch: [5][40/52]\tLoss 1.0003e-02 (1.0005e-02)\n",
      "Epoch: [5][50/52]\tLoss 1.0002e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/52]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [6][10/52]\tLoss 1.0002e-02 (1.0007e-02)\n",
      "Epoch: [6][20/52]\tLoss 1.0003e-02 (1.0006e-02)\n",
      "Epoch: [6][30/52]\tLoss 1.0003e-02 (1.0007e-02)\n",
      "Epoch: [6][40/52]\tLoss 1.0007e-02 (1.0006e-02)\n",
      "Epoch: [6][50/52]\tLoss 1.0029e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [7][10/52]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [7][20/52]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [7][30/52]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [7][40/52]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [7][50/52]\tLoss 1.0005e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/52]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][10/52]\tLoss 1.0005e-02 (1.0003e-02)\n",
      "Epoch: [8][20/52]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][30/52]\tLoss 1.0013e-02 (1.0003e-02)\n",
      "Epoch: [8][40/52]\tLoss 1.0115e-02 (1.0006e-02)\n",
      "Epoch: [8][50/52]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][10/52]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [9][20/52]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [9][30/52]\tLoss 1.0016e-02 (1.0004e-02)\n",
      "Epoch: [9][40/52]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [9][50/52]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [10][10/52]\tLoss 1.0001e-02 (1.0009e-02)\n",
      "Epoch: [10][20/52]\tLoss 1.0001e-02 (1.0006e-02)\n",
      "Epoch: [10][30/52]\tLoss 1.0003e-02 (1.0005e-02)\n",
      "Epoch: [10][40/52]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [10][50/52]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][10/52]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [11][20/52]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [11][30/52]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [11][40/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][50/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][10/52]\tLoss 1.0006e-02 (1.0003e-02)\n",
      "Epoch: [12][20/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][30/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][40/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][50/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][10/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][20/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][30/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][40/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][50/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/52]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [14][10/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [14][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][30/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [14][40/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [14][50/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][10/52]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [15][20/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [15][30/52]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [15][40/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [15][50/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/52]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [16][10/52]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [16][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][20/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][30/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][40/52]\tLoss 1.0010e-02 (1.0001e-02)\n",
      "Epoch: [17][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][10/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [18][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/52]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [19][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][10/52]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [20][20/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [20][30/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [22][10/52]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [22][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][30/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][20/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][30/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][30/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][30/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][10/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [26][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [27][20/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][10/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [28][20/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [28][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][30/52]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [29][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][20/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][30/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/53]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/166]\n",
      "Fill TS Repository [100/166]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries G-1\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5240 - Test samples size: 8269\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/G-1/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/104]\tTotal Loss -2.3009e+00 (-2.3009e+00)\tConsistency Loss 2.2582e+00 (2.2582e+00)\tInconsistency Loss 1.1043e-01 (1.1043e-01)\tEntropy 2.2796e+00 (2.2796e+00)\n",
      "Epoch: [1][100/104]\tTotal Loss -2.3842e+00 (-2.3595e+00)\tConsistency Loss 1.7342e+00 (1.8487e+00)\tInconsistency Loss 1.9439e-01 (1.7638e-01)\tEntropy 2.0592e+00 (2.1041e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/104]\tTotal Loss -2.3504e+00 (-2.3504e+00)\tConsistency Loss 1.6863e+00 (1.6863e+00)\tInconsistency Loss 2.0730e-01 (2.0730e-01)\tEntropy 2.0183e+00 (2.0183e+00)\n",
      "Epoch: [2][100/104]\tTotal Loss -2.3791e+00 (-2.3766e+00)\tConsistency Loss 1.7360e+00 (1.7199e+00)\tInconsistency Loss 1.9236e-01 (1.9737e-01)\tEntropy 2.0576e+00 (2.0483e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/104]\tTotal Loss -2.3892e+00 (-2.3892e+00)\tConsistency Loss 1.7450e+00 (1.7450e+00)\tInconsistency Loss 1.9182e-01 (1.9182e-01)\tEntropy 2.0671e+00 (2.0671e+00)\n",
      "Epoch: [3][100/104]\tTotal Loss -2.3774e+00 (-2.3760e+00)\tConsistency Loss 1.7183e+00 (1.7218e+00)\tInconsistency Loss 1.9768e-01 (1.9689e-01)\tEntropy 2.0479e+00 (2.0489e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/104]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7208e+00 (1.7208e+00)\tInconsistency Loss 1.9719e-01 (1.9719e-01)\tEntropy 2.0489e+00 (2.0489e+00)\n",
      "Epoch: [4][100/104]\tTotal Loss -2.3773e+00 (-2.3760e+00)\tConsistency Loss 1.7318e+00 (1.7224e+00)\tInconsistency Loss 1.9564e-01 (1.9687e-01)\tEntropy 2.0546e+00 (2.0492e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/104]\tTotal Loss -2.3803e+00 (-2.3803e+00)\tConsistency Loss 1.7301e+00 (1.7301e+00)\tInconsistency Loss 1.9445e-01 (1.9445e-01)\tEntropy 2.0552e+00 (2.0552e+00)\n",
      "Epoch: [5][100/104]\tTotal Loss -2.3812e+00 (-2.3768e+00)\tConsistency Loss 1.7277e+00 (1.7196e+00)\tInconsistency Loss 1.9601e-01 (1.9744e-01)\tEntropy 2.0544e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/104]\tTotal Loss -2.3769e+00 (-2.3769e+00)\tConsistency Loss 1.7145e+00 (1.7145e+00)\tInconsistency Loss 1.9910e-01 (1.9910e-01)\tEntropy 2.0457e+00 (2.0457e+00)\n",
      "Epoch: [6][100/104]\tTotal Loss -2.3772e+00 (-2.3770e+00)\tConsistency Loss 1.7177e+00 (1.7187e+00)\tInconsistency Loss 1.9782e-01 (1.9761e-01)\tEntropy 2.0474e+00 (2.0478e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7184e+00 (1.7184e+00)\tInconsistency Loss 1.9767e-01 (1.9767e-01)\tEntropy 2.0478e+00 (2.0478e+00)\n",
      "Epoch: [7][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7201e+00 (1.7182e+00)\tInconsistency Loss 1.9729e-01 (1.9771e-01)\tEntropy 2.0486e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7186e+00 (1.7186e+00)\tInconsistency Loss 1.9762e-01 (1.9762e-01)\tEntropy 2.0479e+00 (2.0479e+00)\n",
      "Epoch: [8][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7182e+00)\tInconsistency Loss 1.9766e-01 (1.9771e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [9][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7182e+00)\tInconsistency Loss 1.9770e-01 (1.9771e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [10][100/104]\tTotal Loss -2.3774e+00 (-2.3771e+00)\tConsistency Loss 1.7205e+00 (1.7184e+00)\tInconsistency Loss 1.9720e-01 (1.9767e-01)\tEntropy 2.0489e+00 (2.0478e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/104]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7179e+00 (1.7179e+00)\tInconsistency Loss 1.9777e-01 (1.9777e-01)\tEntropy 2.0475e+00 (2.0475e+00)\n",
      "Epoch: [11][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7179e+00 (1.7180e+00)\tInconsistency Loss 1.9777e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7182e+00)\tInconsistency Loss 1.9771e-01 (1.9771e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [12][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "G-2\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2278/7161 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/G-2/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/45]\tLoss 1.0036e+00 (1.0036e+00)\n",
      "Epoch: [1][10/45]\tLoss 3.4843e-01 (6.2432e-01)\n",
      "Epoch: [1][20/45]\tLoss 1.2142e-01 (4.2423e-01)\n",
      "Epoch: [1][30/45]\tLoss 4.2285e-02 (3.1032e-01)\n",
      "Epoch: [1][40/45]\tLoss 1.4747e-02 (2.4068e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/45]\tLoss 1.0066e-02 (1.0066e-02)\n",
      "Epoch: [2][10/45]\tLoss 1.0156e-02 (1.0049e-02)\n",
      "Epoch: [2][20/45]\tLoss 1.0023e-02 (1.0033e-02)\n",
      "Epoch: [2][30/45]\tLoss 1.0013e-02 (1.0029e-02)\n",
      "Epoch: [2][40/45]\tLoss 1.0008e-02 (1.0025e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/45]\tLoss 1.0009e-02 (1.0009e-02)\n",
      "Epoch: [3][10/45]\tLoss 1.0005e-02 (1.0027e-02)\n",
      "Epoch: [3][20/45]\tLoss 1.0020e-02 (1.0022e-02)\n",
      "Epoch: [3][30/45]\tLoss 1.0001e-02 (1.0016e-02)\n",
      "Epoch: [3][40/45]\tLoss 1.0005e-02 (1.0016e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/45]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [4][10/45]\tLoss 1.0004e-02 (1.0008e-02)\n",
      "Epoch: [4][20/45]\tLoss 1.0002e-02 (1.0008e-02)\n",
      "Epoch: [4][30/45]\tLoss 1.0005e-02 (1.0007e-02)\n",
      "Epoch: [4][40/45]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/45]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [5][10/45]\tLoss 1.0006e-02 (1.0005e-02)\n",
      "Epoch: [5][20/45]\tLoss 1.0007e-02 (1.0005e-02)\n",
      "Epoch: [5][30/45]\tLoss 1.0002e-02 (1.0005e-02)\n",
      "Epoch: [5][40/45]\tLoss 1.0002e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/45]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [6][10/45]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [6][20/45]\tLoss 1.0016e-02 (1.0003e-02)\n",
      "Epoch: [6][30/45]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [6][40/45]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/45]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [7][10/45]\tLoss 1.0002e-02 (1.0015e-02)\n",
      "Epoch: [7][20/45]\tLoss 1.0002e-02 (1.0009e-02)\n",
      "Epoch: [7][30/45]\tLoss 1.0002e-02 (1.0007e-02)\n",
      "Epoch: [7][40/45]\tLoss 1.0012e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/45]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [8][10/45]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][20/45]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][30/45]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][40/45]\tLoss 1.0005e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/45]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][10/45]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [9][20/45]\tLoss 1.0048e-02 (1.0003e-02)\n",
      "Epoch: [9][30/45]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [9][40/45]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [10][10/45]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [10][20/45]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [10][30/45]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][40/45]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/45]\tLoss 1.0011e-02 (1.0011e-02)\n",
      "Epoch: [11][10/45]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [11][20/45]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [11][30/45]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [11][40/45]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][10/45]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [12][20/45]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [12][30/45]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [12][40/45]\tLoss 1.0015e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][10/45]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [13][20/45]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [13][30/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][40/45]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/45]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [14][10/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][20/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][30/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][40/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/45]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [15][10/45]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [15][20/45]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [15][30/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][40/45]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][10/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/45]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [16][30/45]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [16][40/45]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/45]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][20/45]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][30/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][40/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][10/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][20/45]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [18][30/45]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [18][40/45]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/45]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [19][30/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][40/45]\tLoss 1.0016e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/45]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][10/45]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [20][20/45]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [20][30/45]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [20][40/45]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/45]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [21][10/45]\tLoss 1.0005e-02 (1.0001e-02)\n",
      "Epoch: [21][20/45]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][30/45]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][40/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/45]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][20/45]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][30/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][40/45]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][10/45]\tLoss 1.0000e-02 (1.0004e-02)\n",
      "Epoch: [23][20/45]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [23][30/45]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [23][40/45]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/45]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][10/45]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][20/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][30/45]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][40/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][10/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][20/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][30/45]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [25][40/45]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/45]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [26][10/45]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][20/45]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][30/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][40/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][10/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][20/45]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [27][30/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][40/45]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/45]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][10/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][20/45]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][30/45]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][40/45]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][10/45]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][20/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][30/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][40/45]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/45]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/45]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [30][20/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][30/45]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [30][40/45]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/46]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/144]\n",
      "Fill TS Repository [100/144]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries G-2\u001b[0m\n",
      "\u001b[32m-- Train samples size: 4556 - Test samples size: 7161\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/G-2/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][ 0/91]\tTotal Loss -2.3016e+00 (-2.3016e+00)\tConsistency Loss 2.2590e+00 (2.2590e+00)\tInconsistency Loss 1.1047e-01 (1.1047e-01)\tEntropy 2.2803e+00 (2.2803e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][ 0/91]\tTotal Loss -2.3613e+00 (-2.3613e+00)\tConsistency Loss 1.6817e+00 (1.6817e+00)\tInconsistency Loss 2.1050e-01 (2.1050e-01)\tEntropy 2.0215e+00 (2.0215e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][ 0/91]\tTotal Loss -2.3747e+00 (-2.3747e+00)\tConsistency Loss 1.7022e+00 (1.7022e+00)\tInconsistency Loss 2.0120e-01 (2.0120e-01)\tEntropy 2.0385e+00 (2.0385e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][ 0/91]\tTotal Loss -2.3767e+00 (-2.3767e+00)\tConsistency Loss 1.7301e+00 (1.7301e+00)\tInconsistency Loss 1.9556e-01 (1.9556e-01)\tEntropy 2.0534e+00 (2.0534e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][ 0/91]\tTotal Loss -2.3782e+00 (-2.3782e+00)\tConsistency Loss 1.7140e+00 (1.7140e+00)\tInconsistency Loss 1.9867e-01 (1.9867e-01)\tEntropy 2.0461e+00 (2.0461e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][ 0/91]\tTotal Loss -2.3768e+00 (-2.3768e+00)\tConsistency Loss 1.7288e+00 (1.7288e+00)\tInconsistency Loss 1.9558e-01 (1.9558e-01)\tEntropy 2.0528e+00 (2.0528e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][ 0/91]\tTotal Loss -2.3779e+00 (-2.3779e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9775e-01 (1.9775e-01)\tEntropy 2.0479e+00 (2.0479e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][ 0/91]\tTotal Loss -2.3778e+00 (-2.3778e+00)\tConsistency Loss 1.7184e+00 (1.7184e+00)\tInconsistency Loss 1.9756e-01 (1.9756e-01)\tEntropy 2.0481e+00 (2.0481e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][ 0/91]\tTotal Loss -2.3776e+00 (-2.3776e+00)\tConsistency Loss 1.7183e+00 (1.7183e+00)\tInconsistency Loss 1.9768e-01 (1.9768e-01)\tEntropy 2.0479e+00 (2.0479e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][ 0/91]\tTotal Loss -2.3769e+00 (-2.3769e+00)\tConsistency Loss 1.7175e+00 (1.7175e+00)\tInconsistency Loss 1.9804e-01 (1.9804e-01)\tEntropy 2.0472e+00 (2.0472e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][ 0/91]\tTotal Loss -2.3764e+00 (-2.3764e+00)\tConsistency Loss 1.7214e+00 (1.7214e+00)\tInconsistency Loss 1.9702e-01 (1.9702e-01)\tEntropy 2.0489e+00 (2.0489e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][ 0/91]\tTotal Loss -2.3766e+00 (-2.3766e+00)\tConsistency Loss 1.7196e+00 (1.7196e+00)\tInconsistency Loss 1.9764e-01 (1.9764e-01)\tEntropy 2.0481e+00 (2.0481e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][ 0/91]\tTotal Loss -2.3775e+00 (-2.3775e+00)\tConsistency Loss 1.7186e+00 (1.7186e+00)\tInconsistency Loss 1.9765e-01 (1.9765e-01)\tEntropy 2.0480e+00 (2.0480e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][ 0/91]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9776e-01 (1.9776e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][ 0/91]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "D-5\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2361/7428 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/D-5/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/47]\tLoss 1.0000e+00 (1.0000e+00)\n",
      "Epoch: [1][10/47]\tLoss 3.4867e-01 (6.2382e-01)\n",
      "Epoch: [1][20/47]\tLoss 1.2157e-01 (4.2409e-01)\n",
      "Epoch: [1][30/47]\tLoss 4.2396e-02 (3.1027e-01)\n",
      "Epoch: [1][40/47]\tLoss 1.4780e-02 (2.4066e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [2][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [2][20/47]\tLoss 1.0000e-02 (1.0003e-02)\n",
      "Epoch: [2][30/47]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [2][40/47]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [3][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [3][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [3][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [3][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [4][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [4][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [4][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [4][40/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [8][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [8][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [8][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [8][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][20/47]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [10][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][20/47]\tLoss 1.0000e-02 (1.0003e-02)\n",
      "Epoch: [11][30/47]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [11][40/47]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][40/47]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/48]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/149]\n",
      "Fill TS Repository [100/149]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries D-5\u001b[0m\n",
      "\u001b[32m-- Train samples size: 4722 - Test samples size: 7428\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/D-5/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][ 0/94]\tTotal Loss -2.3010e+00 (-2.3010e+00)\tConsistency Loss 2.2535e+00 (2.2535e+00)\tInconsistency Loss 1.1099e-01 (1.1099e-01)\tEntropy 2.2772e+00 (2.2772e+00)\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][ 0/94]\tTotal Loss -2.3643e+00 (-2.3643e+00)\tConsistency Loss 1.7354e+00 (1.7354e+00)\tInconsistency Loss 1.9865e-01 (1.9865e-01)\tEntropy 2.0499e+00 (2.0499e+00)\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][ 0/94]\tTotal Loss -2.3673e+00 (-2.3673e+00)\tConsistency Loss 1.7403e+00 (1.7403e+00)\tInconsistency Loss 1.9336e-01 (1.9336e-01)\tEntropy 2.0538e+00 (2.0538e+00)\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][ 0/94]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7089e+00 (1.7089e+00)\tInconsistency Loss 2.0049e-01 (2.0049e-01)\tEntropy 2.0431e+00 (2.0431e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][ 0/94]\tTotal Loss -2.3719e+00 (-2.3719e+00)\tConsistency Loss 1.6952e+00 (1.6952e+00)\tInconsistency Loss 2.0069e-01 (2.0069e-01)\tEntropy 2.0336e+00 (2.0336e+00)\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][ 0/94]\tTotal Loss -2.3780e+00 (-2.3780e+00)\tConsistency Loss 1.7278e+00 (1.7278e+00)\tInconsistency Loss 1.9508e-01 (1.9508e-01)\tEntropy 2.0529e+00 (2.0529e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][ 0/94]\tTotal Loss -2.3782e+00 (-2.3782e+00)\tConsistency Loss 1.7095e+00 (1.7095e+00)\tInconsistency Loss 1.9956e-01 (1.9956e-01)\tEntropy 2.0438e+00 (2.0438e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][ 0/94]\tTotal Loss -2.3731e+00 (-2.3731e+00)\tConsistency Loss 1.6987e+00 (1.6987e+00)\tInconsistency Loss 2.0202e-01 (2.0202e-01)\tEntropy 2.0359e+00 (2.0359e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][ 0/94]\tTotal Loss -2.3737e+00 (-2.3737e+00)\tConsistency Loss 1.7121e+00 (1.7121e+00)\tInconsistency Loss 1.9925e-01 (1.9925e-01)\tEntropy 2.0429e+00 (2.0429e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][ 0/94]\tTotal Loss -2.3784e+00 (-2.3784e+00)\tConsistency Loss 1.7207e+00 (1.7207e+00)\tInconsistency Loss 1.9684e-01 (1.9684e-01)\tEntropy 2.0495e+00 (2.0495e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][ 0/94]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7118e+00 (1.7118e+00)\tInconsistency Loss 1.9907e-01 (1.9907e-01)\tEntropy 2.0445e+00 (2.0445e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][ 0/94]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "D-6\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2394/7684 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/D-6/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/47]\tLoss 1.0037e+00 (1.0037e+00)\n",
      "Epoch: [1][10/47]\tLoss 3.4857e-01 (6.2440e-01)\n",
      "Epoch: [1][20/47]\tLoss 1.2128e-01 (4.2426e-01)\n",
      "Epoch: [1][30/47]\tLoss 4.2248e-02 (3.1033e-01)\n",
      "Epoch: [1][40/47]\tLoss 1.4741e-02 (2.4068e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/47]\tLoss 1.0028e-02 (1.0028e-02)\n",
      "Epoch: [2][10/47]\tLoss 1.0028e-02 (1.0045e-02)\n",
      "Epoch: [2][20/47]\tLoss 1.0016e-02 (1.0034e-02)\n",
      "Epoch: [2][30/47]\tLoss 1.0009e-02 (1.0033e-02)\n",
      "Epoch: [2][40/47]\tLoss 1.0011e-02 (1.0031e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/47]\tLoss 1.0033e-02 (1.0033e-02)\n",
      "Epoch: [3][10/47]\tLoss 1.0004e-02 (1.0019e-02)\n",
      "Epoch: [3][20/47]\tLoss 1.0050e-02 (1.0022e-02)\n",
      "Epoch: [3][30/47]\tLoss 1.0016e-02 (1.0018e-02)\n",
      "Epoch: [3][40/47]\tLoss 1.0015e-02 (1.0016e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/47]\tLoss 1.0027e-02 (1.0027e-02)\n",
      "Epoch: [4][10/47]\tLoss 1.0007e-02 (1.0012e-02)\n",
      "Epoch: [4][20/47]\tLoss 1.0006e-02 (1.0012e-02)\n",
      "Epoch: [4][30/47]\tLoss 1.0007e-02 (1.0011e-02)\n",
      "Epoch: [4][40/47]\tLoss 1.0014e-02 (1.0010e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/47]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [5][10/47]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [5][20/47]\tLoss 1.0004e-02 (1.0008e-02)\n",
      "Epoch: [5][30/47]\tLoss 1.0014e-02 (1.0008e-02)\n",
      "Epoch: [5][40/47]\tLoss 1.0003e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [6][10/47]\tLoss 1.0003e-02 (1.0005e-02)\n",
      "Epoch: [6][20/47]\tLoss 1.0008e-02 (1.0007e-02)\n",
      "Epoch: [6][30/47]\tLoss 1.0005e-02 (1.0007e-02)\n",
      "Epoch: [6][40/47]\tLoss 1.0001e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [7][10/47]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [7][20/47]\tLoss 1.0004e-02 (1.0006e-02)\n",
      "Epoch: [7][30/47]\tLoss 1.0002e-02 (1.0006e-02)\n",
      "Epoch: [7][40/47]\tLoss 1.0003e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/47]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][10/47]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [8][20/47]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [8][30/47]\tLoss 1.0007e-02 (1.0003e-02)\n",
      "Epoch: [8][40/47]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/47]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [9][10/47]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [9][20/47]\tLoss 1.0007e-02 (1.0003e-02)\n",
      "Epoch: [9][30/47]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [9][40/47]\tLoss 1.0005e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][10/47]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [10][20/47]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [10][30/47]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [10][40/47]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][10/47]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][20/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][30/47]\tLoss 1.0011e-02 (1.0003e-02)\n",
      "Epoch: [11][40/47]\tLoss 1.0011e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/47]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [12][10/47]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [12][20/47]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [12][30/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][40/47]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][10/47]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][20/47]\tLoss 1.0005e-02 (1.0002e-02)\n",
      "Epoch: [13][30/47]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [13][40/47]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][10/47]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [14][20/47]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [14][30/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [14][40/47]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/47]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [15][10/47]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [15][20/47]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [15][30/47]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [15][40/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/47]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [16][10/47]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [16][20/47]\tLoss 1.0006e-02 (1.0002e-02)\n",
      "Epoch: [16][30/47]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [16][40/47]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [17][10/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][30/47]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [17][40/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][10/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][20/47]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [18][30/47]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [18][40/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][40/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/47]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [20][10/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][30/47]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [20][40/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][20/47]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [21][30/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][40/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][10/47]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [22][20/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [22][30/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][40/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [23][10/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][20/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][30/47]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [23][40/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][10/47]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [24][20/47]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [24][30/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][40/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][30/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][40/47]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][10/47]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [26][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][30/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][40/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][20/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][30/47]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [27][40/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][10/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][30/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][10/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][20/47]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [29][30/47]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [29][40/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][10/47]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [30][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][30/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/48]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/154]\n",
      "Fill TS Repository [100/154]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries D-6\u001b[0m\n",
      "\u001b[32m-- Train samples size: 4788 - Test samples size: 7684\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/D-6/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][ 0/95]\tTotal Loss -2.3004e+00 (-2.3004e+00)\tConsistency Loss 2.2583e+00 (2.2583e+00)\tInconsistency Loss 1.1037e-01 (1.1037e-01)\tEntropy 2.2794e+00 (2.2794e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][ 0/95]\tTotal Loss -2.3716e+00 (-2.3716e+00)\tConsistency Loss 1.7330e+00 (1.7330e+00)\tInconsistency Loss 1.9424e-01 (1.9424e-01)\tEntropy 2.0523e+00 (2.0523e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][ 0/95]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7237e+00 (1.7237e+00)\tInconsistency Loss 1.9682e-01 (1.9682e-01)\tEntropy 2.0504e+00 (2.0504e+00)\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][ 0/95]\tTotal Loss -2.3769e+00 (-2.3769e+00)\tConsistency Loss 1.7221e+00 (1.7221e+00)\tInconsistency Loss 1.9632e-01 (1.9632e-01)\tEntropy 2.0495e+00 (2.0495e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][ 0/95]\tTotal Loss -2.3781e+00 (-2.3781e+00)\tConsistency Loss 1.7161e+00 (1.7161e+00)\tInconsistency Loss 1.9807e-01 (1.9807e-01)\tEntropy 2.0471e+00 (2.0471e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][ 0/95]\tTotal Loss -2.3808e+00 (-2.3808e+00)\tConsistency Loss 1.7128e+00 (1.7128e+00)\tInconsistency Loss 1.9780e-01 (1.9780e-01)\tEntropy 2.0468e+00 (2.0468e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][ 0/95]\tTotal Loss -2.3767e+00 (-2.3767e+00)\tConsistency Loss 1.7107e+00 (1.7107e+00)\tInconsistency Loss 1.9946e-01 (1.9946e-01)\tEntropy 2.0437e+00 (2.0437e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][ 0/95]\tTotal Loss -2.3846e+00 (-2.3846e+00)\tConsistency Loss 1.7251e+00 (1.7251e+00)\tInconsistency Loss 1.9535e-01 (1.9535e-01)\tEntropy 2.0549e+00 (2.0549e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][ 0/95]\tTotal Loss -2.3777e+00 (-2.3777e+00)\tConsistency Loss 1.7131e+00 (1.7131e+00)\tInconsistency Loss 1.9825e-01 (1.9825e-01)\tEntropy 2.0454e+00 (2.0454e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][ 0/95]\tTotal Loss -2.3787e+00 (-2.3787e+00)\tConsistency Loss 1.7017e+00 (1.7017e+00)\tInconsistency Loss 2.0127e-01 (2.0127e-01)\tEntropy 2.0402e+00 (2.0402e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][ 0/95]\tTotal Loss -2.3761e+00 (-2.3761e+00)\tConsistency Loss 1.7238e+00 (1.7238e+00)\tInconsistency Loss 1.9659e-01 (1.9659e-01)\tEntropy 2.0500e+00 (2.0500e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][ 0/95]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7186e+00 (1.7186e+00)\tInconsistency Loss 1.9769e-01 (1.9769e-01)\tEntropy 2.0479e+00 (2.0479e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][ 0/95]\tTotal Loss -2.3774e+00 (-2.3774e+00)\tConsistency Loss 1.7173e+00 (1.7173e+00)\tInconsistency Loss 1.9788e-01 (1.9788e-01)\tEntropy 2.0474e+00 (2.0474e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][ 0/95]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][ 0/95]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7173e+00 (1.7173e+00)\tInconsistency Loss 1.9794e-01 (1.9794e-01)\tEntropy 2.0472e+00 (2.0472e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7178e+00 (1.7178e+00)\tInconsistency Loss 1.9780e-01 (1.9780e-01)\tEntropy 2.0475e+00 (2.0475e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "D-7\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2383/7442 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/D-7/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/47]\tLoss 1.0050e+00 (1.0050e+00)\n",
      "Epoch: [1][10/47]\tLoss 3.4857e-01 (6.2435e-01)\n",
      "Epoch: [1][20/47]\tLoss 1.2135e-01 (4.2422e-01)\n",
      "Epoch: [1][30/47]\tLoss 4.2312e-02 (3.1032e-01)\n",
      "Epoch: [1][40/47]\tLoss 1.4738e-02 (2.4068e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/47]\tLoss 1.0013e-02 (1.0013e-02)\n",
      "Epoch: [2][10/47]\tLoss 1.0012e-02 (1.0020e-02)\n",
      "Epoch: [2][20/47]\tLoss 1.0015e-02 (1.0019e-02)\n",
      "Epoch: [2][30/47]\tLoss 1.0015e-02 (1.0018e-02)\n",
      "Epoch: [2][40/47]\tLoss 1.0021e-02 (1.0019e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/47]\tLoss 1.0010e-02 (1.0010e-02)\n",
      "Epoch: [3][10/47]\tLoss 1.0020e-02 (1.0015e-02)\n",
      "Epoch: [3][20/47]\tLoss 1.0010e-02 (1.0018e-02)\n",
      "Epoch: [3][30/47]\tLoss 1.0007e-02 (1.0014e-02)\n",
      "Epoch: [3][40/47]\tLoss 1.0006e-02 (1.0013e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/47]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [4][10/47]\tLoss 1.0008e-02 (1.0005e-02)\n",
      "Epoch: [4][20/47]\tLoss 1.0005e-02 (1.0010e-02)\n",
      "Epoch: [4][30/47]\tLoss 1.0005e-02 (1.0008e-02)\n",
      "Epoch: [4][40/47]\tLoss 1.0003e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/47]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [5][10/47]\tLoss 1.0003e-02 (1.0005e-02)\n",
      "Epoch: [5][20/47]\tLoss 1.0006e-02 (1.0004e-02)\n",
      "Epoch: [5][30/47]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [5][40/47]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/47]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [6][10/47]\tLoss 1.0007e-02 (1.0003e-02)\n",
      "Epoch: [6][20/47]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [6][30/47]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [6][40/47]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [7][10/47]\tLoss 1.0010e-02 (1.0003e-02)\n",
      "Epoch: [7][20/47]\tLoss 1.0002e-02 (1.0005e-02)\n",
      "Epoch: [7][30/47]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [7][40/47]\tLoss 1.0005e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [8][10/47]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [8][20/47]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [8][30/47]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [8][40/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/47]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [9][10/47]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [9][20/47]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][30/47]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][40/47]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][10/47]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][20/47]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][30/47]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][40/47]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][10/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][30/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][40/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][10/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][30/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][40/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][10/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][30/47]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [13][40/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/47]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [14][10/47]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [14][20/47]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [14][30/47]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [14][40/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][10/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][30/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][40/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][10/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][30/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [16][40/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][10/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][20/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][30/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][40/47]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][10/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][40/47]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [19][40/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][10/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][30/47]\tLoss 1.0007e-02 (1.0001e-02)\n",
      "Epoch: [20][40/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][10/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][20/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][30/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][40/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][30/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][40/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][10/47]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [23][20/47]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [23][30/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [23][40/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/47]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [24][10/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][30/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][40/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/47]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [25][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][30/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][40/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/47]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [26][10/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][20/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][30/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][40/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][40/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][10/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][20/47]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [28][30/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][40/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [29][10/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][30/47]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [29][40/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][30/47]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [30][40/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/48]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/149]\n",
      "Fill TS Repository [100/149]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries D-7\u001b[0m\n",
      "\u001b[32m-- Train samples size: 4766 - Test samples size: 7442\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/D-7/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][ 0/95]\tTotal Loss -2.3033e+00 (-2.3033e+00)\tConsistency Loss 2.2578e+00 (2.2578e+00)\tInconsistency Loss 1.1015e-01 (1.1015e-01)\tEntropy 2.2805e+00 (2.2805e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][ 0/95]\tTotal Loss -2.3745e+00 (-2.3745e+00)\tConsistency Loss 1.6983e+00 (1.6983e+00)\tInconsistency Loss 2.0570e-01 (2.0570e-01)\tEntropy 2.0364e+00 (2.0364e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][ 0/95]\tTotal Loss -2.3696e+00 (-2.3696e+00)\tConsistency Loss 1.6941e+00 (1.6941e+00)\tInconsistency Loss 2.0156e-01 (2.0156e-01)\tEntropy 2.0318e+00 (2.0318e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][ 0/95]\tTotal Loss -2.3885e+00 (-2.3885e+00)\tConsistency Loss 1.7351e+00 (1.7351e+00)\tInconsistency Loss 1.9364e-01 (1.9364e-01)\tEntropy 2.0618e+00 (2.0618e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][ 0/95]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7236e+00 (1.7236e+00)\tInconsistency Loss 1.9627e-01 (1.9627e-01)\tEntropy 2.0503e+00 (2.0503e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][ 0/95]\tTotal Loss -2.3774e+00 (-2.3774e+00)\tConsistency Loss 1.7109e+00 (1.7109e+00)\tInconsistency Loss 1.9941e-01 (1.9941e-01)\tEntropy 2.0441e+00 (2.0441e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][ 0/95]\tTotal Loss -2.3758e+00 (-2.3758e+00)\tConsistency Loss 1.7170e+00 (1.7170e+00)\tInconsistency Loss 1.9823e-01 (1.9823e-01)\tEntropy 2.0464e+00 (2.0464e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7182e+00)\tInconsistency Loss 1.9771e-01 (1.9771e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9772e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][ 0/95]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "F-1\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2669/8384 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/F-1/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0022e+00 (1.0022e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4871e-01 (6.2451e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2134e-01 (4.2426e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2252e-02 (3.1033e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4702e-02 (2.4068e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0020e-02 (1.9557e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0028e-02 (1.0028e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0019e-02 (1.0034e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0045e-02 (1.0029e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0005e-02 (1.0024e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0018e-02 (1.0021e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0007e-02 (1.0019e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0011e-02 (1.0011e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0091e-02 (1.0024e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0009e-02 (1.0017e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0034e-02 (1.0017e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0054e-02 (1.0015e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0003e-02 (1.0016e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0028e-02 (1.0012e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0008e-02 (1.0020e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0003e-02 (1.0016e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0002e-02 (1.0013e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0002e-02 (1.0012e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0006e-02 (1.0005e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0002e-02 (1.0005e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0006e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0005e-02 (1.0003e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0008e-02 (1.0004e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0002e-02 (1.0006e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0006e-02 (1.0004e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0021e-02 (1.0021e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0010e-02 (1.0003e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][50/53]\tLoss 1.0007e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [17][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0031e-02 (1.0002e-02)\n",
      "Epoch: [19][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][40/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0005e-02 (1.0001e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][10/53]\tLoss 1.0000e-02 (1.0003e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/168]\n",
      "Fill TS Repository [100/168]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries F-1\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5338 - Test samples size: 8384\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/F-1/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/106]\tTotal Loss -2.3017e+00 (-2.3017e+00)\tConsistency Loss 2.2579e+00 (2.2579e+00)\tInconsistency Loss 1.1033e-01 (1.1033e-01)\tEntropy 2.2798e+00 (2.2798e+00)\n",
      "Epoch: [1][100/106]\tTotal Loss -2.3502e+00 (-2.3624e+00)\tConsistency Loss 1.7641e+00 (1.8206e+00)\tInconsistency Loss 1.9177e-01 (1.8091e-01)\tEntropy 2.0572e+00 (2.0915e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/106]\tTotal Loss -2.3822e+00 (-2.3822e+00)\tConsistency Loss 1.7283e+00 (1.7283e+00)\tInconsistency Loss 1.9578e-01 (1.9578e-01)\tEntropy 2.0552e+00 (2.0552e+00)\n",
      "Epoch: [2][100/106]\tTotal Loss -2.3668e+00 (-2.3762e+00)\tConsistency Loss 1.7273e+00 (1.7220e+00)\tInconsistency Loss 1.9937e-01 (1.9713e-01)\tEntropy 2.0470e+00 (2.0491e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/106]\tTotal Loss -2.3907e+00 (-2.3907e+00)\tConsistency Loss 1.7075e+00 (1.7075e+00)\tInconsistency Loss 1.9769e-01 (1.9769e-01)\tEntropy 2.0491e+00 (2.0491e+00)\n",
      "Epoch: [3][100/106]\tTotal Loss -2.3854e+00 (-2.3772e+00)\tConsistency Loss 1.7130e+00 (1.7195e+00)\tInconsistency Loss 1.9611e-01 (1.9719e-01)\tEntropy 2.0492e+00 (2.0484e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/106]\tTotal Loss -2.3754e+00 (-2.3754e+00)\tConsistency Loss 1.7368e+00 (1.7368e+00)\tInconsistency Loss 1.9516e-01 (1.9516e-01)\tEntropy 2.0561e+00 (2.0561e+00)\n",
      "Epoch: [4][100/106]\tTotal Loss -2.3706e+00 (-2.3768e+00)\tConsistency Loss 1.7397e+00 (1.7206e+00)\tInconsistency Loss 1.9495e-01 (1.9723e-01)\tEntropy 2.0551e+00 (2.0487e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/106]\tTotal Loss -2.3808e+00 (-2.3808e+00)\tConsistency Loss 1.7240e+00 (1.7240e+00)\tInconsistency Loss 1.9663e-01 (1.9663e-01)\tEntropy 2.0524e+00 (2.0524e+00)\n",
      "Epoch: [5][100/106]\tTotal Loss -2.3764e+00 (-2.3768e+00)\tConsistency Loss 1.6992e+00 (1.7203e+00)\tInconsistency Loss 2.0241e-01 (1.9746e-01)\tEntropy 2.0378e+00 (2.0485e+00)\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/106]\tTotal Loss -2.3813e+00 (-2.3813e+00)\tConsistency Loss 1.7226e+00 (1.7226e+00)\tInconsistency Loss 1.9616e-01 (1.9616e-01)\tEntropy 2.0520e+00 (2.0520e+00)\n",
      "Epoch: [6][100/106]\tTotal Loss -2.3775e+00 (-2.3772e+00)\tConsistency Loss 1.7042e+00 (1.7192e+00)\tInconsistency Loss 2.0005e-01 (1.9747e-01)\tEntropy 2.0409e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/106]\tTotal Loss -2.3699e+00 (-2.3699e+00)\tConsistency Loss 1.7289e+00 (1.7289e+00)\tInconsistency Loss 1.9596e-01 (1.9596e-01)\tEntropy 2.0494e+00 (2.0494e+00)\n",
      "Epoch: [7][100/106]\tTotal Loss -2.3773e+00 (-2.3769e+00)\tConsistency Loss 1.7110e+00 (1.7193e+00)\tInconsistency Loss 1.9918e-01 (1.9757e-01)\tEntropy 2.0442e+00 (2.0481e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/106]\tTotal Loss -2.3769e+00 (-2.3769e+00)\tConsistency Loss 1.7204e+00 (1.7204e+00)\tInconsistency Loss 1.9710e-01 (1.9710e-01)\tEntropy 2.0486e+00 (2.0486e+00)\n",
      "Epoch: [8][100/106]\tTotal Loss -2.3769e+00 (-2.3771e+00)\tConsistency Loss 1.7188e+00 (1.7184e+00)\tInconsistency Loss 1.9765e-01 (1.9764e-01)\tEntropy 2.0478e+00 (2.0478e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/106]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7193e+00 (1.7193e+00)\tInconsistency Loss 1.9751e-01 (1.9751e-01)\tEntropy 2.0481e+00 (2.0481e+00)\n",
      "Epoch: [9][100/106]\tTotal Loss -2.3773e+00 (-2.3771e+00)\tConsistency Loss 1.7194e+00 (1.7182e+00)\tInconsistency Loss 1.9743e-01 (1.9773e-01)\tEntropy 2.0483e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/106]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7175e+00 (1.7175e+00)\tInconsistency Loss 1.9783e-01 (1.9783e-01)\tEntropy 2.0474e+00 (2.0474e+00)\n",
      "Epoch: [10][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7186e+00 (1.7181e+00)\tInconsistency Loss 1.9759e-01 (1.9773e-01)\tEntropy 2.0479e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7176e+00 (1.7176e+00)\tInconsistency Loss 1.9784e-01 (1.9784e-01)\tEntropy 2.0474e+00 (2.0474e+00)\n",
      "Epoch: [11][100/106]\tTotal Loss -2.3773e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0477e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [12][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "P-4\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2409/7583 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/P-4/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/48]\tLoss 1.0037e+00 (1.0037e+00)\n",
      "Epoch: [1][10/48]\tLoss 3.4881e-01 (6.2445e-01)\n",
      "Epoch: [1][20/48]\tLoss 1.2133e-01 (4.2425e-01)\n",
      "Epoch: [1][30/48]\tLoss 4.2254e-02 (3.1033e-01)\n",
      "Epoch: [1][40/48]\tLoss 1.4743e-02 (2.4068e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/48]\tLoss 1.0011e-02 (1.0011e-02)\n",
      "Epoch: [2][10/48]\tLoss 1.0014e-02 (1.0028e-02)\n",
      "Epoch: [2][20/48]\tLoss 1.0039e-02 (1.0030e-02)\n",
      "Epoch: [2][30/48]\tLoss 1.0021e-02 (1.0026e-02)\n",
      "Epoch: [2][40/48]\tLoss 1.0009e-02 (1.0022e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/48]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [3][10/48]\tLoss 1.0025e-02 (1.0011e-02)\n",
      "Epoch: [3][20/48]\tLoss 1.0009e-02 (1.0012e-02)\n",
      "Epoch: [3][30/48]\tLoss 1.0012e-02 (1.0012e-02)\n",
      "Epoch: [3][40/48]\tLoss 1.0006e-02 (1.0013e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/48]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [4][10/48]\tLoss 1.0007e-02 (1.0006e-02)\n",
      "Epoch: [4][20/48]\tLoss 1.0014e-02 (1.0007e-02)\n",
      "Epoch: [4][30/48]\tLoss 1.0024e-02 (1.0008e-02)\n",
      "Epoch: [4][40/48]\tLoss 1.0007e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/48]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [5][10/48]\tLoss 1.0006e-02 (1.0008e-02)\n",
      "Epoch: [5][20/48]\tLoss 1.0009e-02 (1.0010e-02)\n",
      "Epoch: [5][30/48]\tLoss 1.0006e-02 (1.0009e-02)\n",
      "Epoch: [5][40/48]\tLoss 1.0004e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [6][10/48]\tLoss 1.0002e-02 (1.0005e-02)\n",
      "Epoch: [6][20/48]\tLoss 1.0002e-02 (1.0005e-02)\n",
      "Epoch: [6][30/48]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [6][40/48]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/48]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [7][10/48]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [7][20/48]\tLoss 1.0007e-02 (1.0003e-02)\n",
      "Epoch: [7][30/48]\tLoss 1.0005e-02 (1.0004e-02)\n",
      "Epoch: [7][40/48]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/48]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [8][10/48]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][20/48]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [8][30/48]\tLoss 1.0007e-02 (1.0004e-02)\n",
      "Epoch: [8][40/48]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/48]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [9][10/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][20/48]\tLoss 1.0006e-02 (1.0003e-02)\n",
      "Epoch: [9][30/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][40/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][10/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][20/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][30/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][40/48]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/48]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [11][10/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][20/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][30/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][40/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][10/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [12][20/48]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [12][30/48]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [12][40/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/48]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [13][10/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [13][20/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][30/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][40/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][10/48]\tLoss 1.0005e-02 (1.0006e-02)\n",
      "Epoch: [14][20/48]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [14][30/48]\tLoss 1.0000e-02 (1.0003e-02)\n",
      "Epoch: [14][40/48]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/48]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][10/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [15][20/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [15][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][40/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/48]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][10/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [16][20/48]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [16][30/48]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [16][40/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [17][20/48]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [17][30/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [17][40/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/48]\tLoss 1.0012e-02 (1.0012e-02)\n",
      "Epoch: [18][10/48]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [18][20/48]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [18][30/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [18][40/48]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/48]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][10/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [19][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][10/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][30/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [20][40/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][20/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [21][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][40/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/48]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][20/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [22][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][40/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/48]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][30/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/48]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][10/48]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [24][20/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [24][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][10/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [25][20/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [25][30/48]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [25][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/48]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][20/48]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [26][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][40/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][10/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][40/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [28][10/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][20/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][20/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][30/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][10/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][30/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [30][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/49]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/152]\n",
      "Fill TS Repository [100/152]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries P-4\u001b[0m\n",
      "\u001b[32m-- Train samples size: 4818 - Test samples size: 7583\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/P-4/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][ 0/96]\tTotal Loss -2.3047e+00 (-2.3047e+00)\tConsistency Loss 2.2550e+00 (2.2550e+00)\tInconsistency Loss 1.1024e-01 (1.1024e-01)\tEntropy 2.2799e+00 (2.2799e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][ 0/96]\tTotal Loss -2.3781e+00 (-2.3781e+00)\tConsistency Loss 1.7194e+00 (1.7194e+00)\tInconsistency Loss 1.9816e-01 (1.9816e-01)\tEntropy 2.0488e+00 (2.0488e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][ 0/96]\tTotal Loss -2.3769e+00 (-2.3769e+00)\tConsistency Loss 1.7278e+00 (1.7278e+00)\tInconsistency Loss 1.9612e-01 (1.9612e-01)\tEntropy 2.0524e+00 (2.0524e+00)\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][ 0/96]\tTotal Loss -2.3764e+00 (-2.3764e+00)\tConsistency Loss 1.7373e+00 (1.7373e+00)\tInconsistency Loss 1.9391e-01 (1.9391e-01)\tEntropy 2.0568e+00 (2.0568e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][ 0/96]\tTotal Loss -2.3751e+00 (-2.3751e+00)\tConsistency Loss 1.7178e+00 (1.7178e+00)\tInconsistency Loss 1.9805e-01 (1.9805e-01)\tEntropy 2.0465e+00 (2.0465e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][ 0/96]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9785e-01 (1.9785e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][ 0/96]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7234e+00 (1.7234e+00)\tInconsistency Loss 1.9674e-01 (1.9674e-01)\tEntropy 2.0503e+00 (2.0503e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][ 0/96]\tTotal Loss -2.3755e+00 (-2.3755e+00)\tConsistency Loss 1.7161e+00 (1.7161e+00)\tInconsistency Loss 1.9814e-01 (1.9814e-01)\tEntropy 2.0458e+00 (2.0458e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][ 0/96]\tTotal Loss -2.3789e+00 (-2.3789e+00)\tConsistency Loss 1.7211e+00 (1.7211e+00)\tInconsistency Loss 1.9666e-01 (1.9666e-01)\tEntropy 2.0500e+00 (2.0500e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][ 0/96]\tTotal Loss -2.3766e+00 (-2.3766e+00)\tConsistency Loss 1.7189e+00 (1.7189e+00)\tInconsistency Loss 1.9752e-01 (1.9752e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][ 0/96]\tTotal Loss -2.3768e+00 (-2.3768e+00)\tConsistency Loss 1.7190e+00 (1.7190e+00)\tInconsistency Loss 1.9755e-01 (1.9755e-01)\tEntropy 2.0479e+00 (2.0479e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9775e-01 (1.9775e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "G-3\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2424/7707 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/G-3/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/48]\tLoss 1.0055e+00 (1.0055e+00)\n",
      "Epoch: [1][10/48]\tLoss 3.4822e-01 (6.2444e-01)\n",
      "Epoch: [1][20/48]\tLoss 1.2129e-01 (4.2425e-01)\n",
      "Epoch: [1][30/48]\tLoss 4.2304e-02 (3.1033e-01)\n",
      "Epoch: [1][40/48]\tLoss 1.4719e-02 (2.4068e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/48]\tLoss 1.0018e-02 (1.0018e-02)\n",
      "Epoch: [2][10/48]\tLoss 1.0020e-02 (1.0028e-02)\n",
      "Epoch: [2][20/48]\tLoss 1.0010e-02 (1.0028e-02)\n",
      "Epoch: [2][30/48]\tLoss 1.0025e-02 (1.0027e-02)\n",
      "Epoch: [2][40/48]\tLoss 1.0020e-02 (1.0025e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/48]\tLoss 1.0020e-02 (1.0020e-02)\n",
      "Epoch: [3][10/48]\tLoss 1.0016e-02 (1.0015e-02)\n",
      "Epoch: [3][20/48]\tLoss 1.0013e-02 (1.0014e-02)\n",
      "Epoch: [3][30/48]\tLoss 1.0005e-02 (1.0013e-02)\n",
      "Epoch: [3][40/48]\tLoss 1.0008e-02 (1.0012e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/48]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [4][10/48]\tLoss 1.0009e-02 (1.0012e-02)\n",
      "Epoch: [4][20/48]\tLoss 1.0006e-02 (1.0010e-02)\n",
      "Epoch: [4][30/48]\tLoss 1.0003e-02 (1.0012e-02)\n",
      "Epoch: [4][40/48]\tLoss 1.0005e-02 (1.0010e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/48]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [5][10/48]\tLoss 1.0006e-02 (1.0005e-02)\n",
      "Epoch: [5][20/48]\tLoss 1.0006e-02 (1.0005e-02)\n",
      "Epoch: [5][30/48]\tLoss 1.0002e-02 (1.0005e-02)\n",
      "Epoch: [5][40/48]\tLoss 1.0003e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/48]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [6][10/48]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [6][20/48]\tLoss 1.0004e-02 (1.0005e-02)\n",
      "Epoch: [6][30/48]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [6][40/48]\tLoss 1.0010e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/48]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [7][10/48]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [7][20/48]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [7][30/48]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [7][40/48]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/48]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][10/48]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [8][20/48]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][30/48]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][40/48]\tLoss 1.0089e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][10/48]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "Epoch: [9][20/48]\tLoss 1.0005e-02 (1.0003e-02)\n",
      "Epoch: [9][30/48]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [9][40/48]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][10/48]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [10][20/48]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [10][30/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][40/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][10/48]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [11][20/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][30/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][40/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/48]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [12][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][20/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][30/48]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [12][40/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][10/48]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [13][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][30/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [13][40/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][20/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [14][30/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [14][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][10/48]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [15][20/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [15][30/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [15][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/48]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [16][30/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [16][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [17][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][20/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [17][30/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [17][40/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][10/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/48]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][30/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/48]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][10/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][10/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][20/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][30/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/48]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][20/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/48]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [26][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][30/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/48]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/48]\tLoss 1.0000e-02 (1.0003e-02)\n",
      "Epoch: [27][20/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [27][30/48]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [27][40/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/48]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][10/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [28][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/48]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][20/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][40/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/48]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][20/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/49]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/155]\n",
      "Fill TS Repository [100/155]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries G-3\u001b[0m\n",
      "\u001b[32m-- Train samples size: 4848 - Test samples size: 7707\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/G-3/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][ 0/96]\tTotal Loss -2.3024e+00 (-2.3024e+00)\tConsistency Loss 2.2569e+00 (2.2569e+00)\tInconsistency Loss 1.1028e-01 (1.1028e-01)\tEntropy 2.2796e+00 (2.2796e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][ 0/96]\tTotal Loss -2.3782e+00 (-2.3782e+00)\tConsistency Loss 1.7022e+00 (1.7022e+00)\tInconsistency Loss 2.0062e-01 (2.0062e-01)\tEntropy 2.0402e+00 (2.0402e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][ 0/96]\tTotal Loss -2.3666e+00 (-2.3666e+00)\tConsistency Loss 1.7250e+00 (1.7250e+00)\tInconsistency Loss 1.9387e-01 (1.9387e-01)\tEntropy 2.0458e+00 (2.0458e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][ 0/96]\tTotal Loss -2.3659e+00 (-2.3659e+00)\tConsistency Loss 1.7203e+00 (1.7203e+00)\tInconsistency Loss 1.9971e-01 (1.9971e-01)\tEntropy 2.0431e+00 (2.0431e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][ 0/96]\tTotal Loss -2.3724e+00 (-2.3724e+00)\tConsistency Loss 1.7244e+00 (1.7244e+00)\tInconsistency Loss 1.9642e-01 (1.9642e-01)\tEntropy 2.0484e+00 (2.0484e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][ 0/96]\tTotal Loss -2.3718e+00 (-2.3718e+00)\tConsistency Loss 1.7206e+00 (1.7206e+00)\tInconsistency Loss 1.9865e-01 (1.9865e-01)\tEntropy 2.0462e+00 (2.0462e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][ 0/96]\tTotal Loss -2.3756e+00 (-2.3756e+00)\tConsistency Loss 1.7189e+00 (1.7189e+00)\tInconsistency Loss 1.9798e-01 (1.9798e-01)\tEntropy 2.0472e+00 (2.0472e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9781e-01 (1.9781e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][ 0/96]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7192e+00 (1.7192e+00)\tInconsistency Loss 1.9758e-01 (1.9758e-01)\tEntropy 2.0481e+00 (2.0481e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7182e+00)\tInconsistency Loss 1.9770e-01 (1.9770e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][ 0/96]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "T-1\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2675/8412 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/T-1/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0027e+00 (1.0027e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4763e-01 (6.2477e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2104e-01 (4.2434e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2167e-02 (3.1034e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4686e-02 (2.4068e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0020e-02 (1.9557e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0048e-02 (1.0048e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0013e-02 (1.0030e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0014e-02 (1.0029e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0012e-02 (1.0025e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0096e-02 (1.0032e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0009e-02 (1.0049e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0022e-02 (1.0022e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0014e-02 (1.0013e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0008e-02 (1.0026e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0001e-02 (1.0020e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0004e-02 (1.0019e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0010e-02 (1.0019e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0002e-02 (9.9953e-03)\n",
      "Epoch: [4][20/53]\tLoss 1.0004e-02 (9.9999e-03)\n",
      "Epoch: [4][30/53]\tLoss 1.0007e-02 (1.0005e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0012e-02 (1.0005e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0021e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0028e-02 (1.0009e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0006e-02 (1.0008e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0003e-02 (1.0007e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0002e-02 (1.0008e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0007e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0003e-02 (1.0010e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0004e-02 (1.0008e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0001e-02 (1.0006e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0002e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0010e-02 (1.0004e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0003e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0004e-02 (1.0012e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0014e-02 (1.0009e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0005e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0009e-02 (1.0012e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0001e-02 (1.0007e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0002e-02 (1.0005e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0000e-02 (1.0005e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0064e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0018e-02 (1.0018e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0001e-02 (1.0010e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0000e-02 (1.0008e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0018e-02 (1.0007e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0012e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0008e-02 (1.0002e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0013e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0000e-02 (1.0012e-02)\n",
      "Epoch: [14][20/53]\tLoss 9.9318e-03 (1.0004e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0005e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][10/53]\tLoss 9.8140e-03 (9.9856e-03)\n",
      "Epoch: [16][20/53]\tLoss 1.0001e-02 (9.9934e-03)\n",
      "Epoch: [16][30/53]\tLoss 1.0001e-02 (9.9960e-03)\n",
      "Epoch: [16][40/53]\tLoss 1.0001e-02 (9.9973e-03)\n",
      "Epoch: [16][50/53]\tLoss 1.0001e-02 (9.9980e-03)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0001e-02 (9.9976e-03)\n",
      "Epoch: [17][20/53]\tLoss 1.0002e-02 (9.9997e-03)\n",
      "Epoch: [17][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0013e-02 (1.0001e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0028e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0000e-02 (1.0003e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0007e-02 (1.0003e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0000e-02 (9.9860e-03)\n",
      "Epoch: [19][20/53]\tLoss 1.0000e-02 (9.9946e-03)\n",
      "Epoch: [19][30/53]\tLoss 1.0001e-02 (9.9967e-03)\n",
      "Epoch: [19][40/53]\tLoss 1.0000e-02 (9.9978e-03)\n",
      "Epoch: [19][50/53]\tLoss 1.0003e-02 (9.9986e-03)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0000e-02 (9.9957e-03)\n",
      "Epoch: [20][40/53]\tLoss 1.0001e-02 (9.9971e-03)\n",
      "Epoch: [20][50/53]\tLoss 1.0000e-02 (9.9980e-03)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0000e-02 (9.9972e-03)\n",
      "Epoch: [21][50/53]\tLoss 1.0000e-02 (9.9984e-03)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0006e-02 (1.0002e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0005e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][10/53]\tLoss 1.0006e-02 (1.0003e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0007e-02 (1.0001e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0005e-02 (1.0002e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0006e-02 (9.9994e-03)\n",
      "Epoch: [26][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0005e-02 (1.0003e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][20/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0002e-02 (1.0000e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0001e-02 (9.9952e-03)\n",
      "Epoch: [29][40/53]\tLoss 1.0000e-02 (9.9967e-03)\n",
      "Epoch: [29][50/53]\tLoss 1.0000e-02 (9.9980e-03)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0001e-02 (9.9948e-03)\n",
      "Epoch: [30][20/53]\tLoss 1.0011e-02 (9.9982e-03)\n",
      "Epoch: [30][30/53]\tLoss 1.0001e-02 (9.9997e-03)\n",
      "Epoch: [30][40/53]\tLoss 1.0001e-02 (9.9999e-03)\n",
      "Epoch: [30][50/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/169]\n",
      "Fill TS Repository [100/169]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries T-1\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5350 - Test samples size: 8412\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/T-1/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/107]\tTotal Loss -2.3003e+00 (-2.3003e+00)\tConsistency Loss 2.2567e+00 (2.2567e+00)\tInconsistency Loss 1.1070e-01 (1.1070e-01)\tEntropy 2.2785e+00 (2.2785e+00)\n",
      "Epoch: [1][100/107]\tTotal Loss -2.3870e+00 (-2.3471e+00)\tConsistency Loss 1.7295e+00 (1.8927e+00)\tInconsistency Loss 1.9504e-01 (1.7091e-01)\tEntropy 2.0582e+00 (2.1199e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/107]\tTotal Loss -2.3626e+00 (-2.3626e+00)\tConsistency Loss 1.7040e+00 (1.7040e+00)\tInconsistency Loss 2.0018e-01 (2.0018e-01)\tEntropy 2.0333e+00 (2.0333e+00)\n",
      "Epoch: [2][100/107]\tTotal Loss -2.3660e+00 (-2.3785e+00)\tConsistency Loss 1.7105e+00 (1.7158e+00)\tInconsistency Loss 1.9912e-01 (1.9845e-01)\tEntropy 2.0383e+00 (2.0471e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/107]\tTotal Loss -2.3921e+00 (-2.3921e+00)\tConsistency Loss 1.7215e+00 (1.7215e+00)\tInconsistency Loss 1.9527e-01 (1.9527e-01)\tEntropy 2.0568e+00 (2.0568e+00)\n",
      "Epoch: [3][100/107]\tTotal Loss -2.3823e+00 (-2.3764e+00)\tConsistency Loss 1.6937e+00 (1.7227e+00)\tInconsistency Loss 2.0364e-01 (1.9688e-01)\tEntropy 2.0380e+00 (2.0495e+00)\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/107]\tTotal Loss -2.3642e+00 (-2.3642e+00)\tConsistency Loss 1.7078e+00 (1.7078e+00)\tInconsistency Loss 1.9957e-01 (1.9957e-01)\tEntropy 2.0360e+00 (2.0360e+00)\n",
      "Epoch: [4][100/107]\tTotal Loss -2.3866e+00 (-2.3769e+00)\tConsistency Loss 1.7086e+00 (1.7196e+00)\tInconsistency Loss 1.9867e-01 (1.9731e-01)\tEntropy 2.0476e+00 (2.0483e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/107]\tTotal Loss -2.3918e+00 (-2.3918e+00)\tConsistency Loss 1.7047e+00 (1.7047e+00)\tInconsistency Loss 1.9694e-01 (1.9694e-01)\tEntropy 2.0482e+00 (2.0482e+00)\n",
      "Epoch: [5][100/107]\tTotal Loss -2.3772e+00 (-2.3765e+00)\tConsistency Loss 1.7105e+00 (1.7207e+00)\tInconsistency Loss 1.9904e-01 (1.9724e-01)\tEntropy 2.0438e+00 (2.0486e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/107]\tTotal Loss -2.3803e+00 (-2.3803e+00)\tConsistency Loss 1.7278e+00 (1.7278e+00)\tInconsistency Loss 1.9529e-01 (1.9529e-01)\tEntropy 2.0541e+00 (2.0541e+00)\n",
      "Epoch: [6][100/107]\tTotal Loss -2.3766e+00 (-2.3769e+00)\tConsistency Loss 1.7123e+00 (1.7193e+00)\tInconsistency Loss 1.9946e-01 (1.9750e-01)\tEntropy 2.0445e+00 (2.0481e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/107]\tTotal Loss -2.3754e+00 (-2.3754e+00)\tConsistency Loss 1.7220e+00 (1.7220e+00)\tInconsistency Loss 1.9752e-01 (1.9752e-01)\tEntropy 2.0487e+00 (2.0487e+00)\n",
      "Epoch: [7][100/107]\tTotal Loss -2.3812e+00 (-2.3776e+00)\tConsistency Loss 1.6988e+00 (1.7177e+00)\tInconsistency Loss 2.0251e-01 (1.9782e-01)\tEntropy 2.0400e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/107]\tTotal Loss -2.3781e+00 (-2.3781e+00)\tConsistency Loss 1.7247e+00 (1.7247e+00)\tInconsistency Loss 1.9610e-01 (1.9610e-01)\tEntropy 2.0514e+00 (2.0514e+00)\n",
      "Epoch: [8][100/107]\tTotal Loss -2.3761e+00 (-2.3768e+00)\tConsistency Loss 1.7211e+00 (1.7197e+00)\tInconsistency Loss 1.9680e-01 (1.9753e-01)\tEntropy 2.0486e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/107]\tTotal Loss -2.3767e+00 (-2.3767e+00)\tConsistency Loss 1.7217e+00 (1.7217e+00)\tInconsistency Loss 1.9812e-01 (1.9812e-01)\tEntropy 2.0492e+00 (2.0492e+00)\n",
      "Epoch: [9][100/107]\tTotal Loss -2.3769e+00 (-2.3770e+00)\tConsistency Loss 1.7302e+00 (1.7186e+00)\tInconsistency Loss 1.9511e-01 (1.9766e-01)\tEntropy 2.0536e+00 (2.0478e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7152e+00 (1.7152e+00)\tInconsistency Loss 1.9874e-01 (1.9874e-01)\tEntropy 2.0462e+00 (2.0462e+00)\n",
      "Epoch: [10][100/107]\tTotal Loss -2.3780e+00 (-2.3771e+00)\tConsistency Loss 1.7170e+00 (1.7184e+00)\tInconsistency Loss 1.9787e-01 (1.9768e-01)\tEntropy 2.0475e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/107]\tTotal Loss -2.3755e+00 (-2.3755e+00)\tConsistency Loss 1.7169e+00 (1.7169e+00)\tInconsistency Loss 1.9796e-01 (1.9796e-01)\tEntropy 2.0462e+00 (2.0462e+00)\n",
      "Epoch: [11][100/107]\tTotal Loss -2.3765e+00 (-2.3770e+00)\tConsistency Loss 1.7156e+00 (1.7187e+00)\tInconsistency Loss 1.9817e-01 (1.9761e-01)\tEntropy 2.0461e+00 (2.0479e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/107]\tTotal Loss -2.3765e+00 (-2.3765e+00)\tConsistency Loss 1.7224e+00 (1.7224e+00)\tInconsistency Loss 1.9725e-01 (1.9725e-01)\tEntropy 2.0495e+00 (2.0495e+00)\n",
      "Epoch: [12][100/107]\tTotal Loss -2.3771e+00 (-2.3770e+00)\tConsistency Loss 1.7164e+00 (1.7185e+00)\tInconsistency Loss 1.9822e-01 (1.9767e-01)\tEntropy 2.0468e+00 (2.0478e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/107]\tTotal Loss -2.3774e+00 (-2.3774e+00)\tConsistency Loss 1.7203e+00 (1.7203e+00)\tInconsistency Loss 1.9721e-01 (1.9721e-01)\tEntropy 2.0488e+00 (2.0488e+00)\n",
      "Epoch: [13][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7187e+00 (1.7179e+00)\tInconsistency Loss 1.9759e-01 (1.9776e-01)\tEntropy 2.0479e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/107]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7173e+00 (1.7173e+00)\tInconsistency Loss 1.9789e-01 (1.9789e-01)\tEntropy 2.0473e+00 (2.0473e+00)\n",
      "Epoch: [14][100/107]\tTotal Loss -2.3769e+00 (-2.3771e+00)\tConsistency Loss 1.7184e+00 (1.7183e+00)\tInconsistency Loss 1.9773e-01 (1.9770e-01)\tEntropy 2.0476e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7178e+00 (1.7178e+00)\tInconsistency Loss 1.9779e-01 (1.9779e-01)\tEntropy 2.0475e+00 (2.0475e+00)\n",
      "Epoch: [15][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "T-2\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2655/8425 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/T-2/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0042e+00 (1.0042e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4829e-01 (6.2468e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2114e-01 (4.2432e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2388e-02 (3.1036e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4669e-02 (2.4069e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0049e-02 (1.9558e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0020e-02 (1.0020e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0008e-02 (1.0023e-02)\n",
      "Epoch: [2][20/53]\tLoss 9.9807e-03 (1.0025e-02)\n",
      "Epoch: [2][30/53]\tLoss 9.9977e-03 (1.0026e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0007e-02 (1.0026e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0009e-02 (1.0026e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0009e-02 (1.0009e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0009e-02 (1.0014e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0009e-02 (1.0013e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0018e-02 (1.0015e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0021e-02 (1.0016e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0020e-02 (1.0016e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0008e-02 (1.0008e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0015e-02 (1.0017e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0016e-02 (1.0013e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0003e-02 (1.0009e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0055e-02 (1.0010e-02)\n",
      "Epoch: [4][50/53]\tLoss 9.9973e-03 (1.0011e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0010e-02 (1.0010e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0011e-02 (1.0007e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0018e-02 (1.0009e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0027e-02 (1.0010e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0003e-02 (1.0009e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0006e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0019e-02 (1.0019e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0006e-02 (1.0005e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0009e-02 (1.0005e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0016e-02 (1.0005e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0005e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0009e-02 (1.0009e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0007e-02 (1.0006e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0007e-02 (1.0004e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0005e-02 (1.0003e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0005e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0008e-02 (1.0006e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0004e-02 (1.0005e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0012e-02 (1.0005e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0011e-02 (1.0004e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0007e-02 (1.0006e-02)\n",
      "Epoch: [9][30/53]\tLoss 9.9986e-03 (1.0005e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0004e-02 (1.0005e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0002e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0000e-02 (1.0003e-02)\n",
      "Epoch: [10][50/53]\tLoss 9.9968e-03 (1.0003e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [11][10/53]\tLoss 9.9912e-03 (1.0003e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0013e-02 (1.0003e-02)\n",
      "Epoch: [11][30/53]\tLoss 9.9994e-03 (1.0003e-02)\n",
      "Epoch: [11][40/53]\tLoss 9.9960e-03 (1.0003e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0000e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [12][20/53]\tLoss 9.9996e-03 (1.0002e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0005e-02 (1.0003e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][50/53]\tLoss 9.9998e-03 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0015e-02 (1.0003e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [16][30/53]\tLoss 9.9995e-03 (1.0002e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [16][50/53]\tLoss 9.9984e-03 (1.0002e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [17][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 9.9998e-03 (9.9998e-03)\n",
      "Epoch: [18][10/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 9.9994e-03 (9.9994e-03)\n",
      "Epoch: [19][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [19][40/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [20][40/53]\tLoss 9.9984e-03 (1.0002e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0010e-02 (1.0002e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0015e-02 (1.0002e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0006e-02 (1.0002e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0007e-02 (1.0001e-02)\n",
      "Epoch: [22][30/53]\tLoss 9.9990e-03 (1.0001e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][30/53]\tLoss 9.9955e-03 (1.0001e-02)\n",
      "Epoch: [23][40/53]\tLoss 9.9996e-03 (1.0001e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 9.9993e-03 (9.9993e-03)\n",
      "Epoch: [24][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [24][40/53]\tLoss 9.9997e-03 (1.0001e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 9.9986e-03 (9.9986e-03)\n",
      "Epoch: [26][10/53]\tLoss 9.9986e-03 (9.9994e-03)\n",
      "Epoch: [26][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][50/53]\tLoss 9.9994e-03 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [27][20/53]\tLoss 9.9969e-03 (1.0001e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [28][20/53]\tLoss 9.9991e-03 (1.0001e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0006e-02 (1.0001e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [29][50/53]\tLoss 9.9999e-03 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0007e-02 (1.0001e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][50/53]\tLoss 9.9993e-03 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/169]\n",
      "Fill TS Repository [100/169]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries T-2\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5310 - Test samples size: 8425\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/T-2/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/106]\tTotal Loss -2.3017e+00 (-2.3017e+00)\tConsistency Loss 2.2564e+00 (2.2564e+00)\tInconsistency Loss 1.1057e-01 (1.1057e-01)\tEntropy 2.2791e+00 (2.2791e+00)\n",
      "Epoch: [1][100/106]\tTotal Loss -2.3741e+00 (-2.3630e+00)\tConsistency Loss 1.7467e+00 (1.8264e+00)\tInconsistency Loss 1.8880e-01 (1.8039e-01)\tEntropy 2.0604e+00 (2.0947e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/106]\tTotal Loss -2.3405e+00 (-2.3405e+00)\tConsistency Loss 1.7215e+00 (1.7215e+00)\tInconsistency Loss 1.9127e-01 (1.9127e-01)\tEntropy 2.0310e+00 (2.0310e+00)\n",
      "Epoch: [2][100/106]\tTotal Loss -2.3930e+00 (-2.3743e+00)\tConsistency Loss 1.7008e+00 (1.7331e+00)\tInconsistency Loss 2.0026e-01 (1.9483e-01)\tEntropy 2.0469e+00 (2.0537e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/106]\tTotal Loss -2.3621e+00 (-2.3621e+00)\tConsistency Loss 1.7014e+00 (1.7014e+00)\tInconsistency Loss 2.0700e-01 (2.0700e-01)\tEntropy 2.0318e+00 (2.0318e+00)\n",
      "Epoch: [3][100/106]\tTotal Loss -2.3634e+00 (-2.3766e+00)\tConsistency Loss 1.7160e+00 (1.7211e+00)\tInconsistency Loss 1.9740e-01 (1.9706e-01)\tEntropy 2.0397e+00 (2.0489e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/106]\tTotal Loss -2.3707e+00 (-2.3707e+00)\tConsistency Loss 1.6984e+00 (1.6984e+00)\tInconsistency Loss 2.0173e-01 (2.0173e-01)\tEntropy 2.0346e+00 (2.0346e+00)\n",
      "Epoch: [4][100/106]\tTotal Loss -2.3725e+00 (-2.3768e+00)\tConsistency Loss 1.7251e+00 (1.7189e+00)\tInconsistency Loss 1.9658e-01 (1.9760e-01)\tEntropy 2.0488e+00 (2.0478e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/106]\tTotal Loss -2.3775e+00 (-2.3775e+00)\tConsistency Loss 1.7300e+00 (1.7300e+00)\tInconsistency Loss 1.9512e-01 (1.9512e-01)\tEntropy 2.0537e+00 (2.0537e+00)\n",
      "Epoch: [5][100/106]\tTotal Loss -2.3757e+00 (-2.3765e+00)\tConsistency Loss 1.7271e+00 (1.7197e+00)\tInconsistency Loss 1.9639e-01 (1.9767e-01)\tEntropy 2.0514e+00 (2.0481e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/106]\tTotal Loss -2.3749e+00 (-2.3749e+00)\tConsistency Loss 1.7351e+00 (1.7351e+00)\tInconsistency Loss 1.9460e-01 (1.9460e-01)\tEntropy 2.0550e+00 (2.0550e+00)\n",
      "Epoch: [6][100/106]\tTotal Loss -2.3768e+00 (-2.3770e+00)\tConsistency Loss 1.7204e+00 (1.7187e+00)\tInconsistency Loss 1.9754e-01 (1.9763e-01)\tEntropy 2.0486e+00 (2.0479e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/106]\tTotal Loss -2.3785e+00 (-2.3785e+00)\tConsistency Loss 1.7187e+00 (1.7187e+00)\tInconsistency Loss 1.9758e-01 (1.9758e-01)\tEntropy 2.0486e+00 (2.0486e+00)\n",
      "Epoch: [7][100/106]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7188e+00 (1.7185e+00)\tInconsistency Loss 1.9762e-01 (1.9767e-01)\tEntropy 2.0479e+00 (2.0478e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/106]\tTotal Loss -2.3781e+00 (-2.3781e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9766e-01 (1.9766e-01)\tEntropy 2.0481e+00 (2.0481e+00)\n",
      "Epoch: [8][100/106]\tTotal Loss -2.3784e+00 (-2.3772e+00)\tConsistency Loss 1.7140e+00 (1.7183e+00)\tInconsistency Loss 1.9837e-01 (1.9771e-01)\tEntropy 2.0462e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/106]\tTotal Loss -2.3778e+00 (-2.3778e+00)\tConsistency Loss 1.7173e+00 (1.7173e+00)\tInconsistency Loss 1.9787e-01 (1.9787e-01)\tEntropy 2.0475e+00 (2.0475e+00)\n",
      "Epoch: [9][100/106]\tTotal Loss -2.3773e+00 (-2.3772e+00)\tConsistency Loss 1.7152e+00 (1.7184e+00)\tInconsistency Loss 1.9842e-01 (1.9767e-01)\tEntropy 2.0463e+00 (2.0478e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/106]\tTotal Loss -2.3759e+00 (-2.3759e+00)\tConsistency Loss 1.7203e+00 (1.7203e+00)\tInconsistency Loss 1.9756e-01 (1.9756e-01)\tEntropy 2.0481e+00 (2.0481e+00)\n",
      "Epoch: [10][100/106]\tTotal Loss -2.3772e+00 (-2.3771e+00)\tConsistency Loss 1.7194e+00 (1.7184e+00)\tInconsistency Loss 1.9743e-01 (1.9767e-01)\tEntropy 2.0483e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/106]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7171e+00 (1.7171e+00)\tInconsistency Loss 1.9801e-01 (1.9801e-01)\tEntropy 2.0470e+00 (2.0470e+00)\n",
      "Epoch: [11][100/106]\tTotal Loss -2.3774e+00 (-2.3772e+00)\tConsistency Loss 1.7187e+00 (1.7181e+00)\tInconsistency Loss 1.9760e-01 (1.9773e-01)\tEntropy 2.0481e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/106]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7177e+00 (1.7177e+00)\tInconsistency Loss 1.9783e-01 (1.9783e-01)\tEntropy 2.0474e+00 (2.0474e+00)\n",
      "Epoch: [12][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7181e+00)\tInconsistency Loss 1.9776e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "D-8\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2402/7674 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/D-8/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/48]\tLoss 1.0006e+00 (1.0006e+00)\n",
      "Epoch: [1][10/48]\tLoss 3.4846e-01 (6.2429e-01)\n",
      "Epoch: [1][20/48]\tLoss 1.2129e-01 (4.2425e-01)\n",
      "Epoch: [1][30/48]\tLoss 4.2256e-02 (3.1033e-01)\n",
      "Epoch: [1][40/48]\tLoss 1.4711e-02 (2.4068e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/48]\tLoss 1.0044e-02 (1.0044e-02)\n",
      "Epoch: [2][10/48]\tLoss 1.0040e-02 (1.0050e-02)\n",
      "Epoch: [2][20/48]\tLoss 1.0014e-02 (1.0048e-02)\n",
      "Epoch: [2][30/48]\tLoss 1.0020e-02 (1.0039e-02)\n",
      "Epoch: [2][40/48]\tLoss 9.9927e-03 (1.0035e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/48]\tLoss 9.9989e-03 (9.9989e-03)\n",
      "Epoch: [3][10/48]\tLoss 1.0019e-02 (1.0013e-02)\n",
      "Epoch: [3][20/48]\tLoss 1.0013e-02 (1.0036e-02)\n",
      "Epoch: [3][30/48]\tLoss 1.0018e-02 (1.0030e-02)\n",
      "Epoch: [3][40/48]\tLoss 1.0047e-02 (1.0026e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/48]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [4][10/48]\tLoss 9.9985e-03 (1.0013e-02)\n",
      "Epoch: [4][20/48]\tLoss 1.0006e-02 (1.0014e-02)\n",
      "Epoch: [4][30/48]\tLoss 1.0005e-02 (1.0012e-02)\n",
      "Epoch: [4][40/48]\tLoss 1.0012e-02 (1.0011e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [5][10/48]\tLoss 9.9980e-03 (1.0006e-02)\n",
      "Epoch: [5][20/48]\tLoss 9.9994e-03 (1.0011e-02)\n",
      "Epoch: [5][30/48]\tLoss 1.0005e-02 (1.0008e-02)\n",
      "Epoch: [5][40/48]\tLoss 1.0005e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/48]\tLoss 9.9969e-03 (9.9969e-03)\n",
      "Epoch: [6][10/48]\tLoss 1.0006e-02 (1.0002e-02)\n",
      "Epoch: [6][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [6][30/48]\tLoss 9.9937e-03 (1.0001e-02)\n",
      "Epoch: [6][40/48]\tLoss 1.0206e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/48]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [7][10/48]\tLoss 1.0005e-02 (1.0004e-02)\n",
      "Epoch: [7][20/48]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [7][30/48]\tLoss 1.0010e-02 (1.0004e-02)\n",
      "Epoch: [7][40/48]\tLoss 9.9858e-03 (1.0004e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/48]\tLoss 9.9994e-03 (9.9994e-03)\n",
      "Epoch: [8][10/48]\tLoss 9.9958e-03 (9.9995e-03)\n",
      "Epoch: [8][20/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [8][30/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [8][40/48]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][10/48]\tLoss 1.0002e-02 (1.0007e-02)\n",
      "Epoch: [9][20/48]\tLoss 1.0006e-02 (1.0010e-02)\n",
      "Epoch: [9][30/48]\tLoss 1.0004e-02 (1.0007e-02)\n",
      "Epoch: [9][40/48]\tLoss 9.9987e-03 (1.0005e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/48]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [10][10/48]\tLoss 9.9959e-03 (1.0009e-02)\n",
      "Epoch: [10][20/48]\tLoss 9.9929e-03 (1.0005e-02)\n",
      "Epoch: [10][30/48]\tLoss 1.0000e-02 (1.0004e-02)\n",
      "Epoch: [10][40/48]\tLoss 9.9964e-03 (1.0003e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/48]\tLoss 9.9972e-03 (9.9972e-03)\n",
      "Epoch: [11][10/48]\tLoss 9.9975e-03 (1.0000e-02)\n",
      "Epoch: [11][20/48]\tLoss 1.0005e-02 (1.0001e-02)\n",
      "Epoch: [11][30/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [11][40/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/48]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [12][10/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [12][20/48]\tLoss 1.0011e-02 (1.0001e-02)\n",
      "Epoch: [12][30/48]\tLoss 9.9994e-03 (1.0000e-02)\n",
      "Epoch: [12][40/48]\tLoss 9.9966e-03 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/48]\tLoss 9.9992e-03 (9.9992e-03)\n",
      "Epoch: [13][10/48]\tLoss 9.9977e-03 (1.0004e-02)\n",
      "Epoch: [13][20/48]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [13][30/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][40/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/48]\tLoss 9.9939e-03 (9.9939e-03)\n",
      "Epoch: [14][10/48]\tLoss 9.9977e-03 (9.9986e-03)\n",
      "Epoch: [14][20/48]\tLoss 1.0000e-02 (9.9995e-03)\n",
      "Epoch: [14][30/48]\tLoss 9.9957e-03 (9.9991e-03)\n",
      "Epoch: [14][40/48]\tLoss 1.0002e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/48]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][10/48]\tLoss 9.9963e-03 (9.9940e-03)\n",
      "Epoch: [15][20/48]\tLoss 1.0003e-02 (9.9978e-03)\n",
      "Epoch: [15][30/48]\tLoss 9.9992e-03 (9.9989e-03)\n",
      "Epoch: [15][40/48]\tLoss 1.0003e-02 (9.9997e-03)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [16][10/48]\tLoss 9.9855e-03 (9.9933e-03)\n",
      "Epoch: [16][20/48]\tLoss 1.0002e-02 (9.9991e-03)\n",
      "Epoch: [16][30/48]\tLoss 9.9980e-03 (9.9989e-03)\n",
      "Epoch: [16][40/48]\tLoss 1.0056e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/48]\tLoss 9.9963e-03 (9.9963e-03)\n",
      "Epoch: [17][10/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][20/48]\tLoss 9.9993e-03 (1.0003e-02)\n",
      "Epoch: [17][30/48]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [17][40/48]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [18][10/48]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [18][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/48]\tLoss 9.9939e-03 (1.0001e-02)\n",
      "Epoch: [18][40/48]\tLoss 9.9951e-03 (9.9998e-03)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/48]\tLoss 1.0016e-02 (1.0016e-02)\n",
      "Epoch: [19][10/48]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [19][20/48]\tLoss 9.9974e-03 (1.0001e-02)\n",
      "Epoch: [19][30/48]\tLoss 9.9911e-03 (1.0001e-02)\n",
      "Epoch: [19][40/48]\tLoss 1.0007e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][10/48]\tLoss 9.9994e-03 (9.9995e-03)\n",
      "Epoch: [20][20/48]\tLoss 1.0002e-02 (9.9983e-03)\n",
      "Epoch: [20][30/48]\tLoss 1.0001e-02 (9.9995e-03)\n",
      "Epoch: [20][40/48]\tLoss 1.0002e-02 (9.9999e-03)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [21][10/48]\tLoss 9.9852e-03 (9.9994e-03)\n",
      "Epoch: [21][20/48]\tLoss 9.9934e-03 (9.9976e-03)\n",
      "Epoch: [21][30/48]\tLoss 1.0003e-02 (9.9981e-03)\n",
      "Epoch: [21][40/48]\tLoss 9.9986e-03 (9.9984e-03)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/48]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [22][10/48]\tLoss 1.0001e-02 (9.9986e-03)\n",
      "Epoch: [22][20/48]\tLoss 9.9957e-03 (1.0004e-02)\n",
      "Epoch: [22][30/48]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [22][40/48]\tLoss 9.9998e-03 (1.0000e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/48]\tLoss 9.9904e-03 (9.9904e-03)\n",
      "Epoch: [23][10/48]\tLoss 1.0009e-02 (1.0001e-02)\n",
      "Epoch: [23][20/48]\tLoss 9.9848e-03 (9.9996e-03)\n",
      "Epoch: [23][30/48]\tLoss 1.0001e-02 (9.9994e-03)\n",
      "Epoch: [23][40/48]\tLoss 1.0004e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/48]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [24][10/48]\tLoss 1.0000e-02 (9.9999e-03)\n",
      "Epoch: [24][20/48]\tLoss 9.9989e-03 (9.9994e-03)\n",
      "Epoch: [24][30/48]\tLoss 9.9998e-03 (9.9986e-03)\n",
      "Epoch: [24][40/48]\tLoss 1.0001e-02 (9.9983e-03)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/48]\tLoss 9.9921e-03 (9.9921e-03)\n",
      "Epoch: [25][10/48]\tLoss 1.0007e-02 (1.0001e-02)\n",
      "Epoch: [25][20/48]\tLoss 1.0000e-02 (9.9982e-03)\n",
      "Epoch: [25][30/48]\tLoss 9.9941e-03 (9.9985e-03)\n",
      "Epoch: [25][40/48]\tLoss 9.9955e-03 (9.9981e-03)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/48]\tLoss 9.9988e-03 (9.9988e-03)\n",
      "Epoch: [26][10/48]\tLoss 1.0006e-02 (9.9999e-03)\n",
      "Epoch: [26][20/48]\tLoss 1.0004e-02 (1.0000e-02)\n",
      "Epoch: [26][30/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][40/48]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/48]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [27][10/48]\tLoss 9.9985e-03 (9.9997e-03)\n",
      "Epoch: [27][20/48]\tLoss 9.9995e-03 (9.9981e-03)\n",
      "Epoch: [27][30/48]\tLoss 1.0012e-02 (1.0000e-02)\n",
      "Epoch: [27][40/48]\tLoss 1.0000e-02 (9.9994e-03)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [28][10/48]\tLoss 1.0000e-02 (9.9938e-03)\n",
      "Epoch: [28][20/48]\tLoss 1.0000e-02 (9.9977e-03)\n",
      "Epoch: [28][30/48]\tLoss 1.0003e-02 (9.9979e-03)\n",
      "Epoch: [28][40/48]\tLoss 9.9907e-03 (9.9981e-03)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/48]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [29][10/48]\tLoss 1.0005e-02 (9.9970e-03)\n",
      "Epoch: [29][20/48]\tLoss 1.0003e-02 (9.9983e-03)\n",
      "Epoch: [29][30/48]\tLoss 1.0000e-02 (9.9984e-03)\n",
      "Epoch: [29][40/48]\tLoss 1.0000e-02 (9.9981e-03)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/48]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [30][10/48]\tLoss 9.9998e-03 (9.9994e-03)\n",
      "Epoch: [30][20/48]\tLoss 1.0002e-02 (9.9991e-03)\n",
      "Epoch: [30][30/48]\tLoss 9.9964e-03 (9.9986e-03)\n",
      "Epoch: [30][40/48]\tLoss 1.0003e-02 (9.9992e-03)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/49]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/154]\n",
      "Fill TS Repository [100/154]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries D-8\u001b[0m\n",
      "\u001b[32m-- Train samples size: 4804 - Test samples size: 7674\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/D-8/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][ 0/96]\tTotal Loss -2.2999e+00 (-2.2999e+00)\tConsistency Loss 2.2586e+00 (2.2586e+00)\tInconsistency Loss 1.1033e-01 (1.1033e-01)\tEntropy 2.2793e+00 (2.2793e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][ 0/96]\tTotal Loss -2.3789e+00 (-2.3789e+00)\tConsistency Loss 1.7623e+00 (1.7623e+00)\tInconsistency Loss 1.9256e-01 (1.9256e-01)\tEntropy 2.0706e+00 (2.0706e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][ 0/96]\tTotal Loss -2.3983e+00 (-2.3983e+00)\tConsistency Loss 1.7090e+00 (1.7090e+00)\tInconsistency Loss 1.9784e-01 (1.9784e-01)\tEntropy 2.0536e+00 (2.0536e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][ 0/96]\tTotal Loss -2.3654e+00 (-2.3654e+00)\tConsistency Loss 1.7675e+00 (1.7675e+00)\tInconsistency Loss 1.9219e-01 (1.9219e-01)\tEntropy 2.0664e+00 (2.0664e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][ 0/96]\tTotal Loss -2.3803e+00 (-2.3803e+00)\tConsistency Loss 1.7091e+00 (1.7091e+00)\tInconsistency Loss 1.9914e-01 (1.9914e-01)\tEntropy 2.0447e+00 (2.0447e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][ 0/96]\tTotal Loss -2.3694e+00 (-2.3694e+00)\tConsistency Loss 1.7125e+00 (1.7125e+00)\tInconsistency Loss 1.9834e-01 (1.9834e-01)\tEntropy 2.0410e+00 (2.0410e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][ 0/96]\tTotal Loss -2.3732e+00 (-2.3732e+00)\tConsistency Loss 1.7462e+00 (1.7462e+00)\tInconsistency Loss 1.9282e-01 (1.9282e-01)\tEntropy 2.0597e+00 (2.0597e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][ 0/96]\tTotal Loss -2.4011e+00 (-2.4011e+00)\tConsistency Loss 1.6800e+00 (1.6800e+00)\tInconsistency Loss 2.0361e-01 (2.0361e-01)\tEntropy 2.0405e+00 (2.0405e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][ 0/96]\tTotal Loss -2.4041e+00 (-2.4041e+00)\tConsistency Loss 1.6893e+00 (1.6893e+00)\tInconsistency Loss 1.9810e-01 (1.9810e-01)\tEntropy 2.0467e+00 (2.0467e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][ 0/96]\tTotal Loss -2.3982e+00 (-2.3982e+00)\tConsistency Loss 1.6794e+00 (1.6794e+00)\tInconsistency Loss 2.0813e-01 (2.0813e-01)\tEntropy 2.0388e+00 (2.0388e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][ 0/96]\tTotal Loss -2.3812e+00 (-2.3812e+00)\tConsistency Loss 1.7360e+00 (1.7360e+00)\tInconsistency Loss 1.9647e-01 (1.9647e-01)\tEntropy 2.0586e+00 (2.0586e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][ 0/96]\tTotal Loss -2.3724e+00 (-2.3724e+00)\tConsistency Loss 1.7059e+00 (1.7059e+00)\tInconsistency Loss 2.0733e-01 (2.0733e-01)\tEntropy 2.0392e+00 (2.0392e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][ 0/96]\tTotal Loss -2.3966e+00 (-2.3966e+00)\tConsistency Loss 1.6815e+00 (1.6815e+00)\tInconsistency Loss 2.0779e-01 (2.0779e-01)\tEntropy 2.0391e+00 (2.0391e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][ 0/96]\tTotal Loss -2.3892e+00 (-2.3892e+00)\tConsistency Loss 1.7685e+00 (1.7685e+00)\tInconsistency Loss 1.9578e-01 (1.9578e-01)\tEntropy 2.0788e+00 (2.0788e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][ 0/96]\tTotal Loss -2.3688e+00 (-2.3688e+00)\tConsistency Loss 1.7464e+00 (1.7464e+00)\tInconsistency Loss 1.9835e-01 (1.9835e-01)\tEntropy 2.0576e+00 (2.0576e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][ 0/96]\tTotal Loss -2.4136e+00 (-2.4136e+00)\tConsistency Loss 1.7101e+00 (1.7101e+00)\tInconsistency Loss 1.9640e-01 (1.9640e-01)\tEntropy 2.0618e+00 (2.0618e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][ 0/96]\tTotal Loss -2.4400e+00 (-2.4400e+00)\tConsistency Loss 1.6523e+00 (1.6523e+00)\tInconsistency Loss 2.0723e-01 (2.0723e-01)\tEntropy 2.0461e+00 (2.0461e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][ 0/96]\tTotal Loss -2.4483e+00 (-2.4483e+00)\tConsistency Loss 1.6927e+00 (1.6927e+00)\tInconsistency Loss 1.9131e-01 (1.9131e-01)\tEntropy 2.0705e+00 (2.0705e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][ 0/96]\tTotal Loss -2.3898e+00 (-2.3898e+00)\tConsistency Loss 1.7598e+00 (1.7598e+00)\tInconsistency Loss 1.9110e-01 (1.9110e-01)\tEntropy 2.0748e+00 (2.0748e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][ 0/96]\tTotal Loss -2.3933e+00 (-2.3933e+00)\tConsistency Loss 1.7432e+00 (1.7432e+00)\tInconsistency Loss 1.9270e-01 (1.9270e-01)\tEntropy 2.0683e+00 (2.0683e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][ 0/96]\tTotal Loss -2.3683e+00 (-2.3683e+00)\tConsistency Loss 1.7205e+00 (1.7205e+00)\tInconsistency Loss 2.0009e-01 (2.0009e-01)\tEntropy 2.0444e+00 (2.0444e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][ 0/96]\tTotal Loss -2.3951e+00 (-2.3951e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9572e-01 (1.9572e-01)\tEntropy 2.0566e+00 (2.0566e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][ 0/96]\tTotal Loss -2.3869e+00 (-2.3869e+00)\tConsistency Loss 1.7258e+00 (1.7258e+00)\tInconsistency Loss 1.9727e-01 (1.9727e-01)\tEntropy 2.0563e+00 (2.0563e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][ 0/96]\tTotal Loss -2.4259e+00 (-2.4259e+00)\tConsistency Loss 1.6877e+00 (1.6877e+00)\tInconsistency Loss 1.9798e-01 (1.9798e-01)\tEntropy 2.0568e+00 (2.0568e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][ 0/96]\tTotal Loss -2.3970e+00 (-2.3970e+00)\tConsistency Loss 1.6756e+00 (1.6756e+00)\tInconsistency Loss 2.0487e-01 (2.0487e-01)\tEntropy 2.0363e+00 (2.0363e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][ 0/96]\tTotal Loss -2.4428e+00 (-2.4428e+00)\tConsistency Loss 1.6471e+00 (1.6471e+00)\tInconsistency Loss 2.0836e-01 (2.0836e-01)\tEntropy 2.0449e+00 (2.0449e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][ 0/96]\tTotal Loss -2.3639e+00 (-2.3639e+00)\tConsistency Loss 1.7254e+00 (1.7254e+00)\tInconsistency Loss 1.9780e-01 (1.9780e-01)\tEntropy 2.0447e+00 (2.0447e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][ 0/96]\tTotal Loss -2.4572e+00 (-2.4572e+00)\tConsistency Loss 1.6006e+00 (1.6006e+00)\tInconsistency Loss 2.0599e-01 (2.0599e-01)\tEntropy 2.0289e+00 (2.0289e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][ 0/96]\tTotal Loss -2.4091e+00 (-2.4091e+00)\tConsistency Loss 1.6853e+00 (1.6853e+00)\tInconsistency Loss 1.9678e-01 (1.9678e-01)\tEntropy 2.0472e+00 (2.0472e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][ 0/96]\tTotal Loss -2.3971e+00 (-2.3971e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 2.0323e-01 (2.0323e-01)\tEntropy 2.0576e+00 (2.0576e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][ 0/96]\tTotal Loss -2.3593e+00 (-2.3593e+00)\tConsistency Loss 1.7255e+00 (1.7255e+00)\tInconsistency Loss 2.0642e-01 (2.0642e-01)\tEntropy 2.0424e+00 (2.0424e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][ 0/96]\tTotal Loss -2.3776e+00 (-2.3776e+00)\tConsistency Loss 1.7468e+00 (1.7468e+00)\tInconsistency Loss 2.0102e-01 (2.0102e-01)\tEntropy 2.0622e+00 (2.0622e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][ 0/96]\tTotal Loss -2.4135e+00 (-2.4135e+00)\tConsistency Loss 1.7025e+00 (1.7025e+00)\tInconsistency Loss 2.0256e-01 (2.0256e-01)\tEntropy 2.0580e+00 (2.0580e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][ 0/96]\tTotal Loss -2.4166e+00 (-2.4166e+00)\tConsistency Loss 1.7145e+00 (1.7145e+00)\tInconsistency Loss 1.9325e-01 (1.9325e-01)\tEntropy 2.0655e+00 (2.0655e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][ 0/96]\tTotal Loss -2.4018e+00 (-2.4018e+00)\tConsistency Loss 1.7009e+00 (1.7009e+00)\tInconsistency Loss 2.0073e-01 (2.0073e-01)\tEntropy 2.0513e+00 (2.0513e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][ 0/96]\tTotal Loss -2.3287e+00 (-2.3287e+00)\tConsistency Loss 1.7456e+00 (1.7456e+00)\tInconsistency Loss 2.0203e-01 (2.0203e-01)\tEntropy 2.0371e+00 (2.0371e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][ 0/96]\tTotal Loss -2.3265e+00 (-2.3265e+00)\tConsistency Loss 1.7172e+00 (1.7172e+00)\tInconsistency Loss 2.1048e-01 (2.1048e-01)\tEntropy 2.0219e+00 (2.0219e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][ 0/96]\tTotal Loss -2.3469e+00 (-2.3469e+00)\tConsistency Loss 1.7587e+00 (1.7587e+00)\tInconsistency Loss 1.9698e-01 (1.9698e-01)\tEntropy 2.0528e+00 (2.0528e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][ 0/96]\tTotal Loss -2.4496e+00 (-2.4496e+00)\tConsistency Loss 1.6732e+00 (1.6732e+00)\tInconsistency Loss 1.9483e-01 (1.9483e-01)\tEntropy 2.0614e+00 (2.0614e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][ 0/96]\tTotal Loss -2.3751e+00 (-2.3751e+00)\tConsistency Loss 1.7392e+00 (1.7392e+00)\tInconsistency Loss 1.9736e-01 (1.9736e-01)\tEntropy 2.0572e+00 (2.0572e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][ 0/96]\tTotal Loss -2.4204e+00 (-2.4204e+00)\tConsistency Loss 1.6972e+00 (1.6972e+00)\tInconsistency Loss 1.9527e-01 (1.9527e-01)\tEntropy 2.0588e+00 (2.0588e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][ 0/96]\tTotal Loss -2.3909e+00 (-2.3909e+00)\tConsistency Loss 1.7173e+00 (1.7173e+00)\tInconsistency Loss 1.9968e-01 (1.9968e-01)\tEntropy 2.0541e+00 (2.0541e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][ 0/96]\tTotal Loss -2.4126e+00 (-2.4126e+00)\tConsistency Loss 1.7077e+00 (1.7077e+00)\tInconsistency Loss 2.0078e-01 (2.0078e-01)\tEntropy 2.0602e+00 (2.0602e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][ 0/96]\tTotal Loss -2.4703e+00 (-2.4703e+00)\tConsistency Loss 1.6462e+00 (1.6462e+00)\tInconsistency Loss 2.0646e-01 (2.0646e-01)\tEntropy 2.0582e+00 (2.0582e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][ 0/96]\tTotal Loss -2.4361e+00 (-2.4361e+00)\tConsistency Loss 1.6958e+00 (1.6958e+00)\tInconsistency Loss 1.9032e-01 (1.9032e-01)\tEntropy 2.0660e+00 (2.0660e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][ 0/96]\tTotal Loss -2.4102e+00 (-2.4102e+00)\tConsistency Loss 1.7359e+00 (1.7359e+00)\tInconsistency Loss 1.8802e-01 (1.8802e-01)\tEntropy 2.0731e+00 (2.0731e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][ 0/96]\tTotal Loss -2.4321e+00 (-2.4321e+00)\tConsistency Loss 1.7268e+00 (1.7268e+00)\tInconsistency Loss 1.8340e-01 (1.8340e-01)\tEntropy 2.0795e+00 (2.0795e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][ 0/96]\tTotal Loss -2.3999e+00 (-2.3999e+00)\tConsistency Loss 1.7606e+00 (1.7606e+00)\tInconsistency Loss 1.9712e-01 (1.9712e-01)\tEntropy 2.0802e+00 (2.0802e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][ 0/96]\tTotal Loss -2.4156e+00 (-2.4156e+00)\tConsistency Loss 1.7143e+00 (1.7143e+00)\tInconsistency Loss 1.8775e-01 (1.8775e-01)\tEntropy 2.0650e+00 (2.0650e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][ 0/96]\tTotal Loss -2.4277e+00 (-2.4277e+00)\tConsistency Loss 1.7297e+00 (1.7297e+00)\tInconsistency Loss 1.8670e-01 (1.8670e-01)\tEntropy 2.0787e+00 (2.0787e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "D-9\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2383/7206 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/D-9/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/47]\tLoss 1.0031e+00 (1.0031e+00)\n",
      "Epoch: [1][10/47]\tLoss 3.4844e-01 (6.2486e-01)\n",
      "Epoch: [1][20/47]\tLoss 1.2115e-01 (4.2437e-01)\n",
      "Epoch: [1][30/47]\tLoss 4.2414e-02 (3.1038e-01)\n",
      "Epoch: [1][40/47]\tLoss 1.4822e-02 (2.4070e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/47]\tLoss 1.0101e-02 (1.0101e-02)\n",
      "Epoch: [2][10/47]\tLoss 1.0022e-02 (1.0066e-02)\n",
      "Epoch: [2][20/47]\tLoss 1.0022e-02 (1.0054e-02)\n",
      "Epoch: [2][30/47]\tLoss 1.0056e-02 (1.0052e-02)\n",
      "Epoch: [2][40/47]\tLoss 9.9881e-03 (1.0044e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/47]\tLoss 9.9907e-03 (9.9907e-03)\n",
      "Epoch: [3][10/47]\tLoss 1.0018e-02 (1.0023e-02)\n",
      "Epoch: [3][20/47]\tLoss 1.0061e-02 (1.0026e-02)\n",
      "Epoch: [3][30/47]\tLoss 1.0007e-02 (1.0022e-02)\n",
      "Epoch: [3][40/47]\tLoss 1.0008e-02 (1.0022e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/47]\tLoss 1.0021e-02 (1.0021e-02)\n",
      "Epoch: [4][10/47]\tLoss 1.0012e-02 (1.0016e-02)\n",
      "Epoch: [4][20/47]\tLoss 1.0002e-02 (1.0020e-02)\n",
      "Epoch: [4][30/47]\tLoss 1.0009e-02 (1.0018e-02)\n",
      "Epoch: [4][40/47]\tLoss 1.0007e-02 (1.0016e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/47]\tLoss 1.0014e-02 (1.0014e-02)\n",
      "Epoch: [5][10/47]\tLoss 1.0013e-02 (1.0014e-02)\n",
      "Epoch: [5][20/47]\tLoss 9.9833e-03 (1.0009e-02)\n",
      "Epoch: [5][30/47]\tLoss 1.0007e-02 (1.0009e-02)\n",
      "Epoch: [5][40/47]\tLoss 1.0002e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/47]\tLoss 9.9998e-03 (9.9998e-03)\n",
      "Epoch: [6][10/47]\tLoss 1.0011e-02 (1.0002e-02)\n",
      "Epoch: [6][20/47]\tLoss 1.0008e-02 (1.0005e-02)\n",
      "Epoch: [6][30/47]\tLoss 9.9997e-03 (1.0005e-02)\n",
      "Epoch: [6][40/47]\tLoss 9.9967e-03 (1.0005e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/47]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [7][10/47]\tLoss 1.0012e-02 (1.0004e-02)\n",
      "Epoch: [7][20/47]\tLoss 9.9976e-03 (1.0004e-02)\n",
      "Epoch: [7][30/47]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [7][40/47]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [8][10/47]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [8][20/47]\tLoss 1.0011e-02 (1.0003e-02)\n",
      "Epoch: [8][30/47]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "Epoch: [8][40/47]\tLoss 1.0003e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/47]\tLoss 9.9878e-03 (9.9878e-03)\n",
      "Epoch: [9][10/47]\tLoss 1.0009e-02 (1.0008e-02)\n",
      "Epoch: [9][20/47]\tLoss 9.9938e-03 (1.0007e-02)\n",
      "Epoch: [9][30/47]\tLoss 1.0015e-02 (1.0006e-02)\n",
      "Epoch: [9][40/47]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/47]\tLoss 1.0016e-02 (1.0016e-02)\n",
      "Epoch: [10][10/47]\tLoss 1.0004e-02 (1.0006e-02)\n",
      "Epoch: [10][20/47]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [10][30/47]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [10][40/47]\tLoss 9.9844e-03 (1.0003e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/47]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [11][10/47]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [11][20/47]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [11][30/47]\tLoss 9.9980e-03 (1.0001e-02)\n",
      "Epoch: [11][40/47]\tLoss 9.9989e-03 (1.0001e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][10/47]\tLoss 9.9890e-03 (1.0004e-02)\n",
      "Epoch: [12][20/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][30/47]\tLoss 9.9965e-03 (1.0002e-02)\n",
      "Epoch: [12][40/47]\tLoss 9.9768e-03 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][10/47]\tLoss 1.0002e-02 (9.9973e-03)\n",
      "Epoch: [13][20/47]\tLoss 1.0007e-02 (1.0000e-02)\n",
      "Epoch: [13][30/47]\tLoss 1.0020e-02 (9.9971e-03)\n",
      "Epoch: [13][40/47]\tLoss 1.0002e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/47]\tLoss 9.9903e-03 (9.9903e-03)\n",
      "Epoch: [14][10/47]\tLoss 1.0013e-02 (1.0013e-02)\n",
      "Epoch: [14][20/47]\tLoss 1.0002e-02 (1.0010e-02)\n",
      "Epoch: [14][30/47]\tLoss 1.0004e-02 (1.0007e-02)\n",
      "Epoch: [14][40/47]\tLoss 1.0004e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/47]\tLoss 9.9950e-03 (9.9950e-03)\n",
      "Epoch: [15][10/47]\tLoss 1.0002e-02 (1.0000e-02)\n",
      "Epoch: [15][20/47]\tLoss 9.9961e-03 (9.9999e-03)\n",
      "Epoch: [15][30/47]\tLoss 1.0006e-02 (1.0000e-02)\n",
      "Epoch: [15][40/47]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/47]\tLoss 9.9963e-03 (9.9963e-03)\n",
      "Epoch: [16][10/47]\tLoss 1.0018e-02 (1.0001e-02)\n",
      "Epoch: [16][20/47]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [16][30/47]\tLoss 1.0003e-02 (1.0000e-02)\n",
      "Epoch: [16][40/47]\tLoss 1.0003e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [17][10/47]\tLoss 9.9993e-03 (1.0000e-02)\n",
      "Epoch: [17][20/47]\tLoss 1.0001e-02 (9.9993e-03)\n",
      "Epoch: [17][30/47]\tLoss 9.9993e-03 (9.9999e-03)\n",
      "Epoch: [17][40/47]\tLoss 1.0007e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/47]\tLoss 9.9909e-03 (9.9909e-03)\n",
      "Epoch: [18][10/47]\tLoss 1.0002e-02 (9.9987e-03)\n",
      "Epoch: [18][20/47]\tLoss 9.9922e-03 (9.9973e-03)\n",
      "Epoch: [18][30/47]\tLoss 9.9903e-03 (9.9976e-03)\n",
      "Epoch: [18][40/47]\tLoss 1.0030e-02 (9.9992e-03)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/47]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [19][10/47]\tLoss 1.0002e-02 (9.9984e-03)\n",
      "Epoch: [19][20/47]\tLoss 9.9472e-03 (9.9957e-03)\n",
      "Epoch: [19][30/47]\tLoss 1.0002e-02 (9.9987e-03)\n",
      "Epoch: [19][40/47]\tLoss 1.0002e-02 (9.9990e-03)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/47]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [20][10/47]\tLoss 9.9724e-03 (9.9986e-03)\n",
      "Epoch: [20][20/47]\tLoss 9.9891e-03 (9.9998e-03)\n",
      "Epoch: [20][30/47]\tLoss 9.9965e-03 (1.0000e-02)\n",
      "Epoch: [20][40/47]\tLoss 9.9924e-03 (1.0000e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/47]\tLoss 9.9860e-03 (9.9860e-03)\n",
      "Epoch: [21][10/47]\tLoss 9.9982e-03 (1.0001e-02)\n",
      "Epoch: [21][20/47]\tLoss 1.0004e-02 (1.0000e-02)\n",
      "Epoch: [21][30/47]\tLoss 1.0001e-02 (9.9998e-03)\n",
      "Epoch: [21][40/47]\tLoss 9.9972e-03 (9.9998e-03)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/47]\tLoss 9.9937e-03 (9.9937e-03)\n",
      "Epoch: [22][10/47]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [22][20/47]\tLoss 1.0006e-02 (9.9990e-03)\n",
      "Epoch: [22][30/47]\tLoss 9.9991e-03 (9.9995e-03)\n",
      "Epoch: [22][40/47]\tLoss 1.0007e-02 (9.9991e-03)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/47]\tLoss 9.9942e-03 (9.9942e-03)\n",
      "Epoch: [23][10/47]\tLoss 1.0009e-02 (1.0006e-02)\n",
      "Epoch: [23][20/47]\tLoss 9.9802e-03 (1.0002e-02)\n",
      "Epoch: [23][30/47]\tLoss 9.9934e-03 (1.0001e-02)\n",
      "Epoch: [23][40/47]\tLoss 9.9965e-03 (9.9988e-03)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/47]\tLoss 9.9773e-03 (9.9773e-03)\n",
      "Epoch: [24][10/47]\tLoss 1.0002e-02 (9.9965e-03)\n",
      "Epoch: [24][20/47]\tLoss 9.9945e-03 (9.9953e-03)\n",
      "Epoch: [24][30/47]\tLoss 1.0002e-02 (9.9960e-03)\n",
      "Epoch: [24][40/47]\tLoss 1.0001e-02 (9.9976e-03)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/47]\tLoss 9.9914e-03 (9.9914e-03)\n",
      "Epoch: [25][10/47]\tLoss 1.0009e-02 (9.9981e-03)\n",
      "Epoch: [25][20/47]\tLoss 1.0003e-02 (9.9981e-03)\n",
      "Epoch: [25][30/47]\tLoss 9.9910e-03 (9.9985e-03)\n",
      "Epoch: [25][40/47]\tLoss 9.9845e-03 (9.9982e-03)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/47]\tLoss 1.0044e-02 (1.0044e-02)\n",
      "Epoch: [26][10/47]\tLoss 9.9361e-03 (9.9925e-03)\n",
      "Epoch: [26][20/47]\tLoss 1.0001e-02 (9.9971e-03)\n",
      "Epoch: [26][30/47]\tLoss 9.9951e-03 (9.9961e-03)\n",
      "Epoch: [26][40/47]\tLoss 1.0001e-02 (9.9965e-03)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/47]\tLoss 9.9902e-03 (9.9902e-03)\n",
      "Epoch: [27][10/47]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [27][20/47]\tLoss 1.0001e-02 (9.9990e-03)\n",
      "Epoch: [27][30/47]\tLoss 9.9989e-03 (9.9985e-03)\n",
      "Epoch: [27][40/47]\tLoss 1.0004e-02 (9.9982e-03)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/47]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [28][10/47]\tLoss 9.9973e-03 (1.0000e-02)\n",
      "Epoch: [28][20/47]\tLoss 1.0002e-02 (9.9998e-03)\n",
      "Epoch: [28][30/47]\tLoss 9.9892e-03 (9.9990e-03)\n",
      "Epoch: [28][40/47]\tLoss 9.9830e-03 (9.9996e-03)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/47]\tLoss 1.0012e-02 (1.0012e-02)\n",
      "Epoch: [29][10/47]\tLoss 1.0004e-02 (9.9995e-03)\n",
      "Epoch: [29][20/47]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [29][30/47]\tLoss 1.0015e-02 (1.0000e-02)\n",
      "Epoch: [29][40/47]\tLoss 1.0003e-02 (9.9987e-03)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/47]\tLoss 9.9963e-03 (9.9963e-03)\n",
      "Epoch: [30][10/47]\tLoss 9.9970e-03 (9.9976e-03)\n",
      "Epoch: [30][20/47]\tLoss 9.9880e-03 (9.9973e-03)\n",
      "Epoch: [30][30/47]\tLoss 1.0016e-02 (9.9986e-03)\n",
      "Epoch: [30][40/47]\tLoss 1.0000e-02 (9.9983e-03)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/48]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/145]\n",
      "Fill TS Repository [100/145]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries D-9\u001b[0m\n",
      "\u001b[32m-- Train samples size: 4766 - Test samples size: 7206\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/D-9/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][ 0/95]\tTotal Loss -2.3012e+00 (-2.3012e+00)\tConsistency Loss 2.2596e+00 (2.2596e+00)\tInconsistency Loss 1.1020e-01 (1.1020e-01)\tEntropy 2.2804e+00 (2.2804e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][ 0/95]\tTotal Loss -2.4490e+00 (-2.4490e+00)\tConsistency Loss 1.7753e+00 (1.7753e+00)\tInconsistency Loss 1.7616e-01 (1.7616e-01)\tEntropy 2.1122e+00 (2.1122e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][ 0/95]\tTotal Loss -2.4181e+00 (-2.4181e+00)\tConsistency Loss 1.7351e+00 (1.7351e+00)\tInconsistency Loss 2.0440e-01 (2.0440e-01)\tEntropy 2.0766e+00 (2.0766e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][ 0/95]\tTotal Loss -2.4954e+00 (-2.4954e+00)\tConsistency Loss 1.7254e+00 (1.7254e+00)\tInconsistency Loss 1.8802e-01 (1.8802e-01)\tEntropy 2.1104e+00 (2.1104e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][ 0/95]\tTotal Loss -2.4260e+00 (-2.4260e+00)\tConsistency Loss 1.7758e+00 (1.7758e+00)\tInconsistency Loss 1.9139e-01 (1.9139e-01)\tEntropy 2.1009e+00 (2.1009e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][ 0/95]\tTotal Loss -2.4314e+00 (-2.4314e+00)\tConsistency Loss 1.6880e+00 (1.6880e+00)\tInconsistency Loss 2.1122e-01 (2.1122e-01)\tEntropy 2.0597e+00 (2.0597e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][ 0/95]\tTotal Loss -2.4725e+00 (-2.4725e+00)\tConsistency Loss 1.6776e+00 (1.6776e+00)\tInconsistency Loss 1.9542e-01 (1.9542e-01)\tEntropy 2.0750e+00 (2.0750e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][ 0/95]\tTotal Loss -2.3765e+00 (-2.3765e+00)\tConsistency Loss 1.7764e+00 (1.7764e+00)\tInconsistency Loss 1.8054e-01 (1.8054e-01)\tEntropy 2.0764e+00 (2.0764e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][ 0/95]\tTotal Loss -2.4883e+00 (-2.4883e+00)\tConsistency Loss 1.6555e+00 (1.6555e+00)\tInconsistency Loss 2.0229e-01 (2.0229e-01)\tEntropy 2.0719e+00 (2.0719e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][ 0/95]\tTotal Loss -2.5034e+00 (-2.5034e+00)\tConsistency Loss 1.6424e+00 (1.6424e+00)\tInconsistency Loss 1.9132e-01 (1.9132e-01)\tEntropy 2.0729e+00 (2.0729e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][ 0/95]\tTotal Loss -2.4883e+00 (-2.4883e+00)\tConsistency Loss 1.7049e+00 (1.7049e+00)\tInconsistency Loss 1.9038e-01 (1.9038e-01)\tEntropy 2.0966e+00 (2.0966e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][ 0/95]\tTotal Loss -2.4843e+00 (-2.4843e+00)\tConsistency Loss 1.6708e+00 (1.6708e+00)\tInconsistency Loss 1.9216e-01 (1.9216e-01)\tEntropy 2.0776e+00 (2.0776e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][ 0/95]\tTotal Loss -2.5444e+00 (-2.5444e+00)\tConsistency Loss 1.6132e+00 (1.6132e+00)\tInconsistency Loss 1.9211e-01 (1.9211e-01)\tEntropy 2.0788e+00 (2.0788e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][ 0/95]\tTotal Loss -2.5248e+00 (-2.5248e+00)\tConsistency Loss 1.6511e+00 (1.6511e+00)\tInconsistency Loss 1.9202e-01 (1.9202e-01)\tEntropy 2.0879e+00 (2.0879e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][ 0/95]\tTotal Loss -2.4679e+00 (-2.4679e+00)\tConsistency Loss 1.6894e+00 (1.6894e+00)\tInconsistency Loss 1.8810e-01 (1.8810e-01)\tEntropy 2.0786e+00 (2.0786e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][ 0/95]\tTotal Loss -2.4109e+00 (-2.4109e+00)\tConsistency Loss 1.7500e+00 (1.7500e+00)\tInconsistency Loss 2.4252e-01 (2.4252e-01)\tEntropy 2.0805e+00 (2.0805e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][ 0/95]\tTotal Loss -2.3128e+00 (-2.3128e+00)\tConsistency Loss 1.7825e+00 (1.7825e+00)\tInconsistency Loss 1.9121e-01 (1.9121e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][ 0/95]\tTotal Loss -2.4084e+00 (-2.4084e+00)\tConsistency Loss 1.7977e+00 (1.7977e+00)\tInconsistency Loss 1.7927e-01 (1.7927e-01)\tEntropy 2.1030e+00 (2.1030e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][ 0/95]\tTotal Loss -2.5186e+00 (-2.5186e+00)\tConsistency Loss 1.6675e+00 (1.6675e+00)\tInconsistency Loss 1.9525e-01 (1.9525e-01)\tEntropy 2.0931e+00 (2.0931e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][ 0/95]\tTotal Loss -2.2996e+00 (-2.2996e+00)\tConsistency Loss 1.8926e+00 (1.8926e+00)\tInconsistency Loss 1.8183e-01 (1.8183e-01)\tEntropy 2.0961e+00 (2.0961e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][ 0/95]\tTotal Loss -2.4399e+00 (-2.4399e+00)\tConsistency Loss 1.6815e+00 (1.6815e+00)\tInconsistency Loss 1.8882e-01 (1.8882e-01)\tEntropy 2.0607e+00 (2.0607e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][ 0/95]\tTotal Loss -2.5118e+00 (-2.5118e+00)\tConsistency Loss 1.6275e+00 (1.6275e+00)\tInconsistency Loss 1.9047e-01 (1.9047e-01)\tEntropy 2.0696e+00 (2.0696e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][ 0/95]\tTotal Loss -2.5066e+00 (-2.5066e+00)\tConsistency Loss 1.7012e+00 (1.7012e+00)\tInconsistency Loss 2.0070e-01 (2.0070e-01)\tEntropy 2.1039e+00 (2.1039e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][ 0/95]\tTotal Loss -2.4316e+00 (-2.4316e+00)\tConsistency Loss 1.7569e+00 (1.7569e+00)\tInconsistency Loss 1.9517e-01 (1.9517e-01)\tEntropy 2.0942e+00 (2.0942e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][ 0/95]\tTotal Loss -2.5080e+00 (-2.5080e+00)\tConsistency Loss 1.6986e+00 (1.6986e+00)\tInconsistency Loss 1.7993e-01 (1.7993e-01)\tEntropy 2.1033e+00 (2.1033e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][ 0/95]\tTotal Loss -2.4965e+00 (-2.4965e+00)\tConsistency Loss 1.6896e+00 (1.6896e+00)\tInconsistency Loss 1.8455e-01 (1.8455e-01)\tEntropy 2.0931e+00 (2.0931e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][ 0/95]\tTotal Loss -2.4761e+00 (-2.4761e+00)\tConsistency Loss 1.6977e+00 (1.6977e+00)\tInconsistency Loss 2.2387e-01 (2.2387e-01)\tEntropy 2.0869e+00 (2.0869e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][ 0/95]\tTotal Loss -2.4548e+00 (-2.4548e+00)\tConsistency Loss 1.6585e+00 (1.6585e+00)\tInconsistency Loss 1.8978e-01 (1.8978e-01)\tEntropy 2.0567e+00 (2.0567e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][ 0/95]\tTotal Loss -2.5594e+00 (-2.5594e+00)\tConsistency Loss 1.6384e+00 (1.6384e+00)\tInconsistency Loss 1.8650e-01 (1.8650e-01)\tEntropy 2.0989e+00 (2.0989e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][ 0/95]\tTotal Loss -2.4770e+00 (-2.4770e+00)\tConsistency Loss 1.6985e+00 (1.6985e+00)\tInconsistency Loss 1.8855e-01 (1.8855e-01)\tEntropy 2.0878e+00 (2.0878e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][ 0/95]\tTotal Loss -2.5199e+00 (-2.5199e+00)\tConsistency Loss 1.6671e+00 (1.6671e+00)\tInconsistency Loss 1.9425e-01 (1.9425e-01)\tEntropy 2.0935e+00 (2.0935e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][ 0/95]\tTotal Loss -2.5039e+00 (-2.5039e+00)\tConsistency Loss 1.6663e+00 (1.6663e+00)\tInconsistency Loss 2.3083e-01 (2.3083e-01)\tEntropy 2.0851e+00 (2.0851e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][ 0/95]\tTotal Loss -2.6144e+00 (-2.6144e+00)\tConsistency Loss 1.5540e+00 (1.5540e+00)\tInconsistency Loss 1.8557e-01 (1.8557e-01)\tEntropy 2.0842e+00 (2.0842e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][ 0/95]\tTotal Loss -2.4250e+00 (-2.4250e+00)\tConsistency Loss 1.8212e+00 (1.8212e+00)\tInconsistency Loss 1.9088e-01 (1.9088e-01)\tEntropy 2.1231e+00 (2.1231e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][ 0/95]\tTotal Loss -2.5007e+00 (-2.5007e+00)\tConsistency Loss 1.6884e+00 (1.6884e+00)\tInconsistency Loss 1.9411e-01 (1.9411e-01)\tEntropy 2.0946e+00 (2.0946e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][ 0/95]\tTotal Loss -2.5224e+00 (-2.5224e+00)\tConsistency Loss 1.6715e+00 (1.6715e+00)\tInconsistency Loss 1.7678e-01 (1.7678e-01)\tEntropy 2.0970e+00 (2.0970e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][ 0/95]\tTotal Loss -2.5516e+00 (-2.5516e+00)\tConsistency Loss 1.5697e+00 (1.5697e+00)\tInconsistency Loss 1.9842e-01 (1.9842e-01)\tEntropy 2.0607e+00 (2.0607e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][ 0/95]\tTotal Loss -2.4961e+00 (-2.4961e+00)\tConsistency Loss 1.6586e+00 (1.6586e+00)\tInconsistency Loss 1.9493e-01 (1.9493e-01)\tEntropy 2.0774e+00 (2.0774e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][ 0/95]\tTotal Loss -2.3654e+00 (-2.3654e+00)\tConsistency Loss 1.7880e+00 (1.7880e+00)\tInconsistency Loss 2.0134e-01 (2.0134e-01)\tEntropy 2.0767e+00 (2.0767e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][ 0/95]\tTotal Loss -2.4966e+00 (-2.4966e+00)\tConsistency Loss 1.6884e+00 (1.6884e+00)\tInconsistency Loss 1.9989e-01 (1.9989e-01)\tEntropy 2.0925e+00 (2.0925e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][ 0/95]\tTotal Loss -2.5282e+00 (-2.5282e+00)\tConsistency Loss 1.6620e+00 (1.6620e+00)\tInconsistency Loss 1.9860e-01 (1.9860e-01)\tEntropy 2.0951e+00 (2.0951e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][ 0/95]\tTotal Loss -2.4732e+00 (-2.4732e+00)\tConsistency Loss 1.7356e+00 (1.7356e+00)\tInconsistency Loss 1.9478e-01 (1.9478e-01)\tEntropy 2.1044e+00 (2.1044e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][ 0/95]\tTotal Loss -2.5615e+00 (-2.5615e+00)\tConsistency Loss 1.6549e+00 (1.6549e+00)\tInconsistency Loss 1.8393e-01 (1.8393e-01)\tEntropy 2.1082e+00 (2.1082e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][ 0/95]\tTotal Loss -2.5896e+00 (-2.5896e+00)\tConsistency Loss 1.5926e+00 (1.5926e+00)\tInconsistency Loss 1.8809e-01 (1.8809e-01)\tEntropy 2.0911e+00 (2.0911e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][ 0/95]\tTotal Loss -2.5214e+00 (-2.5214e+00)\tConsistency Loss 1.6561e+00 (1.6561e+00)\tInconsistency Loss 1.9277e-01 (1.9277e-01)\tEntropy 2.0888e+00 (2.0888e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][ 0/95]\tTotal Loss -2.5411e+00 (-2.5411e+00)\tConsistency Loss 1.5880e+00 (1.5880e+00)\tInconsistency Loss 1.9436e-01 (1.9436e-01)\tEntropy 2.0645e+00 (2.0645e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][ 0/95]\tTotal Loss -2.5872e+00 (-2.5872e+00)\tConsistency Loss 1.6019e+00 (1.6019e+00)\tInconsistency Loss 1.8883e-01 (1.8883e-01)\tEntropy 2.0946e+00 (2.0946e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][ 0/95]\tTotal Loss -2.4158e+00 (-2.4158e+00)\tConsistency Loss 1.7438e+00 (1.7438e+00)\tInconsistency Loss 2.3862e-01 (2.3862e-01)\tEntropy 2.0798e+00 (2.0798e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][ 0/95]\tTotal Loss -2.5579e+00 (-2.5579e+00)\tConsistency Loss 1.6002e+00 (1.6002e+00)\tInconsistency Loss 1.9768e-01 (1.9768e-01)\tEntropy 2.0790e+00 (2.0790e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][ 0/95]\tTotal Loss -2.5151e+00 (-2.5151e+00)\tConsistency Loss 1.6823e+00 (1.6823e+00)\tInconsistency Loss 1.9279e-01 (1.9279e-01)\tEntropy 2.0987e+00 (2.0987e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "F-2\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2661/8426 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/F-2/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0011e+00 (1.0011e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4827e-01 (6.2424e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2144e-01 (4.2426e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2107e-02 (3.1036e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4680e-02 (2.4069e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0022e-02 (1.9558e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0050e-02 (1.0050e-02)\n",
      "Epoch: [2][10/53]\tLoss 9.9997e-03 (1.0045e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0015e-02 (1.0030e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0023e-02 (1.0029e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0023e-02 (1.0030e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0017e-02 (1.0027e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0038e-02 (1.0038e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0008e-02 (1.0016e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0014e-02 (1.0012e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0002e-02 (1.0012e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0007e-02 (1.0012e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0005e-02 (1.0011e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0009e-02 (1.0009e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0010e-02 (1.0006e-02)\n",
      "Epoch: [4][30/53]\tLoss 9.9975e-03 (1.0007e-02)\n",
      "Epoch: [4][40/53]\tLoss 9.9976e-03 (1.0006e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0004e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0009e-02 (1.0005e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0039e-02 (1.0007e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0002e-02 (1.0006e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0005e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0006e-02 (1.0005e-02)\n",
      "Epoch: [6][20/53]\tLoss 9.9989e-03 (1.0005e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0022e-02 (1.0006e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0003e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][10/53]\tLoss 9.9970e-03 (1.0004e-02)\n",
      "Epoch: [7][20/53]\tLoss 9.9970e-03 (1.0003e-02)\n",
      "Epoch: [7][30/53]\tLoss 9.9968e-03 (1.0003e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0008e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 9.9919e-03 (9.9919e-03)\n",
      "Epoch: [8][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [8][20/53]\tLoss 9.9997e-03 (1.0003e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][40/53]\tLoss 9.9996e-03 (1.0003e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [9][10/53]\tLoss 9.9950e-03 (1.0002e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0006e-02 (1.0002e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0015e-02 (1.0002e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 9.9998e-03 (9.9998e-03)\n",
      "Epoch: [10][10/53]\tLoss 9.9903e-03 (1.0001e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0000e-02 (9.9999e-03)\n",
      "Epoch: [10][30/53]\tLoss 1.0008e-02 (1.0001e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0006e-02 (1.0001e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0006e-02 (1.0002e-02)\n",
      "Epoch: [11][30/53]\tLoss 9.9938e-03 (1.0002e-02)\n",
      "Epoch: [11][40/53]\tLoss 9.9965e-03 (1.0002e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][10/53]\tLoss 9.9924e-03 (1.0001e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0006e-02 (1.0003e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0007e-02 (1.0001e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 9.9989e-03 (9.9989e-03)\n",
      "Epoch: [15][10/53]\tLoss 9.9999e-03 (1.0000e-02)\n",
      "Epoch: [15][20/53]\tLoss 9.9993e-03 (1.0000e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [15][40/53]\tLoss 9.9988e-03 (1.0001e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 9.9947e-03 (9.9947e-03)\n",
      "Epoch: [16][10/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [16][20/53]\tLoss 9.9997e-03 (1.0001e-02)\n",
      "Epoch: [16][30/53]\tLoss 9.9962e-03 (1.0000e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [16][50/53]\tLoss 9.9883e-03 (9.9995e-03)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 9.9991e-03 (9.9991e-03)\n",
      "Epoch: [17][10/53]\tLoss 1.0003e-02 (9.9971e-03)\n",
      "Epoch: [17][20/53]\tLoss 1.0003e-02 (9.9980e-03)\n",
      "Epoch: [17][30/53]\tLoss 1.0006e-02 (9.9995e-03)\n",
      "Epoch: [17][40/53]\tLoss 9.9997e-03 (1.0000e-02)\n",
      "Epoch: [17][50/53]\tLoss 9.9912e-03 (1.0000e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0006e-02 (1.0004e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0006e-02 (1.0002e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [18][40/53]\tLoss 9.9960e-03 (1.0001e-02)\n",
      "Epoch: [18][50/53]\tLoss 9.9973e-03 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0006e-02 (1.0001e-02)\n",
      "Epoch: [19][40/53]\tLoss 9.9998e-03 (1.0001e-02)\n",
      "Epoch: [19][50/53]\tLoss 9.9956e-03 (1.0000e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 9.9981e-03 (9.9981e-03)\n",
      "Epoch: [20][10/53]\tLoss 9.9944e-03 (1.0003e-02)\n",
      "Epoch: [20][20/53]\tLoss 9.9991e-03 (1.0001e-02)\n",
      "Epoch: [20][30/53]\tLoss 9.9979e-03 (1.0000e-02)\n",
      "Epoch: [20][40/53]\tLoss 9.9983e-03 (1.0001e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 9.9962e-03 (9.9962e-03)\n",
      "Epoch: [21][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0008e-02 (9.9998e-03)\n",
      "Epoch: [21][30/53]\tLoss 9.9928e-03 (9.9996e-03)\n",
      "Epoch: [21][40/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0013e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [22][10/53]\tLoss 9.9999e-03 (9.9999e-03)\n",
      "Epoch: [22][20/53]\tLoss 1.0009e-02 (1.0000e-02)\n",
      "Epoch: [22][30/53]\tLoss 9.9927e-03 (9.9991e-03)\n",
      "Epoch: [22][40/53]\tLoss 1.0002e-02 (9.9994e-03)\n",
      "Epoch: [22][50/53]\tLoss 9.9990e-03 (9.9990e-03)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 9.9995e-03 (9.9995e-03)\n",
      "Epoch: [23][10/53]\tLoss 9.9997e-03 (9.9989e-03)\n",
      "Epoch: [23][20/53]\tLoss 9.9951e-03 (9.9996e-03)\n",
      "Epoch: [23][30/53]\tLoss 9.9783e-03 (9.9988e-03)\n",
      "Epoch: [23][40/53]\tLoss 1.0002e-02 (9.9992e-03)\n",
      "Epoch: [23][50/53]\tLoss 9.9933e-03 (9.9992e-03)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 9.9990e-03 (9.9990e-03)\n",
      "Epoch: [24][10/53]\tLoss 9.9975e-03 (9.9975e-03)\n",
      "Epoch: [24][20/53]\tLoss 1.0002e-02 (9.9975e-03)\n",
      "Epoch: [24][30/53]\tLoss 1.0002e-02 (9.9984e-03)\n",
      "Epoch: [24][40/53]\tLoss 9.9857e-03 (9.9991e-03)\n",
      "Epoch: [24][50/53]\tLoss 9.9985e-03 (9.9991e-03)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 9.9974e-03 (9.9974e-03)\n",
      "Epoch: [25][10/53]\tLoss 1.0005e-02 (1.0000e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0003e-02 (1.0000e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [25][40/53]\tLoss 9.9978e-03 (1.0002e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0002e-02 (1.0000e-02)\n",
      "Epoch: [26][20/53]\tLoss 9.9975e-03 (9.9998e-03)\n",
      "Epoch: [26][30/53]\tLoss 1.0001e-02 (9.9996e-03)\n",
      "Epoch: [26][40/53]\tLoss 1.0011e-02 (1.0000e-02)\n",
      "Epoch: [26][50/53]\tLoss 9.9925e-03 (1.0000e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 9.9955e-03 (9.9955e-03)\n",
      "Epoch: [27][10/53]\tLoss 9.9991e-03 (9.9967e-03)\n",
      "Epoch: [27][20/53]\tLoss 1.0002e-02 (9.9983e-03)\n",
      "Epoch: [27][30/53]\tLoss 1.0007e-02 (9.9973e-03)\n",
      "Epoch: [27][40/53]\tLoss 9.9980e-03 (9.9983e-03)\n",
      "Epoch: [27][50/53]\tLoss 1.0004e-02 (9.9989e-03)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 9.9990e-03 (9.9990e-03)\n",
      "Epoch: [28][10/53]\tLoss 9.9966e-03 (9.9976e-03)\n",
      "Epoch: [28][20/53]\tLoss 1.0002e-02 (9.9984e-03)\n",
      "Epoch: [28][30/53]\tLoss 9.9974e-03 (9.9990e-03)\n",
      "Epoch: [28][40/53]\tLoss 9.9999e-03 (9.9993e-03)\n",
      "Epoch: [28][50/53]\tLoss 9.9974e-03 (9.9994e-03)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [29][10/53]\tLoss 9.9959e-03 (9.9990e-03)\n",
      "Epoch: [29][20/53]\tLoss 1.0001e-02 (9.9994e-03)\n",
      "Epoch: [29][30/53]\tLoss 9.9989e-03 (1.0000e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0005e-02 (1.0000e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0009e-02 (1.0009e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0008e-02 (1.0000e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0003e-02 (9.9990e-03)\n",
      "Epoch: [30][40/53]\tLoss 9.9986e-03 (9.9980e-03)\n",
      "Epoch: [30][50/53]\tLoss 9.9983e-03 (9.9982e-03)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/169]\n",
      "Fill TS Repository [100/169]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries F-2\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5322 - Test samples size: 8426\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/F-2/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/106]\tTotal Loss -2.2999e+00 (-2.2999e+00)\tConsistency Loss 2.2581e+00 (2.2581e+00)\tInconsistency Loss 1.1061e-01 (1.1061e-01)\tEntropy 2.2790e+00 (2.2790e+00)\n",
      "Epoch: [1][100/106]\tTotal Loss -2.3496e+00 (-2.3596e+00)\tConsistency Loss 1.7346e+00 (1.8261e+00)\tInconsistency Loss 1.9911e-01 (1.7995e-01)\tEntropy 2.0421e+00 (2.0929e+00)\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/106]\tTotal Loss -2.3784e+00 (-2.3784e+00)\tConsistency Loss 1.7363e+00 (1.7363e+00)\tInconsistency Loss 1.9613e-01 (1.9613e-01)\tEntropy 2.0573e+00 (2.0573e+00)\n",
      "Epoch: [2][100/106]\tTotal Loss -2.3849e+00 (-2.3751e+00)\tConsistency Loss 1.7300e+00 (1.7256e+00)\tInconsistency Loss 1.9809e-01 (1.9649e-01)\tEntropy 2.0575e+00 (2.0503e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/106]\tTotal Loss -2.3891e+00 (-2.3891e+00)\tConsistency Loss 1.7439e+00 (1.7439e+00)\tInconsistency Loss 1.9050e-01 (1.9050e-01)\tEntropy 2.0665e+00 (2.0665e+00)\n",
      "Epoch: [3][100/106]\tTotal Loss -2.3537e+00 (-2.3755e+00)\tConsistency Loss 1.7399e+00 (1.7251e+00)\tInconsistency Loss 1.9610e-01 (1.9645e-01)\tEntropy 2.0468e+00 (2.0503e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/106]\tTotal Loss -2.3744e+00 (-2.3744e+00)\tConsistency Loss 1.7478e+00 (1.7478e+00)\tInconsistency Loss 1.9276e-01 (1.9276e-01)\tEntropy 2.0611e+00 (2.0611e+00)\n",
      "Epoch: [4][100/106]\tTotal Loss -2.3760e+00 (-2.3769e+00)\tConsistency Loss 1.7325e+00 (1.7186e+00)\tInconsistency Loss 1.9416e-01 (1.9756e-01)\tEntropy 2.0543e+00 (2.0478e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/106]\tTotal Loss -2.3760e+00 (-2.3760e+00)\tConsistency Loss 1.7411e+00 (1.7411e+00)\tInconsistency Loss 1.9322e-01 (1.9322e-01)\tEntropy 2.0586e+00 (2.0586e+00)\n",
      "Epoch: [5][100/106]\tTotal Loss -2.3783e+00 (-2.3768e+00)\tConsistency Loss 1.7189e+00 (1.7196e+00)\tInconsistency Loss 1.9736e-01 (1.9743e-01)\tEntropy 2.0486e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/106]\tTotal Loss -2.3749e+00 (-2.3749e+00)\tConsistency Loss 1.7152e+00 (1.7152e+00)\tInconsistency Loss 1.9857e-01 (1.9857e-01)\tEntropy 2.0450e+00 (2.0450e+00)\n",
      "Epoch: [6][100/106]\tTotal Loss -2.3772e+00 (-2.3770e+00)\tConsistency Loss 1.7178e+00 (1.7188e+00)\tInconsistency Loss 1.9777e-01 (1.9759e-01)\tEntropy 2.0475e+00 (2.0479e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/106]\tTotal Loss -2.3774e+00 (-2.3774e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9775e-01 (1.9775e-01)\tEntropy 2.0478e+00 (2.0478e+00)\n",
      "Epoch: [7][100/106]\tTotal Loss -2.3778e+00 (-2.3772e+00)\tConsistency Loss 1.7165e+00 (1.7182e+00)\tInconsistency Loss 1.9803e-01 (1.9771e-01)\tEntropy 2.0471e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7178e+00 (1.7178e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0475e+00 (2.0475e+00)\n",
      "Epoch: [8][100/106]\tTotal Loss -2.3774e+00 (-2.3771e+00)\tConsistency Loss 1.7181e+00 (1.7182e+00)\tInconsistency Loss 1.9772e-01 (1.9770e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/106]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9770e-01 (1.9770e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [9][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [10][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [11][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [12][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "G-4\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2351/7432 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/G-4/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/47]\tLoss 1.0010e+00 (1.0010e+00)\n",
      "Epoch: [1][10/47]\tLoss 3.4861e-01 (6.2393e-01)\n",
      "Epoch: [1][20/47]\tLoss 1.2149e-01 (4.2413e-01)\n",
      "Epoch: [1][30/47]\tLoss 4.2454e-02 (3.1029e-01)\n",
      "Epoch: [1][40/47]\tLoss 1.5010e-02 (2.4067e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/47]\tLoss 1.0035e-02 (1.0035e-02)\n",
      "Epoch: [2][10/47]\tLoss 1.0003e-02 (1.0012e-02)\n",
      "Epoch: [2][20/47]\tLoss 1.0007e-02 (1.0009e-02)\n",
      "Epoch: [2][30/47]\tLoss 1.0014e-02 (1.0009e-02)\n",
      "Epoch: [2][40/47]\tLoss 9.9996e-03 (1.0007e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [3][10/47]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [3][20/47]\tLoss 1.0000e-02 (1.0004e-02)\n",
      "Epoch: [3][30/47]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [3][40/47]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [4][10/47]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [4][20/47]\tLoss 9.9980e-03 (1.0002e-02)\n",
      "Epoch: [4][30/47]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [4][40/47]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [5][10/47]\tLoss 9.9964e-03 (1.0001e-02)\n",
      "Epoch: [5][20/47]\tLoss 9.9999e-03 (1.0001e-02)\n",
      "Epoch: [5][30/47]\tLoss 1.0013e-02 (1.0002e-02)\n",
      "Epoch: [5][40/47]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/47]\tLoss 9.9993e-03 (9.9993e-03)\n",
      "Epoch: [6][10/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [6][20/47]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [6][30/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [6][40/47]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/47]\tLoss 1.0011e-02 (1.0011e-02)\n",
      "Epoch: [7][10/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [7][20/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [7][30/47]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [7][40/47]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [8][10/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [8][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [8][30/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [8][40/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][10/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [9][20/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [9][30/47]\tLoss 9.9985e-03 (1.0001e-02)\n",
      "Epoch: [9][40/47]\tLoss 9.9990e-03 (1.0001e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/47]\tLoss 9.9999e-03 (9.9999e-03)\n",
      "Epoch: [10][10/47]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [10][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [10][30/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [10][40/47]\tLoss 9.9990e-03 (1.0001e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/47]\tLoss 9.9999e-03 (9.9999e-03)\n",
      "Epoch: [11][10/47]\tLoss 9.9992e-03 (1.0000e-02)\n",
      "Epoch: [11][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][30/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [11][40/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/47]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][30/47]\tLoss 9.9981e-03 (1.0000e-02)\n",
      "Epoch: [12][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][10/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [13][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][30/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [13][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/47]\tLoss 9.9999e-03 (9.9999e-03)\n",
      "Epoch: [14][10/47]\tLoss 1.0000e-02 (9.9999e-03)\n",
      "Epoch: [14][20/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][30/47]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [14][40/47]\tLoss 1.0000e-02 (9.9999e-03)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/47]\tLoss 9.9982e-03 (9.9982e-03)\n",
      "Epoch: [15][10/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [15][20/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][30/47]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [15][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][20/47]\tLoss 9.9985e-03 (1.0000e-02)\n",
      "Epoch: [16][30/47]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [16][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/47]\tLoss 9.9998e-03 (1.0001e-02)\n",
      "Epoch: [17][20/47]\tLoss 1.0004e-02 (1.0000e-02)\n",
      "Epoch: [17][30/47]\tLoss 9.9995e-03 (1.0000e-02)\n",
      "Epoch: [17][40/47]\tLoss 9.9998e-03 (1.0000e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][20/47]\tLoss 9.9994e-03 (9.9998e-03)\n",
      "Epoch: [18][30/47]\tLoss 1.0002e-02 (9.9999e-03)\n",
      "Epoch: [18][40/47]\tLoss 9.9990e-03 (9.9999e-03)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][10/47]\tLoss 1.0003e-02 (9.9995e-03)\n",
      "Epoch: [19][20/47]\tLoss 9.9998e-03 (1.0003e-02)\n",
      "Epoch: [19][30/47]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [19][40/47]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/47]\tLoss 9.9999e-03 (9.9999e-03)\n",
      "Epoch: [20][10/47]\tLoss 9.9999e-03 (1.0000e-02)\n",
      "Epoch: [20][20/47]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [20][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][10/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][20/47]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [21][30/47]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [21][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][10/47]\tLoss 1.0000e-02 (9.9999e-03)\n",
      "Epoch: [22][20/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][30/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][40/47]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/47]\tLoss 9.9995e-03 (1.0000e-02)\n",
      "Epoch: [23][20/47]\tLoss 9.9995e-03 (9.9999e-03)\n",
      "Epoch: [23][30/47]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [23][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/47]\tLoss 9.9996e-03 (9.9996e-03)\n",
      "Epoch: [24][10/47]\tLoss 9.9999e-03 (1.0003e-02)\n",
      "Epoch: [24][20/47]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [24][30/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][40/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/47]\tLoss 1.0000e-02 (9.9997e-03)\n",
      "Epoch: [25][20/47]\tLoss 1.0000e-02 (9.9998e-03)\n",
      "Epoch: [25][30/47]\tLoss 9.9999e-03 (1.0000e-02)\n",
      "Epoch: [25][40/47]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/47]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][10/47]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [26][20/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][30/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/47]\tLoss 1.0000e-02 (9.9999e-03)\n",
      "Epoch: [27][20/47]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [27][30/47]\tLoss 9.9994e-03 (1.0000e-02)\n",
      "Epoch: [27][40/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][10/47]\tLoss 1.0022e-02 (1.0002e-02)\n",
      "Epoch: [28][20/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][30/47]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][40/47]\tLoss 9.9999e-03 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][10/47]\tLoss 1.0000e-02 (9.9997e-03)\n",
      "Epoch: [29][20/47]\tLoss 1.0000e-02 (9.9999e-03)\n",
      "Epoch: [29][30/47]\tLoss 1.0000e-02 (9.9999e-03)\n",
      "Epoch: [29][40/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/47]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/47]\tLoss 1.0000e-02 (9.9997e-03)\n",
      "Epoch: [30][20/47]\tLoss 1.0003e-02 (1.0000e-02)\n",
      "Epoch: [30][30/47]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [30][40/47]\tLoss 9.9999e-03 (1.0000e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/48]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/149]\n",
      "Fill TS Repository [100/149]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries G-4\u001b[0m\n",
      "\u001b[32m-- Train samples size: 4702 - Test samples size: 7432\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/G-4/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][ 0/94]\tTotal Loss -2.2988e+00 (-2.2988e+00)\tConsistency Loss 2.2598e+00 (2.2598e+00)\tInconsistency Loss 1.1040e-01 (1.1040e-01)\tEntropy 2.2793e+00 (2.2793e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][ 0/94]\tTotal Loss -2.4825e+00 (-2.4825e+00)\tConsistency Loss 1.6909e+00 (1.6909e+00)\tInconsistency Loss 1.6492e-01 (1.6492e-01)\tEntropy 2.0867e+00 (2.0867e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][ 0/94]\tTotal Loss -2.4302e+00 (-2.4302e+00)\tConsistency Loss 1.7518e+00 (1.7518e+00)\tInconsistency Loss 1.5792e-01 (1.5792e-01)\tEntropy 2.0910e+00 (2.0910e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][ 0/94]\tTotal Loss -2.3813e+00 (-2.3813e+00)\tConsistency Loss 1.8637e+00 (1.8637e+00)\tInconsistency Loss 1.5155e-01 (1.5155e-01)\tEntropy 2.1225e+00 (2.1225e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][ 0/94]\tTotal Loss -2.5010e+00 (-2.5010e+00)\tConsistency Loss 1.7840e+00 (1.7840e+00)\tInconsistency Loss 1.3771e-01 (1.3771e-01)\tEntropy 2.1425e+00 (2.1425e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][ 0/94]\tTotal Loss -2.4383e+00 (-2.4383e+00)\tConsistency Loss 1.8163e+00 (1.8163e+00)\tInconsistency Loss 1.6674e-01 (1.6674e-01)\tEntropy 2.1273e+00 (2.1273e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][ 0/94]\tTotal Loss -2.4152e+00 (-2.4152e+00)\tConsistency Loss 1.7856e+00 (1.7856e+00)\tInconsistency Loss 1.7179e-01 (1.7179e-01)\tEntropy 2.1004e+00 (2.1004e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][ 0/94]\tTotal Loss -2.3823e+00 (-2.3823e+00)\tConsistency Loss 1.8757e+00 (1.8757e+00)\tInconsistency Loss 1.5309e-01 (1.5309e-01)\tEntropy 2.1290e+00 (2.1290e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][ 0/94]\tTotal Loss -2.4266e+00 (-2.4266e+00)\tConsistency Loss 1.8516e+00 (1.8516e+00)\tInconsistency Loss 1.4345e-01 (1.4345e-01)\tEntropy 2.1391e+00 (2.1391e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][ 0/94]\tTotal Loss -2.4591e+00 (-2.4591e+00)\tConsistency Loss 1.7812e+00 (1.7812e+00)\tInconsistency Loss 1.5077e-01 (1.5077e-01)\tEntropy 2.1201e+00 (2.1201e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][ 0/94]\tTotal Loss -2.4652e+00 (-2.4652e+00)\tConsistency Loss 1.8424e+00 (1.8424e+00)\tInconsistency Loss 1.4495e-01 (1.4495e-01)\tEntropy 2.1538e+00 (2.1538e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][ 0/94]\tTotal Loss -2.5329e+00 (-2.5329e+00)\tConsistency Loss 1.7525e+00 (1.7525e+00)\tInconsistency Loss 1.3738e-01 (1.3738e-01)\tEntropy 2.1427e+00 (2.1427e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][ 0/94]\tTotal Loss -2.4171e+00 (-2.4171e+00)\tConsistency Loss 1.8806e+00 (1.8806e+00)\tInconsistency Loss 1.4066e-01 (1.4066e-01)\tEntropy 2.1489e+00 (2.1489e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][ 0/94]\tTotal Loss -2.4310e+00 (-2.4310e+00)\tConsistency Loss 1.8455e+00 (1.8455e+00)\tInconsistency Loss 1.4955e-01 (1.4955e-01)\tEntropy 2.1383e+00 (2.1383e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][ 0/94]\tTotal Loss -2.4099e+00 (-2.4099e+00)\tConsistency Loss 1.8426e+00 (1.8426e+00)\tInconsistency Loss 1.5428e-01 (1.5428e-01)\tEntropy 2.1263e+00 (2.1263e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][ 0/94]\tTotal Loss -2.4218e+00 (-2.4218e+00)\tConsistency Loss 1.8433e+00 (1.8433e+00)\tInconsistency Loss 1.6418e-01 (1.6418e-01)\tEntropy 2.1326e+00 (2.1326e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][ 0/94]\tTotal Loss -2.4899e+00 (-2.4899e+00)\tConsistency Loss 1.9298e+00 (1.9298e+00)\tInconsistency Loss 1.1776e-01 (1.1776e-01)\tEntropy 2.2098e+00 (2.2098e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][ 0/94]\tTotal Loss -2.4008e+00 (-2.4008e+00)\tConsistency Loss 1.8628e+00 (1.8628e+00)\tInconsistency Loss 1.5229e-01 (1.5229e-01)\tEntropy 2.1318e+00 (2.1318e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][ 0/94]\tTotal Loss -2.3877e+00 (-2.3877e+00)\tConsistency Loss 1.8342e+00 (1.8342e+00)\tInconsistency Loss 1.6560e-01 (1.6560e-01)\tEntropy 2.1109e+00 (2.1109e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][ 0/94]\tTotal Loss -2.4967e+00 (-2.4967e+00)\tConsistency Loss 1.7522e+00 (1.7522e+00)\tInconsistency Loss 1.5757e-01 (1.5757e-01)\tEntropy 2.1244e+00 (2.1244e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][ 0/94]\tTotal Loss -2.4153e+00 (-2.4153e+00)\tConsistency Loss 1.7494e+00 (1.7494e+00)\tInconsistency Loss 1.8218e-01 (1.8218e-01)\tEntropy 2.0823e+00 (2.0823e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][ 0/94]\tTotal Loss -2.4615e+00 (-2.4615e+00)\tConsistency Loss 1.7790e+00 (1.7790e+00)\tInconsistency Loss 1.6479e-01 (1.6479e-01)\tEntropy 2.1202e+00 (2.1202e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][ 0/94]\tTotal Loss -2.4333e+00 (-2.4333e+00)\tConsistency Loss 1.8066e+00 (1.8066e+00)\tInconsistency Loss 1.6487e-01 (1.6487e-01)\tEntropy 2.1200e+00 (2.1200e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][ 0/94]\tTotal Loss -2.3999e+00 (-2.3999e+00)\tConsistency Loss 1.7906e+00 (1.7906e+00)\tInconsistency Loss 1.7372e-01 (1.7372e-01)\tEntropy 2.0953e+00 (2.0953e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][ 0/94]\tTotal Loss -2.4846e+00 (-2.4846e+00)\tConsistency Loss 1.7799e+00 (1.7799e+00)\tInconsistency Loss 1.6037e-01 (1.6037e-01)\tEntropy 2.1322e+00 (2.1322e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][ 0/94]\tTotal Loss -2.4770e+00 (-2.4770e+00)\tConsistency Loss 1.8330e+00 (1.8330e+00)\tInconsistency Loss 1.3632e-01 (1.3632e-01)\tEntropy 2.1550e+00 (2.1550e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][ 0/94]\tTotal Loss -2.4279e+00 (-2.4279e+00)\tConsistency Loss 1.7478e+00 (1.7478e+00)\tInconsistency Loss 1.5791e-01 (1.5791e-01)\tEntropy 2.0878e+00 (2.0878e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][ 0/94]\tTotal Loss -2.4425e+00 (-2.4425e+00)\tConsistency Loss 1.8266e+00 (1.8266e+00)\tInconsistency Loss 1.5414e-01 (1.5414e-01)\tEntropy 2.1345e+00 (2.1345e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][ 0/94]\tTotal Loss -2.4767e+00 (-2.4767e+00)\tConsistency Loss 1.8057e+00 (1.8057e+00)\tInconsistency Loss 1.4976e-01 (1.4976e-01)\tEntropy 2.1412e+00 (2.1412e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][ 0/94]\tTotal Loss -2.5035e+00 (-2.5035e+00)\tConsistency Loss 1.7114e+00 (1.7114e+00)\tInconsistency Loss 1.5903e-01 (1.5903e-01)\tEntropy 2.1074e+00 (2.1074e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][ 0/94]\tTotal Loss -2.4402e+00 (-2.4402e+00)\tConsistency Loss 1.7576e+00 (1.7576e+00)\tInconsistency Loss 1.5877e-01 (1.5877e-01)\tEntropy 2.0989e+00 (2.0989e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][ 0/94]\tTotal Loss -2.4653e+00 (-2.4653e+00)\tConsistency Loss 1.8052e+00 (1.8052e+00)\tInconsistency Loss 1.4980e-01 (1.4980e-01)\tEntropy 2.1352e+00 (2.1352e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][ 0/94]\tTotal Loss -2.4694e+00 (-2.4694e+00)\tConsistency Loss 1.7735e+00 (1.7735e+00)\tInconsistency Loss 1.7227e-01 (1.7227e-01)\tEntropy 2.1215e+00 (2.1215e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][ 0/94]\tTotal Loss -2.4928e+00 (-2.4928e+00)\tConsistency Loss 1.7517e+00 (1.7517e+00)\tInconsistency Loss 1.6389e-01 (1.6389e-01)\tEntropy 2.1223e+00 (2.1223e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][ 0/94]\tTotal Loss -2.5738e+00 (-2.5738e+00)\tConsistency Loss 1.7468e+00 (1.7468e+00)\tInconsistency Loss 1.3530e-01 (1.3530e-01)\tEntropy 2.1603e+00 (2.1603e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][ 0/94]\tTotal Loss -2.4499e+00 (-2.4499e+00)\tConsistency Loss 1.8012e+00 (1.8012e+00)\tInconsistency Loss 1.5102e-01 (1.5102e-01)\tEntropy 2.1256e+00 (2.1256e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][ 0/94]\tTotal Loss -2.4358e+00 (-2.4358e+00)\tConsistency Loss 1.7841e+00 (1.7841e+00)\tInconsistency Loss 1.5505e-01 (1.5505e-01)\tEntropy 2.1100e+00 (2.1100e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][ 0/94]\tTotal Loss -2.5384e+00 (-2.5384e+00)\tConsistency Loss 1.7256e+00 (1.7256e+00)\tInconsistency Loss 1.4735e-01 (1.4735e-01)\tEntropy 2.1320e+00 (2.1320e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][ 0/94]\tTotal Loss -2.4583e+00 (-2.4583e+00)\tConsistency Loss 1.7521e+00 (1.7521e+00)\tInconsistency Loss 1.6545e-01 (1.6545e-01)\tEntropy 2.1052e+00 (2.1052e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][ 0/94]\tTotal Loss -2.5204e+00 (-2.5204e+00)\tConsistency Loss 1.8162e+00 (1.8162e+00)\tInconsistency Loss 1.3363e-01 (1.3363e-01)\tEntropy 2.1683e+00 (2.1683e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][ 0/94]\tTotal Loss -2.4331e+00 (-2.4331e+00)\tConsistency Loss 1.8492e+00 (1.8492e+00)\tInconsistency Loss 1.4393e-01 (1.4393e-01)\tEntropy 2.1411e+00 (2.1411e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][ 0/94]\tTotal Loss -2.4849e+00 (-2.4849e+00)\tConsistency Loss 1.7676e+00 (1.7676e+00)\tInconsistency Loss 1.5399e-01 (1.5399e-01)\tEntropy 2.1263e+00 (2.1263e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][ 0/94]\tTotal Loss -2.4584e+00 (-2.4584e+00)\tConsistency Loss 1.7469e+00 (1.7469e+00)\tInconsistency Loss 1.6190e-01 (1.6190e-01)\tEntropy 2.1027e+00 (2.1027e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][ 0/94]\tTotal Loss -2.5152e+00 (-2.5152e+00)\tConsistency Loss 1.7794e+00 (1.7794e+00)\tInconsistency Loss 1.4968e-01 (1.4968e-01)\tEntropy 2.1473e+00 (2.1473e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][ 0/94]\tTotal Loss -2.4735e+00 (-2.4735e+00)\tConsistency Loss 1.8227e+00 (1.8227e+00)\tInconsistency Loss 1.4575e-01 (1.4575e-01)\tEntropy 2.1481e+00 (2.1481e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][ 0/94]\tTotal Loss -2.4533e+00 (-2.4533e+00)\tConsistency Loss 1.7244e+00 (1.7244e+00)\tInconsistency Loss 1.6385e-01 (1.6385e-01)\tEntropy 2.0889e+00 (2.0889e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][ 0/94]\tTotal Loss -2.5283e+00 (-2.5283e+00)\tConsistency Loss 1.7210e+00 (1.7210e+00)\tInconsistency Loss 1.4802e-01 (1.4802e-01)\tEntropy 2.1247e+00 (2.1247e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][ 0/94]\tTotal Loss -2.4913e+00 (-2.4913e+00)\tConsistency Loss 1.8101e+00 (1.8101e+00)\tInconsistency Loss 1.3843e-01 (1.3843e-01)\tEntropy 2.1507e+00 (2.1507e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][ 0/94]\tTotal Loss -2.4868e+00 (-2.4868e+00)\tConsistency Loss 1.8221e+00 (1.8221e+00)\tInconsistency Loss 1.5637e-01 (1.5637e-01)\tEntropy 2.1545e+00 (2.1545e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][ 0/94]\tTotal Loss -2.3693e+00 (-2.3693e+00)\tConsistency Loss 1.8321e+00 (1.8321e+00)\tInconsistency Loss 1.5249e-01 (1.5249e-01)\tEntropy 2.1007e+00 (2.1007e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "T-3\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2676/8379 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/T-3/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0003e+00 (1.0003e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4872e-01 (6.2402e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2292e-01 (4.2424e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2270e-02 (3.1032e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4732e-02 (2.4067e-01)\n",
      "Epoch: [1][50/53]\tLoss 9.9947e-03 (1.9556e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0006e-02 (1.0011e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0015e-02 (1.0011e-02)\n",
      "Epoch: [2][30/53]\tLoss 9.9362e-03 (1.0008e-02)\n",
      "Epoch: [2][40/53]\tLoss 9.9907e-03 (1.0009e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0005e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0013e-02 (1.0013e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0009e-02 (1.0009e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0041e-02 (1.0007e-02)\n",
      "Epoch: [3][30/53]\tLoss 9.9982e-03 (1.0007e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0004e-02 (1.0008e-02)\n",
      "Epoch: [3][50/53]\tLoss 9.9956e-03 (1.0007e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0000e-02 (1.0003e-02)\n",
      "Epoch: [4][20/53]\tLoss 9.9989e-03 (1.0003e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0007e-02 (1.0003e-02)\n",
      "Epoch: [4][50/53]\tLoss 9.9895e-03 (1.0002e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0006e-02 (1.0001e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [6][20/53]\tLoss 9.9980e-03 (1.0000e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [6][40/53]\tLoss 9.9918e-03 (9.9999e-03)\n",
      "Epoch: [6][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0008e-02 (1.0008e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0003e-02 (9.9977e-03)\n",
      "Epoch: [7][30/53]\tLoss 1.0012e-02 (9.9956e-03)\n",
      "Epoch: [7][40/53]\tLoss 1.0003e-02 (9.9987e-03)\n",
      "Epoch: [7][50/53]\tLoss 1.0005e-02 (9.9995e-03)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 9.9940e-03 (9.9940e-03)\n",
      "Epoch: [8][10/53]\tLoss 1.0051e-02 (1.0003e-02)\n",
      "Epoch: [8][20/53]\tLoss 9.9878e-03 (1.0001e-02)\n",
      "Epoch: [8][30/53]\tLoss 9.9911e-03 (1.0001e-02)\n",
      "Epoch: [8][40/53]\tLoss 9.9927e-03 (9.9999e-03)\n",
      "Epoch: [8][50/53]\tLoss 1.0036e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 9.9943e-03 (9.9943e-03)\n",
      "Epoch: [9][10/53]\tLoss 9.9776e-03 (9.9913e-03)\n",
      "Epoch: [9][20/53]\tLoss 1.0002e-02 (9.9930e-03)\n",
      "Epoch: [9][30/53]\tLoss 9.9624e-03 (9.9938e-03)\n",
      "Epoch: [9][40/53]\tLoss 1.0020e-02 (9.9968e-03)\n",
      "Epoch: [9][50/53]\tLoss 1.0008e-02 (9.9967e-03)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0005e-02 (9.9984e-03)\n",
      "Epoch: [10][20/53]\tLoss 9.9765e-03 (9.9951e-03)\n",
      "Epoch: [10][30/53]\tLoss 9.9966e-03 (9.9964e-03)\n",
      "Epoch: [10][40/53]\tLoss 1.0003e-02 (9.9980e-03)\n",
      "Epoch: [10][50/53]\tLoss 9.9918e-03 (9.9934e-03)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0010e-02 (9.9799e-03)\n",
      "Epoch: [11][20/53]\tLoss 1.0025e-02 (9.9929e-03)\n",
      "Epoch: [11][30/53]\tLoss 1.0038e-02 (9.9869e-03)\n",
      "Epoch: [11][40/53]\tLoss 9.8372e-03 (9.9746e-03)\n",
      "Epoch: [11][50/53]\tLoss 1.0131e-02 (9.9793e-03)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0112e-02 (1.0112e-02)\n",
      "Epoch: [12][10/53]\tLoss 9.9919e-03 (9.9905e-03)\n",
      "Epoch: [12][20/53]\tLoss 1.0056e-02 (9.9917e-03)\n",
      "Epoch: [12][30/53]\tLoss 9.5958e-03 (9.9724e-03)\n",
      "Epoch: [12][40/53]\tLoss 1.0046e-02 (9.9768e-03)\n",
      "Epoch: [12][50/53]\tLoss 9.7985e-03 (9.9718e-03)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0077e-02 (1.0077e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0095e-02 (9.9868e-03)\n",
      "Epoch: [13][20/53]\tLoss 1.0034e-02 (9.9783e-03)\n",
      "Epoch: [13][30/53]\tLoss 9.7952e-03 (9.9614e-03)\n",
      "Epoch: [13][40/53]\tLoss 1.0008e-02 (9.9530e-03)\n",
      "Epoch: [13][50/53]\tLoss 9.9548e-03 (9.9408e-03)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0078e-02 (1.0078e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0015e-02 (9.9390e-03)\n",
      "Epoch: [14][20/53]\tLoss 9.8284e-03 (9.8998e-03)\n",
      "Epoch: [14][30/53]\tLoss 9.5762e-03 (9.8702e-03)\n",
      "Epoch: [14][40/53]\tLoss 1.0040e-02 (9.9275e-03)\n",
      "Epoch: [14][50/53]\tLoss 1.0146e-02 (9.9658e-03)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 9.6661e-03 (9.6661e-03)\n",
      "Epoch: [15][10/53]\tLoss 9.9768e-03 (9.9284e-03)\n",
      "Epoch: [15][20/53]\tLoss 9.8062e-03 (9.8842e-03)\n",
      "Epoch: [15][30/53]\tLoss 9.7732e-03 (9.8889e-03)\n",
      "Epoch: [15][40/53]\tLoss 9.5823e-03 (9.8757e-03)\n",
      "Epoch: [15][50/53]\tLoss 9.4446e-03 (9.8867e-03)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0071e-02 (1.0071e-02)\n",
      "Epoch: [16][10/53]\tLoss 9.7053e-03 (9.8433e-03)\n",
      "Epoch: [16][20/53]\tLoss 9.9837e-03 (9.9100e-03)\n",
      "Epoch: [16][30/53]\tLoss 9.8644e-03 (9.8961e-03)\n",
      "Epoch: [16][40/53]\tLoss 9.5117e-03 (9.8915e-03)\n",
      "Epoch: [16][50/53]\tLoss 9.6471e-03 (9.8885e-03)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 9.8996e-03 (9.8996e-03)\n",
      "Epoch: [17][10/53]\tLoss 9.7468e-03 (9.9482e-03)\n",
      "Epoch: [17][20/53]\tLoss 9.9811e-03 (9.9276e-03)\n",
      "Epoch: [17][30/53]\tLoss 9.2228e-03 (9.8723e-03)\n",
      "Epoch: [17][40/53]\tLoss 9.7542e-03 (9.8545e-03)\n",
      "Epoch: [17][50/53]\tLoss 9.9484e-03 (9.8658e-03)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 9.9183e-03 (9.9183e-03)\n",
      "Epoch: [18][10/53]\tLoss 9.8622e-03 (9.9101e-03)\n",
      "Epoch: [18][20/53]\tLoss 9.7586e-03 (9.8578e-03)\n",
      "Epoch: [18][30/53]\tLoss 9.8009e-03 (9.8147e-03)\n",
      "Epoch: [18][40/53]\tLoss 1.0124e-02 (9.8478e-03)\n",
      "Epoch: [18][50/53]\tLoss 9.6287e-03 (9.8356e-03)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 9.8631e-03 (9.8631e-03)\n",
      "Epoch: [19][10/53]\tLoss 9.9608e-03 (9.8890e-03)\n",
      "Epoch: [19][20/53]\tLoss 9.7709e-03 (9.8455e-03)\n",
      "Epoch: [19][30/53]\tLoss 1.0062e-02 (9.8349e-03)\n",
      "Epoch: [19][40/53]\tLoss 1.0160e-02 (9.8162e-03)\n",
      "Epoch: [19][50/53]\tLoss 9.2942e-03 (9.8286e-03)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 9.8768e-03 (9.8768e-03)\n",
      "Epoch: [20][10/53]\tLoss 1.0007e-02 (9.7053e-03)\n",
      "Epoch: [20][20/53]\tLoss 1.0265e-02 (9.7838e-03)\n",
      "Epoch: [20][30/53]\tLoss 9.9874e-03 (9.7812e-03)\n",
      "Epoch: [20][40/53]\tLoss 9.6915e-03 (9.7946e-03)\n",
      "Epoch: [20][50/53]\tLoss 1.0049e-02 (9.8149e-03)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 9.6875e-03 (9.6875e-03)\n",
      "Epoch: [21][10/53]\tLoss 1.0009e-02 (9.8220e-03)\n",
      "Epoch: [21][20/53]\tLoss 1.0076e-02 (9.8055e-03)\n",
      "Epoch: [21][30/53]\tLoss 9.7995e-03 (9.7898e-03)\n",
      "Epoch: [21][40/53]\tLoss 9.9073e-03 (9.7906e-03)\n",
      "Epoch: [21][50/53]\tLoss 9.7770e-03 (9.7763e-03)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 8.7780e-03 (8.7780e-03)\n",
      "Epoch: [22][10/53]\tLoss 9.4076e-03 (9.6328e-03)\n",
      "Epoch: [22][20/53]\tLoss 8.9111e-03 (9.5955e-03)\n",
      "Epoch: [22][30/53]\tLoss 9.5295e-03 (9.6250e-03)\n",
      "Epoch: [22][40/53]\tLoss 9.8240e-03 (9.6891e-03)\n",
      "Epoch: [22][50/53]\tLoss 9.7934e-03 (9.6920e-03)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 9.2399e-03 (9.2399e-03)\n",
      "Epoch: [23][10/53]\tLoss 9.8966e-03 (9.7305e-03)\n",
      "Epoch: [23][20/53]\tLoss 9.8441e-03 (9.7086e-03)\n",
      "Epoch: [23][30/53]\tLoss 1.0093e-02 (9.7681e-03)\n",
      "Epoch: [23][40/53]\tLoss 9.9014e-03 (9.7614e-03)\n",
      "Epoch: [23][50/53]\tLoss 9.2080e-03 (9.7320e-03)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 9.4174e-03 (9.4174e-03)\n",
      "Epoch: [24][10/53]\tLoss 9.1212e-03 (9.5521e-03)\n",
      "Epoch: [24][20/53]\tLoss 9.2616e-03 (9.5980e-03)\n",
      "Epoch: [24][30/53]\tLoss 9.4994e-03 (9.6431e-03)\n",
      "Epoch: [24][40/53]\tLoss 9.6546e-03 (9.6448e-03)\n",
      "Epoch: [24][50/53]\tLoss 9.9176e-03 (9.6992e-03)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0033e-02 (1.0033e-02)\n",
      "Epoch: [25][10/53]\tLoss 9.5810e-03 (9.7449e-03)\n",
      "Epoch: [25][20/53]\tLoss 9.5151e-03 (9.7110e-03)\n",
      "Epoch: [25][30/53]\tLoss 9.7222e-03 (9.7259e-03)\n",
      "Epoch: [25][40/53]\tLoss 1.0064e-02 (9.7237e-03)\n",
      "Epoch: [25][50/53]\tLoss 9.6980e-03 (9.7448e-03)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 9.6514e-03 (9.6514e-03)\n",
      "Epoch: [26][10/53]\tLoss 9.2937e-03 (9.6386e-03)\n",
      "Epoch: [26][20/53]\tLoss 9.6563e-03 (9.6988e-03)\n",
      "Epoch: [26][30/53]\tLoss 9.6805e-03 (9.6599e-03)\n",
      "Epoch: [26][40/53]\tLoss 9.3965e-03 (9.6499e-03)\n",
      "Epoch: [26][50/53]\tLoss 1.0120e-02 (9.7101e-03)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0163e-02 (1.0163e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0013e-02 (9.7978e-03)\n",
      "Epoch: [27][20/53]\tLoss 9.7618e-03 (9.8077e-03)\n",
      "Epoch: [27][30/53]\tLoss 9.9899e-03 (9.7523e-03)\n",
      "Epoch: [27][40/53]\tLoss 9.7153e-03 (9.7299e-03)\n",
      "Epoch: [27][50/53]\tLoss 1.0294e-02 (9.7301e-03)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 9.8650e-03 (9.8650e-03)\n",
      "Epoch: [28][10/53]\tLoss 9.6962e-03 (9.7937e-03)\n",
      "Epoch: [28][20/53]\tLoss 9.1975e-03 (9.6940e-03)\n",
      "Epoch: [28][30/53]\tLoss 9.0819e-03 (9.6967e-03)\n",
      "Epoch: [28][40/53]\tLoss 9.5693e-03 (9.7001e-03)\n",
      "Epoch: [28][50/53]\tLoss 9.7602e-03 (9.6992e-03)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0050e-02 (1.0050e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0013e-02 (9.6833e-03)\n",
      "Epoch: [29][20/53]\tLoss 9.7174e-03 (9.6736e-03)\n",
      "Epoch: [29][30/53]\tLoss 9.3773e-03 (9.7113e-03)\n",
      "Epoch: [29][40/53]\tLoss 9.8622e-03 (9.6860e-03)\n",
      "Epoch: [29][50/53]\tLoss 9.7634e-03 (9.7176e-03)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 9.4298e-03 (9.4298e-03)\n",
      "Epoch: [30][10/53]\tLoss 9.3759e-03 (9.7160e-03)\n",
      "Epoch: [30][20/53]\tLoss 9.6240e-03 (9.7546e-03)\n",
      "Epoch: [30][30/53]\tLoss 9.6963e-03 (9.7196e-03)\n",
      "Epoch: [30][40/53]\tLoss 1.0014e-02 (9.7269e-03)\n",
      "Epoch: [30][50/53]\tLoss 9.4815e-03 (9.7099e-03)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/168]\n",
      "Fill TS Repository [100/168]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries T-3\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5352 - Test samples size: 8379\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/T-3/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/107]\tTotal Loss -2.3059e+00 (-2.3059e+00)\tConsistency Loss 2.2556e+00 (2.2556e+00)\tInconsistency Loss 1.0967e-01 (1.0967e-01)\tEntropy 2.2808e+00 (2.2808e+00)\n",
      "Epoch: [1][100/107]\tTotal Loss -2.4294e+00 (-2.4031e+00)\tConsistency Loss 1.9009e+00 (2.0238e+00)\tInconsistency Loss 1.3491e-01 (1.2750e-01)\tEntropy 2.1651e+00 (2.2135e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/107]\tTotal Loss -2.4916e+00 (-2.4916e+00)\tConsistency Loss 1.8193e+00 (1.8193e+00)\tInconsistency Loss 1.3549e-01 (1.3549e-01)\tEntropy 2.1554e+00 (2.1554e+00)\n",
      "Epoch: [2][100/107]\tTotal Loss -2.4778e+00 (-2.4262e+00)\tConsistency Loss 1.7771e+00 (1.9012e+00)\tInconsistency Loss 1.5392e-01 (1.4340e-01)\tEntropy 2.1275e+00 (2.1637e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/107]\tTotal Loss -2.3583e+00 (-2.3583e+00)\tConsistency Loss 1.9770e+00 (1.9770e+00)\tInconsistency Loss 1.4578e-01 (1.4578e-01)\tEntropy 2.1677e+00 (2.1677e+00)\n",
      "Epoch: [3][100/107]\tTotal Loss -2.4404e+00 (-2.4395e+00)\tConsistency Loss 1.8526e+00 (1.8501e+00)\tInconsistency Loss 1.5153e-01 (1.5181e-01)\tEntropy 2.1465e+00 (2.1448e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/107]\tTotal Loss -2.4150e+00 (-2.4150e+00)\tConsistency Loss 1.8427e+00 (1.8427e+00)\tInconsistency Loss 1.5074e-01 (1.5074e-01)\tEntropy 2.1288e+00 (2.1288e+00)\n",
      "Epoch: [4][100/107]\tTotal Loss -2.4285e+00 (-2.4502e+00)\tConsistency Loss 1.8352e+00 (1.8068e+00)\tInconsistency Loss 1.5725e-01 (1.5852e-01)\tEntropy 2.1319e+00 (2.1285e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/107]\tTotal Loss -2.4461e+00 (-2.4461e+00)\tConsistency Loss 1.8087e+00 (1.8087e+00)\tInconsistency Loss 1.6436e-01 (1.6436e-01)\tEntropy 2.1274e+00 (2.1274e+00)\n",
      "Epoch: [5][100/107]\tTotal Loss -2.3809e+00 (-2.4436e+00)\tConsistency Loss 1.8728e+00 (1.8047e+00)\tInconsistency Loss 1.6007e-01 (1.6095e-01)\tEntropy 2.1268e+00 (2.1241e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/107]\tTotal Loss -2.4759e+00 (-2.4759e+00)\tConsistency Loss 1.7597e+00 (1.7597e+00)\tInconsistency Loss 1.5946e-01 (1.5946e-01)\tEntropy 2.1178e+00 (2.1178e+00)\n",
      "Epoch: [6][100/107]\tTotal Loss -2.4430e+00 (-2.4463e+00)\tConsistency Loss 1.8044e+00 (1.8101e+00)\tInconsistency Loss 1.5896e-01 (1.5781e-01)\tEntropy 2.1237e+00 (2.1282e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/107]\tTotal Loss -2.5203e+00 (-2.5203e+00)\tConsistency Loss 1.7883e+00 (1.7883e+00)\tInconsistency Loss 1.3635e-01 (1.3635e-01)\tEntropy 2.1543e+00 (2.1543e+00)\n",
      "Epoch: [7][100/107]\tTotal Loss -2.4924e+00 (-2.4490e+00)\tConsistency Loss 1.7679e+00 (1.7950e+00)\tInconsistency Loss 1.5960e-01 (1.5997e-01)\tEntropy 2.1302e+00 (2.1220e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/107]\tTotal Loss -2.5053e+00 (-2.5053e+00)\tConsistency Loss 1.7954e+00 (1.7954e+00)\tInconsistency Loss 1.5320e-01 (1.5320e-01)\tEntropy 2.1504e+00 (2.1504e+00)\n",
      "Epoch: [8][100/107]\tTotal Loss -2.4758e+00 (-2.4522e+00)\tConsistency Loss 1.7586e+00 (1.7958e+00)\tInconsistency Loss 1.7628e-01 (1.5898e-01)\tEntropy 2.1172e+00 (2.1240e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/107]\tTotal Loss -2.3828e+00 (-2.3828e+00)\tConsistency Loss 1.8458e+00 (1.8458e+00)\tInconsistency Loss 1.6508e-01 (1.6508e-01)\tEntropy 2.1143e+00 (2.1143e+00)\n",
      "Epoch: [9][100/107]\tTotal Loss -2.4463e+00 (-2.4492e+00)\tConsistency Loss 1.7823e+00 (1.7940e+00)\tInconsistency Loss 1.6667e-01 (1.6101e-01)\tEntropy 2.1143e+00 (2.1216e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/107]\tTotal Loss -2.3429e+00 (-2.3429e+00)\tConsistency Loss 1.9467e+00 (1.9467e+00)\tInconsistency Loss 1.5034e-01 (1.5034e-01)\tEntropy 2.1448e+00 (2.1448e+00)\n",
      "Epoch: [10][100/107]\tTotal Loss -2.5325e+00 (-2.4520e+00)\tConsistency Loss 1.7050e+00 (1.7946e+00)\tInconsistency Loss 1.6560e-01 (1.5986e-01)\tEntropy 2.1188e+00 (2.1233e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/107]\tTotal Loss -2.4517e+00 (-2.4517e+00)\tConsistency Loss 1.7895e+00 (1.7895e+00)\tInconsistency Loss 1.5870e-01 (1.5870e-01)\tEntropy 2.1206e+00 (2.1206e+00)\n",
      "Epoch: [11][100/107]\tTotal Loss -2.4785e+00 (-2.4502e+00)\tConsistency Loss 1.7388e+00 (1.7843e+00)\tInconsistency Loss 1.6229e-01 (1.6206e-01)\tEntropy 2.1087e+00 (2.1173e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/107]\tTotal Loss -2.3578e+00 (-2.3578e+00)\tConsistency Loss 1.9482e+00 (1.9482e+00)\tInconsistency Loss 1.5385e-01 (1.5385e-01)\tEntropy 2.1530e+00 (2.1530e+00)\n",
      "Epoch: [12][100/107]\tTotal Loss -2.4477e+00 (-2.4543e+00)\tConsistency Loss 1.7527e+00 (1.7864e+00)\tInconsistency Loss 1.6516e-01 (1.6101e-01)\tEntropy 2.1002e+00 (2.1204e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/107]\tTotal Loss -2.4566e+00 (-2.4566e+00)\tConsistency Loss 1.8485e+00 (1.8485e+00)\tInconsistency Loss 1.4073e-01 (1.4073e-01)\tEntropy 2.1525e+00 (2.1525e+00)\n",
      "Epoch: [13][100/107]\tTotal Loss -2.4637e+00 (-2.4576e+00)\tConsistency Loss 1.8310e+00 (1.7904e+00)\tInconsistency Loss 1.5621e-01 (1.5984e-01)\tEntropy 2.1473e+00 (2.1240e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/107]\tTotal Loss -2.4487e+00 (-2.4487e+00)\tConsistency Loss 1.7036e+00 (1.7036e+00)\tInconsistency Loss 1.8268e-01 (1.8268e-01)\tEntropy 2.0761e+00 (2.0761e+00)\n",
      "Epoch: [14][100/107]\tTotal Loss -2.4399e+00 (-2.4565e+00)\tConsistency Loss 1.7912e+00 (1.7861e+00)\tInconsistency Loss 1.6426e-01 (1.6082e-01)\tEntropy 2.1156e+00 (2.1213e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/107]\tTotal Loss -2.4614e+00 (-2.4614e+00)\tConsistency Loss 1.7938e+00 (1.7938e+00)\tInconsistency Loss 1.6557e-01 (1.6557e-01)\tEntropy 2.1276e+00 (2.1276e+00)\n",
      "Epoch: [15][100/107]\tTotal Loss -2.5372e+00 (-2.4533e+00)\tConsistency Loss 1.7710e+00 (1.7870e+00)\tInconsistency Loss 1.4068e-01 (1.6174e-01)\tEntropy 2.1541e+00 (2.1202e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/107]\tTotal Loss -2.4859e+00 (-2.4859e+00)\tConsistency Loss 1.7421e+00 (1.7421e+00)\tInconsistency Loss 1.6007e-01 (1.6007e-01)\tEntropy 2.1140e+00 (2.1140e+00)\n",
      "Epoch: [16][100/107]\tTotal Loss -2.4617e+00 (-2.4580e+00)\tConsistency Loss 1.7814e+00 (1.7783e+00)\tInconsistency Loss 1.6540e-01 (1.6260e-01)\tEntropy 2.1216e+00 (2.1181e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/107]\tTotal Loss -2.5369e+00 (-2.5369e+00)\tConsistency Loss 1.7222e+00 (1.7222e+00)\tInconsistency Loss 1.6202e-01 (1.6202e-01)\tEntropy 2.1296e+00 (2.1296e+00)\n",
      "Epoch: [17][100/107]\tTotal Loss -2.4729e+00 (-2.4516e+00)\tConsistency Loss 1.7674e+00 (1.7909e+00)\tInconsistency Loss 1.6795e-01 (1.6044e-01)\tEntropy 2.1202e+00 (2.1213e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/107]\tTotal Loss -2.4593e+00 (-2.4593e+00)\tConsistency Loss 1.7807e+00 (1.7807e+00)\tInconsistency Loss 1.5901e-01 (1.5901e-01)\tEntropy 2.1200e+00 (2.1200e+00)\n",
      "Epoch: [18][100/107]\tTotal Loss -2.4942e+00 (-2.4496e+00)\tConsistency Loss 1.7319e+00 (1.8002e+00)\tInconsistency Loss 1.6162e-01 (1.5892e-01)\tEntropy 2.1131e+00 (2.1249e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/107]\tTotal Loss -2.4997e+00 (-2.4997e+00)\tConsistency Loss 1.7880e+00 (1.7880e+00)\tInconsistency Loss 1.4511e-01 (1.4511e-01)\tEntropy 2.1438e+00 (2.1438e+00)\n",
      "Epoch: [19][100/107]\tTotal Loss -2.4979e+00 (-2.4541e+00)\tConsistency Loss 1.6533e+00 (1.7877e+00)\tInconsistency Loss 1.5884e-01 (1.6126e-01)\tEntropy 2.0756e+00 (2.1209e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/107]\tTotal Loss -2.4835e+00 (-2.4835e+00)\tConsistency Loss 1.7693e+00 (1.7693e+00)\tInconsistency Loss 1.6353e-01 (1.6353e-01)\tEntropy 2.1264e+00 (2.1264e+00)\n",
      "Epoch: [20][100/107]\tTotal Loss -2.4540e+00 (-2.4557e+00)\tConsistency Loss 1.7756e+00 (1.7931e+00)\tInconsistency Loss 1.5834e-01 (1.5924e-01)\tEntropy 2.1148e+00 (2.1244e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/107]\tTotal Loss -2.4653e+00 (-2.4653e+00)\tConsistency Loss 1.7165e+00 (1.7165e+00)\tInconsistency Loss 1.6619e-01 (1.6619e-01)\tEntropy 2.0909e+00 (2.0909e+00)\n",
      "Epoch: [21][100/107]\tTotal Loss -2.5392e+00 (-2.4557e+00)\tConsistency Loss 1.6997e+00 (1.7820e+00)\tInconsistency Loss 1.4617e-01 (1.6220e-01)\tEntropy 2.1194e+00 (2.1188e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/107]\tTotal Loss -2.5665e+00 (-2.5665e+00)\tConsistency Loss 1.7258e+00 (1.7258e+00)\tInconsistency Loss 1.6667e-01 (1.6667e-01)\tEntropy 2.1462e+00 (2.1462e+00)\n",
      "Epoch: [22][100/107]\tTotal Loss -2.3166e+00 (-2.4521e+00)\tConsistency Loss 1.9539e+00 (1.7892e+00)\tInconsistency Loss 1.6176e-01 (1.6117e-01)\tEntropy 2.1353e+00 (2.1207e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/107]\tTotal Loss -2.4776e+00 (-2.4776e+00)\tConsistency Loss 1.8202e+00 (1.8202e+00)\tInconsistency Loss 1.5219e-01 (1.5219e-01)\tEntropy 2.1489e+00 (2.1489e+00)\n",
      "Epoch: [23][100/107]\tTotal Loss -2.4486e+00 (-2.4512e+00)\tConsistency Loss 1.7941e+00 (1.7852e+00)\tInconsistency Loss 1.6411e-01 (1.6221e-01)\tEntropy 2.1214e+00 (2.1182e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/107]\tTotal Loss -2.5102e+00 (-2.5102e+00)\tConsistency Loss 1.7656e+00 (1.7656e+00)\tInconsistency Loss 1.6088e-01 (1.6088e-01)\tEntropy 2.1379e+00 (2.1379e+00)\n",
      "Epoch: [24][100/107]\tTotal Loss -2.3990e+00 (-2.4572e+00)\tConsistency Loss 1.9376e+00 (1.7867e+00)\tInconsistency Loss 1.4398e-01 (1.5999e-01)\tEntropy 2.1683e+00 (2.1220e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/107]\tTotal Loss -2.3913e+00 (-2.3913e+00)\tConsistency Loss 1.8497e+00 (1.8497e+00)\tInconsistency Loss 1.7129e-01 (1.7129e-01)\tEntropy 2.1205e+00 (2.1205e+00)\n",
      "Epoch: [25][100/107]\tTotal Loss -2.5300e+00 (-2.4532e+00)\tConsistency Loss 1.7577e+00 (1.7914e+00)\tInconsistency Loss 1.4394e-01 (1.6086e-01)\tEntropy 2.1438e+00 (2.1223e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/107]\tTotal Loss -2.4151e+00 (-2.4151e+00)\tConsistency Loss 1.8724e+00 (1.8724e+00)\tInconsistency Loss 1.6768e-01 (1.6768e-01)\tEntropy 2.1437e+00 (2.1437e+00)\n",
      "Epoch: [26][100/107]\tTotal Loss -2.4382e+00 (-2.4553e+00)\tConsistency Loss 1.8156e+00 (1.7914e+00)\tInconsistency Loss 1.5875e-01 (1.6045e-01)\tEntropy 2.1269e+00 (2.1234e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/107]\tTotal Loss -2.3929e+00 (-2.3929e+00)\tConsistency Loss 1.7401e+00 (1.7401e+00)\tInconsistency Loss 1.7918e-01 (1.7918e-01)\tEntropy 2.0665e+00 (2.0665e+00)\n",
      "Epoch: [27][100/107]\tTotal Loss -2.3989e+00 (-2.4538e+00)\tConsistency Loss 1.8456e+00 (1.7929e+00)\tInconsistency Loss 1.5437e-01 (1.6059e-01)\tEntropy 2.1222e+00 (2.1233e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/107]\tTotal Loss -2.4948e+00 (-2.4948e+00)\tConsistency Loss 1.7268e+00 (1.7268e+00)\tInconsistency Loss 1.7326e-01 (1.7326e-01)\tEntropy 2.1108e+00 (2.1108e+00)\n",
      "Epoch: [28][100/107]\tTotal Loss -2.5102e+00 (-2.4533e+00)\tConsistency Loss 1.7448e+00 (1.7928e+00)\tInconsistency Loss 1.4861e-01 (1.6034e-01)\tEntropy 2.1275e+00 (2.1230e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/107]\tTotal Loss -2.5166e+00 (-2.5166e+00)\tConsistency Loss 1.7536e+00 (1.7536e+00)\tInconsistency Loss 1.6086e-01 (1.6086e-01)\tEntropy 2.1351e+00 (2.1351e+00)\n",
      "Epoch: [29][100/107]\tTotal Loss -2.4911e+00 (-2.4577e+00)\tConsistency Loss 1.6629e+00 (1.7835e+00)\tInconsistency Loss 1.8043e-01 (1.6119e-01)\tEntropy 2.0770e+00 (2.1206e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/107]\tTotal Loss -2.3692e+00 (-2.3692e+00)\tConsistency Loss 1.8567e+00 (1.8567e+00)\tInconsistency Loss 1.6317e-01 (1.6317e-01)\tEntropy 2.1129e+00 (2.1129e+00)\n",
      "Epoch: [30][100/107]\tTotal Loss -2.5359e+00 (-2.4592e+00)\tConsistency Loss 1.8118e+00 (1.7749e+00)\tInconsistency Loss 1.3144e-01 (1.6333e-01)\tEntropy 2.1738e+00 (2.1170e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/107]\tTotal Loss -2.4734e+00 (-2.4734e+00)\tConsistency Loss 1.6878e+00 (1.6878e+00)\tInconsistency Loss 1.6967e-01 (1.6967e-01)\tEntropy 2.0806e+00 (2.0806e+00)\n",
      "Epoch: [31][100/107]\tTotal Loss -2.5723e+00 (-2.4554e+00)\tConsistency Loss 1.6718e+00 (1.7871e+00)\tInconsistency Loss 1.5056e-01 (1.6089e-01)\tEntropy 2.1221e+00 (2.1213e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/107]\tTotal Loss -2.4844e+00 (-2.4844e+00)\tConsistency Loss 1.7569e+00 (1.7569e+00)\tInconsistency Loss 1.4706e-01 (1.4706e-01)\tEntropy 2.1206e+00 (2.1206e+00)\n",
      "Epoch: [32][100/107]\tTotal Loss -2.4667e+00 (-2.4564e+00)\tConsistency Loss 1.7532e+00 (1.7892e+00)\tInconsistency Loss 1.4303e-01 (1.5935e-01)\tEntropy 2.1100e+00 (2.1228e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/107]\tTotal Loss -2.5088e+00 (-2.5088e+00)\tConsistency Loss 1.7290e+00 (1.7290e+00)\tInconsistency Loss 1.6798e-01 (1.6798e-01)\tEntropy 2.1189e+00 (2.1189e+00)\n",
      "Epoch: [33][100/107]\tTotal Loss -2.4549e+00 (-2.4577e+00)\tConsistency Loss 1.7435e+00 (1.7816e+00)\tInconsistency Loss 1.8108e-01 (1.6133e-01)\tEntropy 2.0992e+00 (2.1197e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/107]\tTotal Loss -2.5176e+00 (-2.5176e+00)\tConsistency Loss 1.7555e+00 (1.7555e+00)\tInconsistency Loss 1.6391e-01 (1.6391e-01)\tEntropy 2.1365e+00 (2.1365e+00)\n",
      "Epoch: [34][100/107]\tTotal Loss -2.5505e+00 (-2.4536e+00)\tConsistency Loss 1.7181e+00 (1.7906e+00)\tInconsistency Loss 1.5109e-01 (1.6053e-01)\tEntropy 2.1343e+00 (2.1221e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/107]\tTotal Loss -2.4244e+00 (-2.4244e+00)\tConsistency Loss 1.8011e+00 (1.8011e+00)\tInconsistency Loss 1.5946e-01 (1.5946e-01)\tEntropy 2.1128e+00 (2.1128e+00)\n",
      "Epoch: [35][100/107]\tTotal Loss -2.4980e+00 (-2.4565e+00)\tConsistency Loss 1.7886e+00 (1.7870e+00)\tInconsistency Loss 1.5031e-01 (1.6050e-01)\tEntropy 2.1433e+00 (2.1217e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/107]\tTotal Loss -2.4626e+00 (-2.4626e+00)\tConsistency Loss 1.7954e+00 (1.7954e+00)\tInconsistency Loss 1.5216e-01 (1.5216e-01)\tEntropy 2.1290e+00 (2.1290e+00)\n",
      "Epoch: [36][100/107]\tTotal Loss -2.4766e+00 (-2.4563e+00)\tConsistency Loss 1.7788e+00 (1.7849e+00)\tInconsistency Loss 1.4942e-01 (1.6062e-01)\tEntropy 2.1277e+00 (2.1206e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/107]\tTotal Loss -2.4587e+00 (-2.4587e+00)\tConsistency Loss 1.7829e+00 (1.7829e+00)\tInconsistency Loss 1.6288e-01 (1.6288e-01)\tEntropy 2.1208e+00 (2.1208e+00)\n",
      "Epoch: [37][100/107]\tTotal Loss -2.3942e+00 (-2.4619e+00)\tConsistency Loss 1.8360e+00 (1.7808e+00)\tInconsistency Loss 1.7416e-01 (1.6085e-01)\tEntropy 2.1151e+00 (2.1213e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/107]\tTotal Loss -2.4492e+00 (-2.4492e+00)\tConsistency Loss 1.7451e+00 (1.7451e+00)\tInconsistency Loss 1.6597e-01 (1.6597e-01)\tEntropy 2.0972e+00 (2.0972e+00)\n",
      "Epoch: [38][100/107]\tTotal Loss -2.5182e+00 (-2.4544e+00)\tConsistency Loss 1.7257e+00 (1.7823e+00)\tInconsistency Loss 1.6928e-01 (1.6222e-01)\tEntropy 2.1220e+00 (2.1183e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/107]\tTotal Loss -2.4769e+00 (-2.4769e+00)\tConsistency Loss 1.7777e+00 (1.7777e+00)\tInconsistency Loss 1.6853e-01 (1.6853e-01)\tEntropy 2.1273e+00 (2.1273e+00)\n",
      "Epoch: [39][100/107]\tTotal Loss -2.4332e+00 (-2.4528e+00)\tConsistency Loss 1.8061e+00 (1.7904e+00)\tInconsistency Loss 1.6699e-01 (1.6051e-01)\tEntropy 2.1196e+00 (2.1216e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/107]\tTotal Loss -2.5303e+00 (-2.5303e+00)\tConsistency Loss 1.7211e+00 (1.7211e+00)\tInconsistency Loss 1.6306e-01 (1.6306e-01)\tEntropy 2.1257e+00 (2.1257e+00)\n",
      "Epoch: [40][100/107]\tTotal Loss -2.5501e+00 (-2.4527e+00)\tConsistency Loss 1.7427e+00 (1.7912e+00)\tInconsistency Loss 1.4877e-01 (1.6135e-01)\tEntropy 2.1464e+00 (2.1220e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/107]\tTotal Loss -2.5586e+00 (-2.5586e+00)\tConsistency Loss 1.7603e+00 (1.7603e+00)\tInconsistency Loss 1.4142e-01 (1.4142e-01)\tEntropy 2.1594e+00 (2.1594e+00)\n",
      "Epoch: [41][100/107]\tTotal Loss -2.4903e+00 (-2.4547e+00)\tConsistency Loss 1.8030e+00 (1.7876e+00)\tInconsistency Loss 1.4824e-01 (1.6104e-01)\tEntropy 2.1467e+00 (2.1212e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/107]\tTotal Loss -2.4796e+00 (-2.4796e+00)\tConsistency Loss 1.8004e+00 (1.8004e+00)\tInconsistency Loss 1.5255e-01 (1.5255e-01)\tEntropy 2.1400e+00 (2.1400e+00)\n",
      "Epoch: [42][100/107]\tTotal Loss -2.4122e+00 (-2.4526e+00)\tConsistency Loss 1.8368e+00 (1.7950e+00)\tInconsistency Loss 1.6617e-01 (1.5979e-01)\tEntropy 2.1245e+00 (2.1238e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/107]\tTotal Loss -2.5564e+00 (-2.5564e+00)\tConsistency Loss 1.6438e+00 (1.6438e+00)\tInconsistency Loss 1.7121e-01 (1.7121e-01)\tEntropy 2.1001e+00 (2.1001e+00)\n",
      "Epoch: [43][100/107]\tTotal Loss -2.5018e+00 (-2.4604e+00)\tConsistency Loss 1.7654e+00 (1.7794e+00)\tInconsistency Loss 1.4649e-01 (1.6161e-01)\tEntropy 2.1336e+00 (2.1199e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/107]\tTotal Loss -2.4376e+00 (-2.4376e+00)\tConsistency Loss 1.6872e+00 (1.6872e+00)\tInconsistency Loss 1.7565e-01 (1.7565e-01)\tEntropy 2.0624e+00 (2.0624e+00)\n",
      "Epoch: [44][100/107]\tTotal Loss -2.5029e+00 (-2.4525e+00)\tConsistency Loss 1.7914e+00 (1.7795e+00)\tInconsistency Loss 1.6235e-01 (1.6334e-01)\tEntropy 2.1471e+00 (2.1160e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/107]\tTotal Loss -2.4264e+00 (-2.4264e+00)\tConsistency Loss 1.8948e+00 (1.8948e+00)\tInconsistency Loss 1.3820e-01 (1.3820e-01)\tEntropy 2.1606e+00 (2.1606e+00)\n",
      "Epoch: [45][100/107]\tTotal Loss -2.3966e+00 (-2.4512e+00)\tConsistency Loss 1.8708e+00 (1.7957e+00)\tInconsistency Loss 1.6636e-01 (1.5988e-01)\tEntropy 2.1337e+00 (2.1234e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/107]\tTotal Loss -2.4572e+00 (-2.4572e+00)\tConsistency Loss 1.8210e+00 (1.8210e+00)\tInconsistency Loss 1.3436e-01 (1.3436e-01)\tEntropy 2.1391e+00 (2.1391e+00)\n",
      "Epoch: [46][100/107]\tTotal Loss -2.4432e+00 (-2.4567e+00)\tConsistency Loss 1.7807e+00 (1.7838e+00)\tInconsistency Loss 1.7088e-01 (1.6166e-01)\tEntropy 2.1119e+00 (2.1202e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/107]\tTotal Loss -2.3729e+00 (-2.3729e+00)\tConsistency Loss 1.8247e+00 (1.8247e+00)\tInconsistency Loss 1.7428e-01 (1.7428e-01)\tEntropy 2.0988e+00 (2.0988e+00)\n",
      "Epoch: [47][100/107]\tTotal Loss -2.3746e+00 (-2.4557e+00)\tConsistency Loss 1.8388e+00 (1.7784e+00)\tInconsistency Loss 1.8299e-01 (1.6256e-01)\tEntropy 2.1067e+00 (2.1171e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/107]\tTotal Loss -2.4189e+00 (-2.4189e+00)\tConsistency Loss 1.7849e+00 (1.7849e+00)\tInconsistency Loss 1.7067e-01 (1.7067e-01)\tEntropy 2.1019e+00 (2.1019e+00)\n",
      "Epoch: [48][100/107]\tTotal Loss -2.5005e+00 (-2.4594e+00)\tConsistency Loss 1.7831e+00 (1.7793e+00)\tInconsistency Loss 1.6019e-01 (1.6086e-01)\tEntropy 2.1418e+00 (2.1193e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/107]\tTotal Loss -2.4143e+00 (-2.4143e+00)\tConsistency Loss 1.7558e+00 (1.7558e+00)\tInconsistency Loss 1.6983e-01 (1.6983e-01)\tEntropy 2.0850e+00 (2.0850e+00)\n",
      "Epoch: [49][100/107]\tTotal Loss -2.2583e+00 (-2.4525e+00)\tConsistency Loss 2.0175e+00 (1.7894e+00)\tInconsistency Loss 1.7296e-01 (1.6078e-01)\tEntropy 2.1379e+00 (2.1209e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/107]\tTotal Loss -2.5170e+00 (-2.5170e+00)\tConsistency Loss 1.7363e+00 (1.7363e+00)\tInconsistency Loss 1.5596e-01 (1.5596e-01)\tEntropy 2.1266e+00 (2.1266e+00)\n",
      "Epoch: [50][100/107]\tTotal Loss -2.3464e+00 (-2.4559e+00)\tConsistency Loss 1.8793e+00 (1.7834e+00)\tInconsistency Loss 1.7512e-01 (1.6091e-01)\tEntropy 2.1129e+00 (2.1197e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "D-11\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2411/7231 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/D-11/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/48]\tLoss 1.0042e+00 (1.0042e+00)\n",
      "Epoch: [1][10/48]\tLoss 3.4854e-01 (6.2441e-01)\n",
      "Epoch: [1][20/48]\tLoss 1.2153e-01 (4.2425e-01)\n",
      "Epoch: [1][30/48]\tLoss 4.2321e-02 (3.1033e-01)\n",
      "Epoch: [1][40/48]\tLoss 1.4732e-02 (2.4068e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/48]\tLoss 1.0034e-02 (1.0034e-02)\n",
      "Epoch: [2][10/48]\tLoss 1.0024e-02 (1.0031e-02)\n",
      "Epoch: [2][20/48]\tLoss 1.0031e-02 (1.0030e-02)\n",
      "Epoch: [2][30/48]\tLoss 1.0014e-02 (1.0030e-02)\n",
      "Epoch: [2][40/48]\tLoss 1.0037e-02 (1.0030e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/48]\tLoss 1.0013e-02 (1.0013e-02)\n",
      "Epoch: [3][10/48]\tLoss 1.0020e-02 (1.0028e-02)\n",
      "Epoch: [3][20/48]\tLoss 1.0014e-02 (1.0022e-02)\n",
      "Epoch: [3][30/48]\tLoss 1.0003e-02 (1.0022e-02)\n",
      "Epoch: [3][40/48]\tLoss 1.0024e-02 (1.0020e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/48]\tLoss 1.0011e-02 (1.0011e-02)\n",
      "Epoch: [4][10/48]\tLoss 1.0012e-02 (1.0010e-02)\n",
      "Epoch: [4][20/48]\tLoss 1.0004e-02 (1.0009e-02)\n",
      "Epoch: [4][30/48]\tLoss 1.0006e-02 (1.0012e-02)\n",
      "Epoch: [4][40/48]\tLoss 1.0030e-02 (1.0011e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [5][10/48]\tLoss 1.0007e-02 (1.0011e-02)\n",
      "Epoch: [5][20/48]\tLoss 1.0011e-02 (1.0009e-02)\n",
      "Epoch: [5][30/48]\tLoss 1.0015e-02 (1.0011e-02)\n",
      "Epoch: [5][40/48]\tLoss 9.9710e-03 (1.0010e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/48]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][10/48]\tLoss 1.0006e-02 (1.0005e-02)\n",
      "Epoch: [6][20/48]\tLoss 1.0007e-02 (1.0010e-02)\n",
      "Epoch: [6][30/48]\tLoss 1.0010e-02 (1.0009e-02)\n",
      "Epoch: [6][40/48]\tLoss 1.0002e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/48]\tLoss 1.0016e-02 (1.0016e-02)\n",
      "Epoch: [7][10/48]\tLoss 1.0014e-02 (1.0008e-02)\n",
      "Epoch: [7][20/48]\tLoss 1.0001e-02 (1.0007e-02)\n",
      "Epoch: [7][30/48]\tLoss 1.0000e-02 (1.0007e-02)\n",
      "Epoch: [7][40/48]\tLoss 1.0014e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/48]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [8][10/48]\tLoss 1.0002e-02 (1.0005e-02)\n",
      "Epoch: [8][20/48]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [8][30/48]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [8][40/48]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [9][10/48]\tLoss 1.0002e-02 (1.0011e-02)\n",
      "Epoch: [9][20/48]\tLoss 1.0004e-02 (1.0009e-02)\n",
      "Epoch: [9][30/48]\tLoss 1.0019e-02 (1.0008e-02)\n",
      "Epoch: [9][40/48]\tLoss 1.0010e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/48]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [10][10/48]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [10][20/48]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [10][30/48]\tLoss 1.0008e-02 (1.0004e-02)\n",
      "Epoch: [10][40/48]\tLoss 9.9983e-03 (1.0004e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/48]\tLoss 1.0012e-02 (1.0012e-02)\n",
      "Epoch: [11][10/48]\tLoss 1.0000e-02 (1.0005e-02)\n",
      "Epoch: [11][20/48]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [11][30/48]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [11][40/48]\tLoss 1.0005e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/48]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [12][10/48]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [12][20/48]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [12][30/48]\tLoss 9.9996e-03 (1.0003e-02)\n",
      "Epoch: [12][40/48]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][10/48]\tLoss 1.0007e-02 (1.0002e-02)\n",
      "Epoch: [13][20/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][30/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][40/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [14][10/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [14][20/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [14][30/48]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [14][40/48]\tLoss 9.9997e-03 (1.0002e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [15][10/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [15][20/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [15][30/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [15][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][30/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [16][40/48]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/48]\tLoss 9.9994e-03 (1.0002e-02)\n",
      "Epoch: [17][20/48]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [17][30/48]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [17][40/48]\tLoss 1.0005e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/48]\tLoss 9.9982e-03 (9.9982e-03)\n",
      "Epoch: [18][10/48]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [18][20/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [18][30/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [18][40/48]\tLoss 1.0006e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [19][10/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [19][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/48]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [19][40/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][10/48]\tLoss 1.0006e-02 (1.0002e-02)\n",
      "Epoch: [20][20/48]\tLoss 9.9988e-03 (1.0001e-02)\n",
      "Epoch: [20][30/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [20][40/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/48]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [21][20/48]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [21][30/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [21][40/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/48]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [22][20/48]\tLoss 9.9984e-03 (1.0000e-02)\n",
      "Epoch: [22][30/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][40/48]\tLoss 9.9993e-03 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][10/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][20/48]\tLoss 9.9988e-03 (1.0001e-02)\n",
      "Epoch: [23][30/48]\tLoss 9.9988e-03 (1.0001e-02)\n",
      "Epoch: [23][40/48]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/48]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [24][10/48]\tLoss 9.9997e-03 (1.0001e-02)\n",
      "Epoch: [24][20/48]\tLoss 9.9947e-03 (1.0001e-02)\n",
      "Epoch: [24][30/48]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [24][40/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][10/48]\tLoss 9.9986e-03 (1.0001e-02)\n",
      "Epoch: [25][20/48]\tLoss 1.0015e-02 (1.0002e-02)\n",
      "Epoch: [25][30/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][40/48]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][10/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][20/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][30/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][40/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/48]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [27][10/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][20/48]\tLoss 9.9981e-03 (1.0001e-02)\n",
      "Epoch: [27][30/48]\tLoss 9.9981e-03 (1.0001e-02)\n",
      "Epoch: [27][40/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][10/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][20/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][30/48]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [28][40/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/48]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [29][10/48]\tLoss 9.9982e-03 (1.0001e-02)\n",
      "Epoch: [29][20/48]\tLoss 9.9995e-03 (1.0001e-02)\n",
      "Epoch: [29][30/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][40/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][10/48]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][20/48]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][30/48]\tLoss 9.9997e-03 (1.0000e-02)\n",
      "Epoch: [30][40/48]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/49]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/145]\n",
      "Fill TS Repository [100/145]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries D-11\u001b[0m\n",
      "\u001b[32m-- Train samples size: 4822 - Test samples size: 7231\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/D-11/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][ 0/96]\tTotal Loss -2.3011e+00 (-2.3011e+00)\tConsistency Loss 2.2572e+00 (2.2572e+00)\tInconsistency Loss 1.1053e-01 (1.1053e-01)\tEntropy 2.2792e+00 (2.2792e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][ 0/96]\tTotal Loss -2.3694e+00 (-2.3694e+00)\tConsistency Loss 1.7537e+00 (1.7537e+00)\tInconsistency Loss 1.9066e-01 (1.9066e-01)\tEntropy 2.0616e+00 (2.0616e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][ 0/96]\tTotal Loss -2.3750e+00 (-2.3750e+00)\tConsistency Loss 1.7156e+00 (1.7156e+00)\tInconsistency Loss 1.9806e-01 (1.9806e-01)\tEntropy 2.0453e+00 (2.0453e+00)\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][ 0/96]\tTotal Loss -2.3782e+00 (-2.3782e+00)\tConsistency Loss 1.7193e+00 (1.7193e+00)\tInconsistency Loss 1.9722e-01 (1.9722e-01)\tEntropy 2.0487e+00 (2.0487e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][ 0/96]\tTotal Loss -2.3778e+00 (-2.3778e+00)\tConsistency Loss 1.7075e+00 (1.7075e+00)\tInconsistency Loss 1.9974e-01 (1.9974e-01)\tEntropy 2.0426e+00 (2.0426e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][ 0/96]\tTotal Loss -2.5005e+00 (-2.5005e+00)\tConsistency Loss 1.6947e+00 (1.6947e+00)\tInconsistency Loss 1.7820e-01 (1.7820e-01)\tEntropy 2.0976e+00 (2.0976e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][ 0/96]\tTotal Loss -2.3673e+00 (-2.3673e+00)\tConsistency Loss 1.9163e+00 (1.9163e+00)\tInconsistency Loss 1.6708e-01 (1.6708e-01)\tEntropy 2.1418e+00 (2.1418e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][ 0/96]\tTotal Loss -2.5368e+00 (-2.5368e+00)\tConsistency Loss 1.7767e+00 (1.7767e+00)\tInconsistency Loss 1.8245e-01 (1.8245e-01)\tEntropy 2.1567e+00 (2.1567e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][ 0/96]\tTotal Loss -2.5541e+00 (-2.5541e+00)\tConsistency Loss 1.7062e+00 (1.7062e+00)\tInconsistency Loss 1.7587e-01 (1.7587e-01)\tEntropy 2.1302e+00 (2.1302e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][ 0/96]\tTotal Loss -2.4217e+00 (-2.4217e+00)\tConsistency Loss 1.8295e+00 (1.8295e+00)\tInconsistency Loss 1.7709e-01 (1.7709e-01)\tEntropy 2.1256e+00 (2.1256e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][ 0/96]\tTotal Loss -2.4913e+00 (-2.4913e+00)\tConsistency Loss 1.7133e+00 (1.7133e+00)\tInconsistency Loss 1.8048e-01 (1.8048e-01)\tEntropy 2.1023e+00 (2.1023e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][ 0/96]\tTotal Loss -2.5006e+00 (-2.5006e+00)\tConsistency Loss 1.7172e+00 (1.7172e+00)\tInconsistency Loss 1.9456e-01 (1.9456e-01)\tEntropy 2.1089e+00 (2.1089e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][ 0/96]\tTotal Loss -2.5875e+00 (-2.5875e+00)\tConsistency Loss 1.6507e+00 (1.6507e+00)\tInconsistency Loss 2.0011e-01 (2.0011e-01)\tEntropy 2.1191e+00 (2.1191e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][ 0/96]\tTotal Loss -2.4285e+00 (-2.4285e+00)\tConsistency Loss 1.9008e+00 (1.9008e+00)\tInconsistency Loss 1.7378e-01 (1.7378e-01)\tEntropy 2.1646e+00 (2.1646e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][ 0/96]\tTotal Loss -2.5000e+00 (-2.5000e+00)\tConsistency Loss 1.6139e+00 (1.6139e+00)\tInconsistency Loss 1.8997e-01 (1.8997e-01)\tEntropy 2.0570e+00 (2.0570e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][ 0/96]\tTotal Loss -2.4868e+00 (-2.4868e+00)\tConsistency Loss 1.6483e+00 (1.6483e+00)\tInconsistency Loss 1.9641e-01 (1.9641e-01)\tEntropy 2.0676e+00 (2.0676e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][ 0/96]\tTotal Loss -2.4965e+00 (-2.4965e+00)\tConsistency Loss 1.7082e+00 (1.7082e+00)\tInconsistency Loss 1.8967e-01 (1.8967e-01)\tEntropy 2.1023e+00 (2.1023e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][ 0/96]\tTotal Loss -2.4178e+00 (-2.4178e+00)\tConsistency Loss 1.8087e+00 (1.8087e+00)\tInconsistency Loss 1.9759e-01 (1.9759e-01)\tEntropy 2.1132e+00 (2.1132e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][ 0/96]\tTotal Loss -2.5121e+00 (-2.5121e+00)\tConsistency Loss 1.6812e+00 (1.6812e+00)\tInconsistency Loss 1.8734e-01 (1.8734e-01)\tEntropy 2.0967e+00 (2.0967e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][ 0/96]\tTotal Loss -2.4133e+00 (-2.4133e+00)\tConsistency Loss 1.7354e+00 (1.7354e+00)\tInconsistency Loss 1.8286e-01 (1.8286e-01)\tEntropy 2.0744e+00 (2.0744e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][ 0/96]\tTotal Loss -2.5189e+00 (-2.5189e+00)\tConsistency Loss 1.7516e+00 (1.7516e+00)\tInconsistency Loss 1.7716e-01 (1.7716e-01)\tEntropy 2.1352e+00 (2.1352e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][ 0/96]\tTotal Loss -2.5106e+00 (-2.5106e+00)\tConsistency Loss 1.7575e+00 (1.7575e+00)\tInconsistency Loss 1.6531e-01 (1.6531e-01)\tEntropy 2.1341e+00 (2.1341e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][ 0/96]\tTotal Loss -2.5809e+00 (-2.5809e+00)\tConsistency Loss 1.6901e+00 (1.6901e+00)\tInconsistency Loss 1.8176e-01 (1.8176e-01)\tEntropy 2.1355e+00 (2.1355e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][ 0/96]\tTotal Loss -2.4350e+00 (-2.4350e+00)\tConsistency Loss 1.8593e+00 (1.8593e+00)\tInconsistency Loss 1.5506e-01 (1.5506e-01)\tEntropy 2.1472e+00 (2.1472e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][ 0/96]\tTotal Loss -2.5747e+00 (-2.5747e+00)\tConsistency Loss 1.6615e+00 (1.6615e+00)\tInconsistency Loss 1.7617e-01 (1.7617e-01)\tEntropy 2.1181e+00 (2.1181e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][ 0/96]\tTotal Loss -2.5937e+00 (-2.5937e+00)\tConsistency Loss 1.6778e+00 (1.6778e+00)\tInconsistency Loss 1.7975e-01 (1.7975e-01)\tEntropy 2.1358e+00 (2.1358e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][ 0/96]\tTotal Loss -2.4037e+00 (-2.4037e+00)\tConsistency Loss 1.7823e+00 (1.7823e+00)\tInconsistency Loss 1.8128e-01 (1.8128e-01)\tEntropy 2.0930e+00 (2.0930e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][ 0/96]\tTotal Loss -2.4367e+00 (-2.4367e+00)\tConsistency Loss 1.7788e+00 (1.7788e+00)\tInconsistency Loss 1.8698e-01 (1.8698e-01)\tEntropy 2.1077e+00 (2.1077e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][ 0/96]\tTotal Loss -2.6019e+00 (-2.6019e+00)\tConsistency Loss 1.6792e+00 (1.6792e+00)\tInconsistency Loss 1.7575e-01 (1.7575e-01)\tEntropy 2.1405e+00 (2.1405e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][ 0/96]\tTotal Loss -2.5412e+00 (-2.5412e+00)\tConsistency Loss 1.6708e+00 (1.6708e+00)\tInconsistency Loss 1.8909e-01 (1.8909e-01)\tEntropy 2.1060e+00 (2.1060e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][ 0/96]\tTotal Loss -2.4808e+00 (-2.4808e+00)\tConsistency Loss 1.7362e+00 (1.7362e+00)\tInconsistency Loss 2.0288e-01 (2.0288e-01)\tEntropy 2.1085e+00 (2.1085e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][ 0/96]\tTotal Loss -2.4221e+00 (-2.4221e+00)\tConsistency Loss 1.8849e+00 (1.8849e+00)\tInconsistency Loss 1.6502e-01 (1.6502e-01)\tEntropy 2.1535e+00 (2.1535e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][ 0/96]\tTotal Loss -2.5237e+00 (-2.5237e+00)\tConsistency Loss 1.7755e+00 (1.7755e+00)\tInconsistency Loss 1.6971e-01 (1.6971e-01)\tEntropy 2.1496e+00 (2.1496e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][ 0/96]\tTotal Loss -2.5330e+00 (-2.5330e+00)\tConsistency Loss 1.7322e+00 (1.7322e+00)\tInconsistency Loss 1.5877e-01 (1.5877e-01)\tEntropy 2.1326e+00 (2.1326e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][ 0/96]\tTotal Loss -2.6068e+00 (-2.6068e+00)\tConsistency Loss 1.6113e+00 (1.6113e+00)\tInconsistency Loss 1.9802e-01 (1.9802e-01)\tEntropy 2.1090e+00 (2.1090e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][ 0/96]\tTotal Loss -2.5295e+00 (-2.5295e+00)\tConsistency Loss 1.6474e+00 (1.6474e+00)\tInconsistency Loss 1.7790e-01 (1.7790e-01)\tEntropy 2.0884e+00 (2.0884e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][ 0/96]\tTotal Loss -2.5629e+00 (-2.5629e+00)\tConsistency Loss 1.6983e+00 (1.6983e+00)\tInconsistency Loss 1.7850e-01 (1.7850e-01)\tEntropy 2.1306e+00 (2.1306e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][ 0/96]\tTotal Loss -2.5207e+00 (-2.5207e+00)\tConsistency Loss 1.6546e+00 (1.6546e+00)\tInconsistency Loss 2.0318e-01 (2.0318e-01)\tEntropy 2.0877e+00 (2.0877e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][ 0/96]\tTotal Loss -2.4501e+00 (-2.4501e+00)\tConsistency Loss 1.7892e+00 (1.7892e+00)\tInconsistency Loss 1.9256e-01 (1.9256e-01)\tEntropy 2.1197e+00 (2.1197e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][ 0/96]\tTotal Loss -2.3715e+00 (-2.3715e+00)\tConsistency Loss 1.8253e+00 (1.8253e+00)\tInconsistency Loss 1.8871e-01 (1.8871e-01)\tEntropy 2.0984e+00 (2.0984e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][ 0/96]\tTotal Loss -2.5322e+00 (-2.5322e+00)\tConsistency Loss 1.7445e+00 (1.7445e+00)\tInconsistency Loss 1.8422e-01 (1.8422e-01)\tEntropy 2.1383e+00 (2.1383e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][ 0/96]\tTotal Loss -2.3600e+00 (-2.3600e+00)\tConsistency Loss 1.8484e+00 (1.8484e+00)\tInconsistency Loss 1.7112e-01 (1.7112e-01)\tEntropy 2.1042e+00 (2.1042e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][ 0/96]\tTotal Loss -2.5091e+00 (-2.5091e+00)\tConsistency Loss 1.7537e+00 (1.7537e+00)\tInconsistency Loss 1.4096e-01 (1.4096e-01)\tEntropy 2.1314e+00 (2.1314e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][ 0/96]\tTotal Loss -2.4748e+00 (-2.4748e+00)\tConsistency Loss 1.7444e+00 (1.7444e+00)\tInconsistency Loss 1.7921e-01 (1.7921e-01)\tEntropy 2.1096e+00 (2.1096e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][ 0/96]\tTotal Loss -2.5614e+00 (-2.5614e+00)\tConsistency Loss 1.7172e+00 (1.7172e+00)\tInconsistency Loss 1.5073e-01 (1.5073e-01)\tEntropy 2.1393e+00 (2.1393e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][ 0/96]\tTotal Loss -2.5077e+00 (-2.5077e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.7473e-01 (1.7473e-01)\tEntropy 2.1129e+00 (2.1129e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][ 0/96]\tTotal Loss -2.5514e+00 (-2.5514e+00)\tConsistency Loss 1.6447e+00 (1.6447e+00)\tInconsistency Loss 1.8999e-01 (1.8999e-01)\tEntropy 2.0980e+00 (2.0980e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][ 0/96]\tTotal Loss -2.5075e+00 (-2.5075e+00)\tConsistency Loss 1.7779e+00 (1.7779e+00)\tInconsistency Loss 1.6081e-01 (1.6081e-01)\tEntropy 2.1427e+00 (2.1427e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][ 0/96]\tTotal Loss -2.4942e+00 (-2.4942e+00)\tConsistency Loss 1.6925e+00 (1.6925e+00)\tInconsistency Loss 1.8468e-01 (1.8468e-01)\tEntropy 2.0934e+00 (2.0934e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][ 0/96]\tTotal Loss -2.5323e+00 (-2.5323e+00)\tConsistency Loss 1.6530e+00 (1.6530e+00)\tInconsistency Loss 1.8291e-01 (1.8291e-01)\tEntropy 2.0926e+00 (2.0926e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "D-12\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 112/7718 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/D-12/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][0/2]\tLoss 1.0022e+00 (1.0022e+00)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][0/2]\tLoss 8.9987e-01 (8.9987e-01)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][0/2]\tLoss 8.0983e-01 (8.0983e-01)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][0/2]\tLoss 7.2887e-01 (7.2887e-01)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][0/2]\tLoss 6.5596e-01 (6.5596e-01)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][0/2]\tLoss 5.9035e-01 (5.9035e-01)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][0/2]\tLoss 5.3130e-01 (5.3130e-01)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][0/2]\tLoss 4.7816e-01 (4.7816e-01)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][0/2]\tLoss 4.3037e-01 (4.3037e-01)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][0/2]\tLoss 3.8734e-01 (3.8734e-01)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][0/2]\tLoss 3.4858e-01 (3.4858e-01)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][0/2]\tLoss 3.1373e-01 (3.1373e-01)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][0/2]\tLoss 2.8235e-01 (2.8235e-01)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][0/2]\tLoss 2.5411e-01 (2.5411e-01)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][0/2]\tLoss 2.2870e-01 (2.2870e-01)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][0/2]\tLoss 2.0583e-01 (2.0583e-01)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][0/2]\tLoss 1.8525e-01 (1.8525e-01)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][0/2]\tLoss 1.6672e-01 (1.6672e-01)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][0/2]\tLoss 1.5005e-01 (1.5005e-01)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][0/2]\tLoss 1.3504e-01 (1.3504e-01)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][0/2]\tLoss 1.2154e-01 (1.2154e-01)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][0/2]\tLoss 1.0939e-01 (1.0939e-01)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][0/2]\tLoss 9.8447e-02 (9.8447e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][0/2]\tLoss 8.8602e-02 (8.8602e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][0/2]\tLoss 7.9743e-02 (7.9743e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][0/2]\tLoss 7.1767e-02 (7.1767e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][0/2]\tLoss 6.4595e-02 (6.4595e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][0/2]\tLoss 5.8131e-02 (5.8131e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][0/2]\tLoss 5.2318e-02 (5.2318e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][0/2]\tLoss 4.7086e-02 (4.7086e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/3]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/155]\n",
      "Fill TS Repository [100/155]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries D-12\u001b[0m\n",
      "\u001b[32m-- Train samples size: 224 - Test samples size: 7718\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/D-12/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][0/4]\tTotal Loss -2.2996e+00 (-2.2996e+00)\tConsistency Loss 2.2610e+00 (2.2610e+00)\tInconsistency Loss 1.1015e-01 (1.1015e-01)\tEntropy 2.2803e+00 (2.2803e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][0/4]\tTotal Loss -2.3029e+00 (-2.3029e+00)\tConsistency Loss 2.2618e+00 (2.2618e+00)\tInconsistency Loss 1.1006e-01 (1.1006e-01)\tEntropy 2.2824e+00 (2.2824e+00)\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][0/4]\tTotal Loss -2.3065e+00 (-2.3065e+00)\tConsistency Loss 2.2519e+00 (2.2519e+00)\tInconsistency Loss 1.1125e-01 (1.1125e-01)\tEntropy 2.2792e+00 (2.2792e+00)\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][0/4]\tTotal Loss -2.3113e+00 (-2.3113e+00)\tConsistency Loss 2.2141e+00 (2.2141e+00)\tInconsistency Loss 1.1576e-01 (1.1576e-01)\tEntropy 2.2627e+00 (2.2627e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][0/4]\tTotal Loss -2.3329e+00 (-2.3329e+00)\tConsistency Loss 2.1414e+00 (2.1414e+00)\tInconsistency Loss 1.2634e-01 (1.2634e-01)\tEntropy 2.2372e+00 (2.2372e+00)\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][0/4]\tTotal Loss -2.3599e+00 (-2.3599e+00)\tConsistency Loss 1.9619e+00 (1.9619e+00)\tInconsistency Loss 1.4899e-01 (1.4899e-01)\tEntropy 2.1609e+00 (2.1609e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][0/4]\tTotal Loss -2.4739e+00 (-2.4739e+00)\tConsistency Loss 1.7829e+00 (1.7829e+00)\tInconsistency Loss 1.8601e-01 (1.8601e-01)\tEntropy 2.1284e+00 (2.1284e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][0/4]\tTotal Loss -2.3812e+00 (-2.3812e+00)\tConsistency Loss 1.6333e+00 (1.6333e+00)\tInconsistency Loss 2.2264e-01 (2.2264e-01)\tEntropy 2.0073e+00 (2.0073e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][0/4]\tTotal Loss -2.3966e+00 (-2.3966e+00)\tConsistency Loss 1.7424e+00 (1.7424e+00)\tInconsistency Loss 1.8765e-01 (1.8765e-01)\tEntropy 2.0695e+00 (2.0695e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][0/4]\tTotal Loss -2.3599e+00 (-2.3599e+00)\tConsistency Loss 1.7295e+00 (1.7295e+00)\tInconsistency Loss 1.9493e-01 (1.9493e-01)\tEntropy 2.0447e+00 (2.0447e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][0/4]\tTotal Loss -2.4059e+00 (-2.4059e+00)\tConsistency Loss 1.7754e+00 (1.7754e+00)\tInconsistency Loss 1.7841e-01 (1.7841e-01)\tEntropy 2.0907e+00 (2.0907e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][0/4]\tTotal Loss -2.3868e+00 (-2.3868e+00)\tConsistency Loss 1.6795e+00 (1.6795e+00)\tInconsistency Loss 1.9889e-01 (1.9889e-01)\tEntropy 2.0332e+00 (2.0332e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][0/4]\tTotal Loss -2.3343e+00 (-2.3343e+00)\tConsistency Loss 1.7206e+00 (1.7206e+00)\tInconsistency Loss 2.0194e-01 (2.0194e-01)\tEntropy 2.0274e+00 (2.0274e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][0/4]\tTotal Loss -2.3541e+00 (-2.3541e+00)\tConsistency Loss 1.7514e+00 (1.7514e+00)\tInconsistency Loss 1.8987e-01 (1.8987e-01)\tEntropy 2.0528e+00 (2.0528e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][0/4]\tTotal Loss -2.3893e+00 (-2.3893e+00)\tConsistency Loss 1.7868e+00 (1.7868e+00)\tInconsistency Loss 1.8185e-01 (1.8185e-01)\tEntropy 2.0881e+00 (2.0881e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][0/4]\tTotal Loss -2.3264e+00 (-2.3264e+00)\tConsistency Loss 1.8184e+00 (1.8184e+00)\tInconsistency Loss 1.7827e-01 (1.7827e-01)\tEntropy 2.0724e+00 (2.0724e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][0/4]\tTotal Loss -2.4066e+00 (-2.4066e+00)\tConsistency Loss 1.8032e+00 (1.8032e+00)\tInconsistency Loss 1.8134e-01 (1.8134e-01)\tEntropy 2.1049e+00 (2.1049e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][0/4]\tTotal Loss -2.3965e+00 (-2.3965e+00)\tConsistency Loss 1.6983e+00 (1.6983e+00)\tInconsistency Loss 1.9556e-01 (1.9556e-01)\tEntropy 2.0474e+00 (2.0474e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][0/4]\tTotal Loss -2.3860e+00 (-2.3860e+00)\tConsistency Loss 1.7040e+00 (1.7040e+00)\tInconsistency Loss 1.9848e-01 (1.9848e-01)\tEntropy 2.0450e+00 (2.0450e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][0/4]\tTotal Loss -2.3842e+00 (-2.3842e+00)\tConsistency Loss 1.7162e+00 (1.7162e+00)\tInconsistency Loss 1.9553e-01 (1.9553e-01)\tEntropy 2.0502e+00 (2.0502e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][0/4]\tTotal Loss -2.3660e+00 (-2.3660e+00)\tConsistency Loss 1.7429e+00 (1.7429e+00)\tInconsistency Loss 1.9696e-01 (1.9696e-01)\tEntropy 2.0544e+00 (2.0544e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][0/4]\tTotal Loss -2.3696e+00 (-2.3696e+00)\tConsistency Loss 1.7144e+00 (1.7144e+00)\tInconsistency Loss 1.9980e-01 (1.9980e-01)\tEntropy 2.0420e+00 (2.0420e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][0/4]\tTotal Loss -2.3741e+00 (-2.3741e+00)\tConsistency Loss 1.7366e+00 (1.7366e+00)\tInconsistency Loss 1.9551e-01 (1.9551e-01)\tEntropy 2.0553e+00 (2.0553e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][0/4]\tTotal Loss -2.3796e+00 (-2.3796e+00)\tConsistency Loss 1.7114e+00 (1.7114e+00)\tInconsistency Loss 1.9942e-01 (1.9942e-01)\tEntropy 2.0455e+00 (2.0455e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][0/4]\tTotal Loss -2.3710e+00 (-2.3710e+00)\tConsistency Loss 1.7082e+00 (1.7082e+00)\tInconsistency Loss 1.9895e-01 (1.9895e-01)\tEntropy 2.0396e+00 (2.0396e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][0/4]\tTotal Loss -2.3994e+00 (-2.3994e+00)\tConsistency Loss 1.7448e+00 (1.7448e+00)\tInconsistency Loss 1.9119e-01 (1.9119e-01)\tEntropy 2.0721e+00 (2.0721e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][0/4]\tTotal Loss -2.3824e+00 (-2.3824e+00)\tConsistency Loss 1.7269e+00 (1.7269e+00)\tInconsistency Loss 1.9778e-01 (1.9778e-01)\tEntropy 2.0546e+00 (2.0546e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][0/4]\tTotal Loss -2.3814e+00 (-2.3814e+00)\tConsistency Loss 1.7117e+00 (1.7117e+00)\tInconsistency Loss 1.9911e-01 (1.9911e-01)\tEntropy 2.0466e+00 (2.0466e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][0/4]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7197e+00 (1.7197e+00)\tInconsistency Loss 1.9786e-01 (1.9786e-01)\tEntropy 2.0484e+00 (2.0484e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][0/4]\tTotal Loss -2.3868e+00 (-2.3868e+00)\tConsistency Loss 1.7129e+00 (1.7129e+00)\tInconsistency Loss 1.9985e-01 (1.9985e-01)\tEntropy 2.0498e+00 (2.0498e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][0/4]\tTotal Loss -2.3958e+00 (-2.3958e+00)\tConsistency Loss 1.7569e+00 (1.7569e+00)\tInconsistency Loss 1.9452e-01 (1.9452e-01)\tEntropy 2.0763e+00 (2.0763e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][0/4]\tTotal Loss -2.3352e+00 (-2.3352e+00)\tConsistency Loss 1.6936e+00 (1.6936e+00)\tInconsistency Loss 1.9549e-01 (1.9549e-01)\tEntropy 2.0144e+00 (2.0144e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][0/4]\tTotal Loss -2.3701e+00 (-2.3701e+00)\tConsistency Loss 1.7275e+00 (1.7275e+00)\tInconsistency Loss 1.9736e-01 (1.9736e-01)\tEntropy 2.0488e+00 (2.0488e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][0/4]\tTotal Loss -2.3452e+00 (-2.3452e+00)\tConsistency Loss 1.7825e+00 (1.7825e+00)\tInconsistency Loss 1.8320e-01 (1.8320e-01)\tEntropy 2.0639e+00 (2.0639e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][0/4]\tTotal Loss -2.3851e+00 (-2.3851e+00)\tConsistency Loss 1.7636e+00 (1.7636e+00)\tInconsistency Loss 1.8755e-01 (1.8755e-01)\tEntropy 2.0744e+00 (2.0744e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][0/4]\tTotal Loss -2.3778e+00 (-2.3778e+00)\tConsistency Loss 1.7117e+00 (1.7117e+00)\tInconsistency Loss 2.0417e-01 (2.0417e-01)\tEntropy 2.0447e+00 (2.0447e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][0/4]\tTotal Loss -2.3701e+00 (-2.3701e+00)\tConsistency Loss 1.6717e+00 (1.6717e+00)\tInconsistency Loss 2.0815e-01 (2.0815e-01)\tEntropy 2.0209e+00 (2.0209e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][0/4]\tTotal Loss -2.3899e+00 (-2.3899e+00)\tConsistency Loss 1.6816e+00 (1.6816e+00)\tInconsistency Loss 2.0275e-01 (2.0275e-01)\tEntropy 2.0358e+00 (2.0358e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][0/4]\tTotal Loss -2.3703e+00 (-2.3703e+00)\tConsistency Loss 1.7286e+00 (1.7286e+00)\tInconsistency Loss 1.9887e-01 (1.9887e-01)\tEntropy 2.0494e+00 (2.0494e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][0/4]\tTotal Loss -2.3730e+00 (-2.3730e+00)\tConsistency Loss 1.7134e+00 (1.7134e+00)\tInconsistency Loss 1.9684e-01 (1.9684e-01)\tEntropy 2.0432e+00 (2.0432e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][0/4]\tTotal Loss -2.3894e+00 (-2.3894e+00)\tConsistency Loss 1.7797e+00 (1.7797e+00)\tInconsistency Loss 1.8597e-01 (1.8597e-01)\tEntropy 2.0845e+00 (2.0845e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][0/4]\tTotal Loss -2.3822e+00 (-2.3822e+00)\tConsistency Loss 1.7267e+00 (1.7267e+00)\tInconsistency Loss 1.9680e-01 (1.9680e-01)\tEntropy 2.0544e+00 (2.0544e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][0/4]\tTotal Loss -2.3628e+00 (-2.3628e+00)\tConsistency Loss 1.7088e+00 (1.7088e+00)\tInconsistency Loss 2.0087e-01 (2.0087e-01)\tEntropy 2.0358e+00 (2.0358e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][0/4]\tTotal Loss -2.3814e+00 (-2.3814e+00)\tConsistency Loss 1.7610e+00 (1.7610e+00)\tInconsistency Loss 1.8804e-01 (1.8804e-01)\tEntropy 2.0712e+00 (2.0712e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][0/4]\tTotal Loss -2.3937e+00 (-2.3937e+00)\tConsistency Loss 1.7671e+00 (1.7671e+00)\tInconsistency Loss 1.8771e-01 (1.8771e-01)\tEntropy 2.0804e+00 (2.0804e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][0/4]\tTotal Loss -2.3791e+00 (-2.3791e+00)\tConsistency Loss 1.7175e+00 (1.7175e+00)\tInconsistency Loss 1.9688e-01 (1.9688e-01)\tEntropy 2.0483e+00 (2.0483e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][0/4]\tTotal Loss -2.3714e+00 (-2.3714e+00)\tConsistency Loss 1.7009e+00 (1.7009e+00)\tInconsistency Loss 2.0363e-01 (2.0363e-01)\tEntropy 2.0361e+00 (2.0361e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][0/4]\tTotal Loss -2.3568e+00 (-2.3568e+00)\tConsistency Loss 1.7288e+00 (1.7288e+00)\tInconsistency Loss 1.9665e-01 (1.9665e-01)\tEntropy 2.0428e+00 (2.0428e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][0/4]\tTotal Loss -2.3857e+00 (-2.3857e+00)\tConsistency Loss 1.7258e+00 (1.7258e+00)\tInconsistency Loss 1.9663e-01 (1.9663e-01)\tEntropy 2.0558e+00 (2.0558e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][0/4]\tTotal Loss -2.3618e+00 (-2.3618e+00)\tConsistency Loss 1.7203e+00 (1.7203e+00)\tInconsistency Loss 2.0010e-01 (2.0010e-01)\tEntropy 2.0410e+00 (2.0410e+00)\n",
      "B-1\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2235/7844 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/B-1/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/44]\tLoss 1.0005e+00 (1.0005e+00)\n",
      "Epoch: [1][10/44]\tLoss 3.4840e-01 (6.2415e-01)\n",
      "Epoch: [1][20/44]\tLoss 1.2143e-01 (4.2419e-01)\n",
      "Epoch: [1][30/44]\tLoss 4.2220e-02 (3.1032e-01)\n",
      "Epoch: [1][40/44]\tLoss 1.4742e-02 (2.4067e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/44]\tLoss 1.0740e-02 (1.0740e-02)\n",
      "Epoch: [2][10/44]\tLoss 1.0017e-02 (1.0077e-02)\n",
      "Epoch: [2][20/44]\tLoss 1.0016e-02 (1.0047e-02)\n",
      "Epoch: [2][30/44]\tLoss 1.0028e-02 (1.0037e-02)\n",
      "Epoch: [2][40/44]\tLoss 9.9954e-03 (1.0034e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/44]\tLoss 1.0013e-02 (1.0013e-02)\n",
      "Epoch: [3][10/44]\tLoss 1.0001e-02 (1.0020e-02)\n",
      "Epoch: [3][20/44]\tLoss 1.0018e-02 (1.0050e-02)\n",
      "Epoch: [3][30/44]\tLoss 1.0003e-02 (1.0036e-02)\n",
      "Epoch: [3][40/44]\tLoss 1.0018e-02 (1.0029e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/44]\tLoss 1.0031e-02 (1.0031e-02)\n",
      "Epoch: [4][10/44]\tLoss 1.0002e-02 (1.0010e-02)\n",
      "Epoch: [4][20/44]\tLoss 1.0006e-02 (1.0007e-02)\n",
      "Epoch: [4][30/44]\tLoss 1.0013e-02 (1.0007e-02)\n",
      "Epoch: [4][40/44]\tLoss 1.0001e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/44]\tLoss 1.0010e-02 (1.0010e-02)\n",
      "Epoch: [5][10/44]\tLoss 1.0009e-02 (1.0005e-02)\n",
      "Epoch: [5][20/44]\tLoss 1.0018e-02 (1.0005e-02)\n",
      "Epoch: [5][30/44]\tLoss 1.0003e-02 (1.0006e-02)\n",
      "Epoch: [5][40/44]\tLoss 1.0002e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [6][10/44]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [6][20/44]\tLoss 9.9993e-03 (1.0003e-02)\n",
      "Epoch: [6][30/44]\tLoss 1.0006e-02 (1.0003e-02)\n",
      "Epoch: [6][40/44]\tLoss 1.0012e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [7][10/44]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [7][20/44]\tLoss 1.0008e-02 (1.0004e-02)\n",
      "Epoch: [7][30/44]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [7][40/44]\tLoss 1.0006e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/44]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [8][10/44]\tLoss 9.9958e-03 (1.0004e-02)\n",
      "Epoch: [8][20/44]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [8][30/44]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [8][40/44]\tLoss 1.0009e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/44]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [9][10/44]\tLoss 1.0000e-02 (1.0003e-02)\n",
      "Epoch: [9][20/44]\tLoss 1.0005e-02 (1.0003e-02)\n",
      "Epoch: [9][30/44]\tLoss 1.0000e-02 (1.0003e-02)\n",
      "Epoch: [9][40/44]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/44]\tLoss 9.9981e-03 (9.9981e-03)\n",
      "Epoch: [10][10/44]\tLoss 1.0003e-02 (1.0000e-02)\n",
      "Epoch: [10][20/44]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [10][30/44]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [10][40/44]\tLoss 9.9999e-03 (1.0003e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][10/44]\tLoss 1.0004e-02 (1.0000e-02)\n",
      "Epoch: [11][20/44]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [11][30/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][40/44]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][10/44]\tLoss 1.0005e-02 (1.0002e-02)\n",
      "Epoch: [12][20/44]\tLoss 9.9982e-03 (1.0003e-02)\n",
      "Epoch: [12][30/44]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [12][40/44]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/44]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [13][10/44]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][20/44]\tLoss 9.9962e-03 (1.0003e-02)\n",
      "Epoch: [13][30/44]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][40/44]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [14][10/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][20/44]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [14][30/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][40/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/44]\tLoss 9.9986e-03 (9.9986e-03)\n",
      "Epoch: [15][10/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [15][20/44]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [15][30/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][40/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/44]\tLoss 9.9994e-03 (9.9994e-03)\n",
      "Epoch: [16][10/44]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [16][20/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][30/44]\tLoss 9.9998e-03 (1.0001e-02)\n",
      "Epoch: [16][40/44]\tLoss 9.9996e-03 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [17][10/44]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][20/44]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [17][30/44]\tLoss 1.0006e-02 (1.0001e-02)\n",
      "Epoch: [17][40/44]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/44]\tLoss 9.9979e-03 (9.9979e-03)\n",
      "Epoch: [18][10/44]\tLoss 1.0000e-02 (9.9996e-03)\n",
      "Epoch: [18][20/44]\tLoss 1.0001e-02 (9.9999e-03)\n",
      "Epoch: [18][30/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][40/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/44]\tLoss 9.9929e-03 (1.0001e-02)\n",
      "Epoch: [19][20/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/44]\tLoss 9.9986e-03 (1.0001e-02)\n",
      "Epoch: [19][40/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/44]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][10/44]\tLoss 1.0002e-02 (1.0000e-02)\n",
      "Epoch: [20][20/44]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [20][30/44]\tLoss 9.9995e-03 (1.0001e-02)\n",
      "Epoch: [20][40/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/44]\tLoss 9.9998e-03 (9.9998e-03)\n",
      "Epoch: [21][10/44]\tLoss 1.0066e-02 (1.0006e-02)\n",
      "Epoch: [21][20/44]\tLoss 9.9975e-03 (1.0003e-02)\n",
      "Epoch: [21][30/44]\tLoss 1.0000e-02 (1.0003e-02)\n",
      "Epoch: [21][40/44]\tLoss 9.9984e-03 (1.0002e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/44]\tLoss 9.9994e-03 (9.9994e-03)\n",
      "Epoch: [22][10/44]\tLoss 9.9958e-03 (1.0001e-02)\n",
      "Epoch: [22][20/44]\tLoss 9.9987e-03 (1.0002e-02)\n",
      "Epoch: [22][30/44]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [22][40/44]\tLoss 9.9994e-03 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [23][10/44]\tLoss 9.9982e-03 (9.9998e-03)\n",
      "Epoch: [23][20/44]\tLoss 9.9968e-03 (1.0000e-02)\n",
      "Epoch: [23][30/44]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [23][40/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][10/44]\tLoss 9.9997e-03 (1.0002e-02)\n",
      "Epoch: [24][20/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][30/44]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [24][40/44]\tLoss 9.9998e-03 (1.0000e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/44]\tLoss 9.9987e-03 (9.9987e-03)\n",
      "Epoch: [25][10/44]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [25][20/44]\tLoss 9.9952e-03 (1.0001e-02)\n",
      "Epoch: [25][30/44]\tLoss 9.9995e-03 (1.0001e-02)\n",
      "Epoch: [25][40/44]\tLoss 9.9971e-03 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [26][10/44]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [26][20/44]\tLoss 1.0000e-02 (9.9999e-03)\n",
      "Epoch: [26][30/44]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [26][40/44]\tLoss 1.0003e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/44]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [27][10/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][20/44]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][30/44]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [27][40/44]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/44]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][10/44]\tLoss 9.9960e-03 (1.0001e-02)\n",
      "Epoch: [28][20/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][30/44]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [28][40/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/44]\tLoss 9.9968e-03 (9.9968e-03)\n",
      "Epoch: [29][10/44]\tLoss 1.0002e-02 (1.0000e-02)\n",
      "Epoch: [29][20/44]\tLoss 9.9903e-03 (9.9993e-03)\n",
      "Epoch: [29][30/44]\tLoss 9.9996e-03 (9.9998e-03)\n",
      "Epoch: [29][40/44]\tLoss 9.9998e-03 (9.9998e-03)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/44]\tLoss 9.9974e-03 (9.9974e-03)\n",
      "Epoch: [30][10/44]\tLoss 9.9963e-03 (1.0000e-02)\n",
      "Epoch: [30][20/44]\tLoss 9.9997e-03 (1.0001e-02)\n",
      "Epoch: [30][30/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/44]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/45]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/157]\n",
      "Fill TS Repository [100/157]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries B-1\u001b[0m\n",
      "\u001b[32m-- Train samples size: 4470 - Test samples size: 7844\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/B-1/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][ 0/89]\tTotal Loss -2.3011e+00 (-2.3011e+00)\tConsistency Loss 2.2576e+00 (2.2576e+00)\tInconsistency Loss 1.1043e-01 (1.1043e-01)\tEntropy 2.2794e+00 (2.2794e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][ 0/89]\tTotal Loss -2.4748e+00 (-2.4748e+00)\tConsistency Loss 1.6409e+00 (1.6409e+00)\tInconsistency Loss 1.9980e-01 (1.9980e-01)\tEntropy 2.0578e+00 (2.0578e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][ 0/89]\tTotal Loss -2.4485e+00 (-2.4485e+00)\tConsistency Loss 1.8009e+00 (1.8009e+00)\tInconsistency Loss 1.8576e-01 (1.8576e-01)\tEntropy 2.1247e+00 (2.1247e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][ 0/89]\tTotal Loss -2.4188e+00 (-2.4188e+00)\tConsistency Loss 1.7194e+00 (1.7194e+00)\tInconsistency Loss 1.8981e-01 (1.8981e-01)\tEntropy 2.0691e+00 (2.0691e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][ 0/89]\tTotal Loss -2.4400e+00 (-2.4400e+00)\tConsistency Loss 1.7050e+00 (1.7050e+00)\tInconsistency Loss 1.9325e-01 (1.9325e-01)\tEntropy 2.0725e+00 (2.0725e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][ 0/89]\tTotal Loss -2.4292e+00 (-2.4292e+00)\tConsistency Loss 1.7432e+00 (1.7432e+00)\tInconsistency Loss 1.8808e-01 (1.8808e-01)\tEntropy 2.0862e+00 (2.0862e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][ 0/89]\tTotal Loss -2.4326e+00 (-2.4326e+00)\tConsistency Loss 1.7154e+00 (1.7154e+00)\tInconsistency Loss 2.0385e-01 (2.0385e-01)\tEntropy 2.0740e+00 (2.0740e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][ 0/89]\tTotal Loss -2.4489e+00 (-2.4489e+00)\tConsistency Loss 1.7825e+00 (1.7825e+00)\tInconsistency Loss 1.8118e-01 (1.8118e-01)\tEntropy 2.1157e+00 (2.1157e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][ 0/89]\tTotal Loss -2.4446e+00 (-2.4446e+00)\tConsistency Loss 1.7310e+00 (1.7310e+00)\tInconsistency Loss 1.9535e-01 (1.9535e-01)\tEntropy 2.0878e+00 (2.0878e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][ 0/89]\tTotal Loss -2.4684e+00 (-2.4684e+00)\tConsistency Loss 1.7065e+00 (1.7065e+00)\tInconsistency Loss 1.8115e-01 (1.8115e-01)\tEntropy 2.0874e+00 (2.0874e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][ 0/89]\tTotal Loss -2.4896e+00 (-2.4896e+00)\tConsistency Loss 1.7007e+00 (1.7007e+00)\tInconsistency Loss 2.0185e-01 (2.0185e-01)\tEntropy 2.0952e+00 (2.0952e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][ 0/89]\tTotal Loss -2.4817e+00 (-2.4817e+00)\tConsistency Loss 1.7088e+00 (1.7088e+00)\tInconsistency Loss 1.9807e-01 (1.9807e-01)\tEntropy 2.0953e+00 (2.0953e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][ 0/89]\tTotal Loss -2.4226e+00 (-2.4226e+00)\tConsistency Loss 1.7416e+00 (1.7416e+00)\tInconsistency Loss 1.8067e-01 (1.8067e-01)\tEntropy 2.0821e+00 (2.0821e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][ 0/89]\tTotal Loss -2.4615e+00 (-2.4615e+00)\tConsistency Loss 1.6869e+00 (1.6869e+00)\tInconsistency Loss 1.9331e-01 (1.9331e-01)\tEntropy 2.0742e+00 (2.0742e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][ 0/89]\tTotal Loss -2.3803e+00 (-2.3803e+00)\tConsistency Loss 1.8617e+00 (1.8617e+00)\tInconsistency Loss 1.6984e-01 (1.6984e-01)\tEntropy 2.1210e+00 (2.1210e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][ 0/89]\tTotal Loss -2.2509e+00 (-2.2509e+00)\tConsistency Loss 1.9597e+00 (1.9597e+00)\tInconsistency Loss 1.6384e-01 (1.6384e-01)\tEntropy 2.1053e+00 (2.1053e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][ 0/89]\tTotal Loss -2.4701e+00 (-2.4701e+00)\tConsistency Loss 1.7111e+00 (1.7111e+00)\tInconsistency Loss 2.2863e-01 (2.2863e-01)\tEntropy 2.0906e+00 (2.0906e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][ 0/89]\tTotal Loss -2.4460e+00 (-2.4460e+00)\tConsistency Loss 1.7016e+00 (1.7016e+00)\tInconsistency Loss 1.9374e-01 (1.9374e-01)\tEntropy 2.0738e+00 (2.0738e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][ 0/89]\tTotal Loss -2.4533e+00 (-2.4533e+00)\tConsistency Loss 1.7108e+00 (1.7108e+00)\tInconsistency Loss 1.9536e-01 (1.9536e-01)\tEntropy 2.0820e+00 (2.0820e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][ 0/89]\tTotal Loss -2.3785e+00 (-2.3785e+00)\tConsistency Loss 1.7679e+00 (1.7679e+00)\tInconsistency Loss 2.0651e-01 (2.0651e-01)\tEntropy 2.0732e+00 (2.0732e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][ 0/89]\tTotal Loss -2.4636e+00 (-2.4636e+00)\tConsistency Loss 1.7210e+00 (1.7210e+00)\tInconsistency Loss 2.0620e-01 (2.0620e-01)\tEntropy 2.0923e+00 (2.0923e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][ 0/89]\tTotal Loss -2.4189e+00 (-2.4189e+00)\tConsistency Loss 1.7118e+00 (1.7118e+00)\tInconsistency Loss 2.2670e-01 (2.2670e-01)\tEntropy 2.0654e+00 (2.0654e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][ 0/89]\tTotal Loss -2.4467e+00 (-2.4467e+00)\tConsistency Loss 1.7589e+00 (1.7589e+00)\tInconsistency Loss 1.9211e-01 (1.9211e-01)\tEntropy 2.1028e+00 (2.1028e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][ 0/89]\tTotal Loss -2.5049e+00 (-2.5049e+00)\tConsistency Loss 1.7211e+00 (1.7211e+00)\tInconsistency Loss 1.8156e-01 (1.8156e-01)\tEntropy 2.1130e+00 (2.1130e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][ 0/89]\tTotal Loss -2.5108e+00 (-2.5108e+00)\tConsistency Loss 1.6831e+00 (1.6831e+00)\tInconsistency Loss 1.9744e-01 (1.9744e-01)\tEntropy 2.0970e+00 (2.0970e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][ 0/89]\tTotal Loss -2.4370e+00 (-2.4370e+00)\tConsistency Loss 1.7297e+00 (1.7297e+00)\tInconsistency Loss 2.0769e-01 (2.0769e-01)\tEntropy 2.0833e+00 (2.0833e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][ 0/89]\tTotal Loss -2.4649e+00 (-2.4649e+00)\tConsistency Loss 1.7509e+00 (1.7509e+00)\tInconsistency Loss 1.7667e-01 (1.7667e-01)\tEntropy 2.1079e+00 (2.1079e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][ 0/89]\tTotal Loss -2.3821e+00 (-2.3821e+00)\tConsistency Loss 1.7666e+00 (1.7666e+00)\tInconsistency Loss 2.0018e-01 (2.0018e-01)\tEntropy 2.0743e+00 (2.0743e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][ 0/89]\tTotal Loss -2.5748e+00 (-2.5748e+00)\tConsistency Loss 1.5914e+00 (1.5914e+00)\tInconsistency Loss 2.1041e-01 (2.1041e-01)\tEntropy 2.0831e+00 (2.0831e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][ 0/89]\tTotal Loss -2.5019e+00 (-2.5019e+00)\tConsistency Loss 1.6287e+00 (1.6287e+00)\tInconsistency Loss 2.1470e-01 (2.1470e-01)\tEntropy 2.0653e+00 (2.0653e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][ 0/89]\tTotal Loss -2.4648e+00 (-2.4648e+00)\tConsistency Loss 1.7343e+00 (1.7343e+00)\tInconsistency Loss 1.8210e-01 (1.8210e-01)\tEntropy 2.0996e+00 (2.0996e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][ 0/89]\tTotal Loss -2.3936e+00 (-2.3936e+00)\tConsistency Loss 1.7512e+00 (1.7512e+00)\tInconsistency Loss 1.8415e-01 (1.8415e-01)\tEntropy 2.0724e+00 (2.0724e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][ 0/89]\tTotal Loss -2.3294e+00 (-2.3294e+00)\tConsistency Loss 1.8953e+00 (1.8953e+00)\tInconsistency Loss 1.8337e-01 (1.8337e-01)\tEntropy 2.1123e+00 (2.1123e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][ 0/89]\tTotal Loss -2.4843e+00 (-2.4843e+00)\tConsistency Loss 1.6844e+00 (1.6844e+00)\tInconsistency Loss 1.8917e-01 (1.8917e-01)\tEntropy 2.0843e+00 (2.0843e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][ 0/89]\tTotal Loss -2.5090e+00 (-2.5090e+00)\tConsistency Loss 1.6990e+00 (1.6990e+00)\tInconsistency Loss 1.8942e-01 (1.8942e-01)\tEntropy 2.1040e+00 (2.1040e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][ 0/89]\tTotal Loss -2.4472e+00 (-2.4472e+00)\tConsistency Loss 1.7336e+00 (1.7336e+00)\tInconsistency Loss 2.0872e-01 (2.0872e-01)\tEntropy 2.0904e+00 (2.0904e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][ 0/89]\tTotal Loss -2.4106e+00 (-2.4106e+00)\tConsistency Loss 1.7778e+00 (1.7778e+00)\tInconsistency Loss 1.7225e-01 (1.7225e-01)\tEntropy 2.0942e+00 (2.0942e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][ 0/89]\tTotal Loss -2.4163e+00 (-2.4163e+00)\tConsistency Loss 1.6994e+00 (1.6994e+00)\tInconsistency Loss 1.9326e-01 (1.9326e-01)\tEntropy 2.0579e+00 (2.0579e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][ 0/89]\tTotal Loss -2.4940e+00 (-2.4940e+00)\tConsistency Loss 1.6914e+00 (1.6914e+00)\tInconsistency Loss 1.7876e-01 (1.7876e-01)\tEntropy 2.0927e+00 (2.0927e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][ 0/89]\tTotal Loss -2.5294e+00 (-2.5294e+00)\tConsistency Loss 1.6446e+00 (1.6446e+00)\tInconsistency Loss 2.0704e-01 (2.0704e-01)\tEntropy 2.0870e+00 (2.0870e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][ 0/89]\tTotal Loss -2.4642e+00 (-2.4642e+00)\tConsistency Loss 1.6863e+00 (1.6863e+00)\tInconsistency Loss 2.0871e-01 (2.0871e-01)\tEntropy 2.0753e+00 (2.0753e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][ 0/89]\tTotal Loss -2.4417e+00 (-2.4417e+00)\tConsistency Loss 1.7404e+00 (1.7404e+00)\tInconsistency Loss 2.0229e-01 (2.0229e-01)\tEntropy 2.0910e+00 (2.0910e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][ 0/89]\tTotal Loss -2.4328e+00 (-2.4328e+00)\tConsistency Loss 1.7609e+00 (1.7609e+00)\tInconsistency Loss 1.8790e-01 (1.8790e-01)\tEntropy 2.0969e+00 (2.0969e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][ 0/89]\tTotal Loss -2.5066e+00 (-2.5066e+00)\tConsistency Loss 1.6748e+00 (1.6748e+00)\tInconsistency Loss 2.1887e-01 (2.1887e-01)\tEntropy 2.0907e+00 (2.0907e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][ 0/89]\tTotal Loss -2.4471e+00 (-2.4471e+00)\tConsistency Loss 1.7218e+00 (1.7218e+00)\tInconsistency Loss 1.9109e-01 (1.9109e-01)\tEntropy 2.0845e+00 (2.0845e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][ 0/89]\tTotal Loss -2.3783e+00 (-2.3783e+00)\tConsistency Loss 1.7740e+00 (1.7740e+00)\tInconsistency Loss 1.9504e-01 (1.9504e-01)\tEntropy 2.0762e+00 (2.0762e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][ 0/89]\tTotal Loss -2.4381e+00 (-2.4381e+00)\tConsistency Loss 1.6961e+00 (1.6961e+00)\tInconsistency Loss 2.0949e-01 (2.0949e-01)\tEntropy 2.0671e+00 (2.0671e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][ 0/89]\tTotal Loss -2.5377e+00 (-2.5377e+00)\tConsistency Loss 1.6589e+00 (1.6589e+00)\tInconsistency Loss 1.9987e-01 (1.9987e-01)\tEntropy 2.0983e+00 (2.0983e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][ 0/89]\tTotal Loss -2.4803e+00 (-2.4803e+00)\tConsistency Loss 1.7189e+00 (1.7189e+00)\tInconsistency Loss 1.8607e-01 (1.8607e-01)\tEntropy 2.0996e+00 (2.0996e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][ 0/89]\tTotal Loss -2.4002e+00 (-2.4002e+00)\tConsistency Loss 1.7701e+00 (1.7701e+00)\tInconsistency Loss 1.7481e-01 (1.7481e-01)\tEntropy 2.0851e+00 (2.0851e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "G-6\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2681/8440 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/G-6/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0016e+00 (1.0016e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4893e-01 (6.2427e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2148e-01 (4.2423e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2298e-02 (3.1034e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4747e-02 (2.4069e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0067e-02 (1.9558e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0090e-02 (1.0090e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0023e-02 (1.0063e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0039e-02 (1.0044e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0022e-02 (1.0038e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0010e-02 (1.0035e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0007e-02 (1.0031e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0023e-02 (1.0023e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0021e-02 (1.0023e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0005e-02 (1.0019e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0015e-02 (1.0017e-02)\n",
      "Epoch: [3][40/53]\tLoss 9.9989e-03 (1.0016e-02)\n",
      "Epoch: [3][50/53]\tLoss 9.9929e-03 (1.0015e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0009e-02 (1.0009e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0016e-02 (1.0011e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0053e-02 (1.0014e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0016e-02 (1.0013e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0016e-02 (1.0013e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0019e-02 (1.0012e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0017e-02 (1.0010e-02)\n",
      "Epoch: [5][20/53]\tLoss 9.9997e-03 (1.0012e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0012e-02 (1.0010e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0012e-02 (1.0010e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0008e-02 (1.0010e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [6][10/53]\tLoss 9.9991e-03 (1.0006e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0003e-02 (1.0005e-02)\n",
      "Epoch: [6][30/53]\tLoss 9.9986e-03 (1.0005e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0005e-02 (1.0006e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0006e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0003e-02 (1.0008e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0000e-02 (1.0007e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0005e-02 (1.0006e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0000e-02 (1.0006e-02)\n",
      "Epoch: [7][50/53]\tLoss 9.9985e-03 (1.0006e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [8][30/53]\tLoss 9.9991e-03 (1.0004e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0005e-02 (1.0004e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0006e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0005e-02 (1.0004e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0011e-02 (1.0003e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][50/53]\tLoss 9.9981e-03 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0009e-02 (1.0009e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0006e-02 (1.0003e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][50/53]\tLoss 9.9942e-03 (1.0001e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 9.9786e-03 (9.9786e-03)\n",
      "Epoch: [13][10/53]\tLoss 1.0002e-02 (1.0000e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [13][40/53]\tLoss 9.9963e-03 (1.0002e-02)\n",
      "Epoch: [13][50/53]\tLoss 9.9995e-03 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [14][30/53]\tLoss 9.9999e-03 (1.0001e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][50/53]\tLoss 9.9987e-03 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0007e-02 (1.0001e-02)\n",
      "Epoch: [15][30/53]\tLoss 9.9976e-03 (1.0001e-02)\n",
      "Epoch: [15][40/53]\tLoss 9.9933e-03 (1.0000e-02)\n",
      "Epoch: [15][50/53]\tLoss 9.9962e-03 (1.0000e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 9.9994e-03 (9.9994e-03)\n",
      "Epoch: [16][10/53]\tLoss 9.9968e-03 (9.9986e-03)\n",
      "Epoch: [16][20/53]\tLoss 9.9978e-03 (9.9983e-03)\n",
      "Epoch: [16][30/53]\tLoss 1.0002e-02 (9.9990e-03)\n",
      "Epoch: [16][40/53]\tLoss 9.9984e-03 (9.9986e-03)\n",
      "Epoch: [16][50/53]\tLoss 9.9913e-03 (9.9985e-03)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 9.9950e-03 (9.9950e-03)\n",
      "Epoch: [17][10/53]\tLoss 9.9958e-03 (9.9985e-03)\n",
      "Epoch: [17][20/53]\tLoss 9.9940e-03 (9.9972e-03)\n",
      "Epoch: [17][30/53]\tLoss 1.0000e-02 (9.9972e-03)\n",
      "Epoch: [17][40/53]\tLoss 1.0003e-02 (9.9969e-03)\n",
      "Epoch: [17][50/53]\tLoss 9.9787e-03 (9.9962e-03)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 9.9946e-03 (9.9946e-03)\n",
      "Epoch: [18][10/53]\tLoss 9.9962e-03 (9.9941e-03)\n",
      "Epoch: [18][20/53]\tLoss 9.9849e-03 (9.9913e-03)\n",
      "Epoch: [18][30/53]\tLoss 9.9113e-03 (9.9883e-03)\n",
      "Epoch: [18][40/53]\tLoss 9.9912e-03 (9.9882e-03)\n",
      "Epoch: [18][50/53]\tLoss 9.9447e-03 (9.9871e-03)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 9.9870e-03 (9.9870e-03)\n",
      "Epoch: [19][10/53]\tLoss 9.9796e-03 (9.9834e-03)\n",
      "Epoch: [19][20/53]\tLoss 9.9628e-03 (9.9805e-03)\n",
      "Epoch: [19][30/53]\tLoss 9.9505e-03 (9.9702e-03)\n",
      "Epoch: [19][40/53]\tLoss 9.9868e-03 (9.9623e-03)\n",
      "Epoch: [19][50/53]\tLoss 9.8114e-03 (9.9399e-03)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 9.5843e-03 (9.5843e-03)\n",
      "Epoch: [20][10/53]\tLoss 9.3763e-03 (9.6977e-03)\n",
      "Epoch: [20][20/53]\tLoss 9.6002e-03 (9.3418e-03)\n",
      "Epoch: [20][30/53]\tLoss 8.5512e-03 (9.1700e-03)\n",
      "Epoch: [20][40/53]\tLoss 8.1561e-03 (9.0469e-03)\n",
      "Epoch: [20][50/53]\tLoss 8.2965e-03 (8.9269e-03)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 7.3289e-03 (7.3289e-03)\n",
      "Epoch: [21][10/53]\tLoss 8.8330e-03 (8.2366e-03)\n",
      "Epoch: [21][20/53]\tLoss 8.2203e-03 (8.2600e-03)\n",
      "Epoch: [21][30/53]\tLoss 7.7453e-03 (8.1742e-03)\n",
      "Epoch: [21][40/53]\tLoss 8.2144e-03 (8.2271e-03)\n",
      "Epoch: [21][50/53]\tLoss 8.4568e-03 (8.1954e-03)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 8.0232e-03 (8.0232e-03)\n",
      "Epoch: [22][10/53]\tLoss 7.1269e-03 (7.8441e-03)\n",
      "Epoch: [22][20/53]\tLoss 7.0070e-03 (7.9017e-03)\n",
      "Epoch: [22][30/53]\tLoss 9.3489e-03 (8.0360e-03)\n",
      "Epoch: [22][40/53]\tLoss 7.8678e-03 (7.9922e-03)\n",
      "Epoch: [22][50/53]\tLoss 8.1918e-03 (7.9235e-03)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 7.3574e-03 (7.3574e-03)\n",
      "Epoch: [23][10/53]\tLoss 8.6352e-03 (7.8480e-03)\n",
      "Epoch: [23][20/53]\tLoss 8.9767e-03 (7.9882e-03)\n",
      "Epoch: [23][30/53]\tLoss 6.4993e-03 (8.0057e-03)\n",
      "Epoch: [23][40/53]\tLoss 7.4469e-03 (8.0505e-03)\n",
      "Epoch: [23][50/53]\tLoss 7.7008e-03 (8.0775e-03)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 7.6866e-03 (7.6866e-03)\n",
      "Epoch: [24][10/53]\tLoss 7.4691e-03 (7.8802e-03)\n",
      "Epoch: [24][20/53]\tLoss 6.9248e-03 (7.9594e-03)\n",
      "Epoch: [24][30/53]\tLoss 8.9318e-03 (7.9589e-03)\n",
      "Epoch: [24][40/53]\tLoss 7.0820e-03 (7.9204e-03)\n",
      "Epoch: [24][50/53]\tLoss 6.6941e-03 (7.8092e-03)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 8.8616e-03 (8.8616e-03)\n",
      "Epoch: [25][10/53]\tLoss 7.3408e-03 (7.9832e-03)\n",
      "Epoch: [25][20/53]\tLoss 7.1573e-03 (7.7411e-03)\n",
      "Epoch: [25][30/53]\tLoss 8.4500e-03 (7.8056e-03)\n",
      "Epoch: [25][40/53]\tLoss 9.1123e-03 (7.7726e-03)\n",
      "Epoch: [25][50/53]\tLoss 7.5047e-03 (7.7987e-03)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 7.9968e-03 (7.9968e-03)\n",
      "Epoch: [26][10/53]\tLoss 8.3359e-03 (7.7417e-03)\n",
      "Epoch: [26][20/53]\tLoss 7.7731e-03 (7.9859e-03)\n",
      "Epoch: [26][30/53]\tLoss 8.3762e-03 (7.8303e-03)\n",
      "Epoch: [26][40/53]\tLoss 7.2941e-03 (7.8249e-03)\n",
      "Epoch: [26][50/53]\tLoss 8.4978e-03 (7.7879e-03)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 5.3873e-03 (5.3873e-03)\n",
      "Epoch: [27][10/53]\tLoss 7.4535e-03 (7.4336e-03)\n",
      "Epoch: [27][20/53]\tLoss 7.9383e-03 (7.5490e-03)\n",
      "Epoch: [27][30/53]\tLoss 7.5253e-03 (7.7502e-03)\n",
      "Epoch: [27][40/53]\tLoss 7.8155e-03 (7.6947e-03)\n",
      "Epoch: [27][50/53]\tLoss 7.8963e-03 (7.6857e-03)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 8.7326e-03 (8.7326e-03)\n",
      "Epoch: [28][10/53]\tLoss 8.0379e-03 (7.4314e-03)\n",
      "Epoch: [28][20/53]\tLoss 7.9715e-03 (7.5058e-03)\n",
      "Epoch: [28][30/53]\tLoss 8.3723e-03 (7.6426e-03)\n",
      "Epoch: [28][40/53]\tLoss 7.6284e-03 (7.6473e-03)\n",
      "Epoch: [28][50/53]\tLoss 8.8067e-03 (7.7350e-03)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 7.4016e-03 (7.4016e-03)\n",
      "Epoch: [29][10/53]\tLoss 6.5555e-03 (7.8023e-03)\n",
      "Epoch: [29][20/53]\tLoss 8.1281e-03 (7.8557e-03)\n",
      "Epoch: [29][30/53]\tLoss 7.2728e-03 (7.6047e-03)\n",
      "Epoch: [29][40/53]\tLoss 8.8239e-03 (7.6695e-03)\n",
      "Epoch: [29][50/53]\tLoss 6.7064e-03 (7.6535e-03)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 8.0964e-03 (8.0964e-03)\n",
      "Epoch: [30][10/53]\tLoss 6.9415e-03 (7.8613e-03)\n",
      "Epoch: [30][20/53]\tLoss 7.6413e-03 (7.5511e-03)\n",
      "Epoch: [30][30/53]\tLoss 8.1507e-03 (7.6628e-03)\n",
      "Epoch: [30][40/53]\tLoss 8.5978e-03 (7.6774e-03)\n",
      "Epoch: [30][50/53]\tLoss 7.4524e-03 (7.6665e-03)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/169]\n",
      "Fill TS Repository [100/169]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries G-6\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5362 - Test samples size: 8440\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/G-6/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/107]\tTotal Loss -2.2894e+00 (-2.2894e+00)\tConsistency Loss 2.2644e+00 (2.2644e+00)\tInconsistency Loss 1.1224e-01 (1.1224e-01)\tEntropy 2.2769e+00 (2.2769e+00)\n",
      "Epoch: [1][100/107]\tTotal Loss -2.3615e+00 (-2.3565e+00)\tConsistency Loss 1.7599e+00 (1.8639e+00)\tInconsistency Loss 1.8873e-01 (1.7349e-01)\tEntropy 2.0607e+00 (2.1102e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/107]\tTotal Loss -2.3654e+00 (-2.3654e+00)\tConsistency Loss 1.7487e+00 (1.7487e+00)\tInconsistency Loss 1.9742e-01 (1.9742e-01)\tEntropy 2.0571e+00 (2.0571e+00)\n",
      "Epoch: [2][100/107]\tTotal Loss -2.3980e+00 (-2.3760e+00)\tConsistency Loss 1.7498e+00 (1.7220e+00)\tInconsistency Loss 1.9361e-01 (1.9718e-01)\tEntropy 2.0739e+00 (2.0490e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/107]\tTotal Loss -2.3649e+00 (-2.3649e+00)\tConsistency Loss 1.7127e+00 (1.7127e+00)\tInconsistency Loss 1.9917e-01 (1.9917e-01)\tEntropy 2.0388e+00 (2.0388e+00)\n",
      "Epoch: [3][100/107]\tTotal Loss -2.3679e+00 (-2.3765e+00)\tConsistency Loss 1.7317e+00 (1.7209e+00)\tInconsistency Loss 1.9629e-01 (1.9711e-01)\tEntropy 2.0498e+00 (2.0487e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/107]\tTotal Loss -2.3616e+00 (-2.3616e+00)\tConsistency Loss 1.7466e+00 (1.7466e+00)\tInconsistency Loss 1.9385e-01 (1.9385e-01)\tEntropy 2.0541e+00 (2.0541e+00)\n",
      "Epoch: [4][100/107]\tTotal Loss -2.3804e+00 (-2.3768e+00)\tConsistency Loss 1.7063e+00 (1.7197e+00)\tInconsistency Loss 2.0007e-01 (1.9765e-01)\tEntropy 2.0434e+00 (2.0483e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/107]\tTotal Loss -2.3757e+00 (-2.3757e+00)\tConsistency Loss 1.7214e+00 (1.7214e+00)\tInconsistency Loss 1.9579e-01 (1.9579e-01)\tEntropy 2.0486e+00 (2.0486e+00)\n",
      "Epoch: [5][100/107]\tTotal Loss -2.3776e+00 (-2.3768e+00)\tConsistency Loss 1.7274e+00 (1.7198e+00)\tInconsistency Loss 1.9646e-01 (1.9739e-01)\tEntropy 2.0525e+00 (2.0483e+00)\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/107]\tTotal Loss -2.3811e+00 (-2.3811e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9752e-01 (1.9752e-01)\tEntropy 2.0496e+00 (2.0496e+00)\n",
      "Epoch: [6][100/107]\tTotal Loss -2.3808e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7184e+00)\tInconsistency Loss 1.9753e-01 (1.9768e-01)\tEntropy 2.0494e+00 (2.0478e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/107]\tTotal Loss -2.3722e+00 (-2.3722e+00)\tConsistency Loss 1.7198e+00 (1.7198e+00)\tInconsistency Loss 1.9809e-01 (1.9809e-01)\tEntropy 2.0460e+00 (2.0460e+00)\n",
      "Epoch: [7][100/107]\tTotal Loss -2.3771e+00 (-2.3769e+00)\tConsistency Loss 1.7183e+00 (1.7190e+00)\tInconsistency Loss 1.9794e-01 (1.9754e-01)\tEntropy 2.0477e+00 (2.0480e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/107]\tTotal Loss -2.3790e+00 (-2.3790e+00)\tConsistency Loss 1.7159e+00 (1.7159e+00)\tInconsistency Loss 1.9822e-01 (1.9822e-01)\tEntropy 2.0474e+00 (2.0474e+00)\n",
      "Epoch: [8][100/107]\tTotal Loss -2.3786e+00 (-2.3771e+00)\tConsistency Loss 1.7187e+00 (1.7185e+00)\tInconsistency Loss 1.9749e-01 (1.9763e-01)\tEntropy 2.0487e+00 (2.0478e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/107]\tTotal Loss -2.3760e+00 (-2.3760e+00)\tConsistency Loss 1.7138e+00 (1.7138e+00)\tInconsistency Loss 1.9871e-01 (1.9871e-01)\tEntropy 2.0449e+00 (2.0449e+00)\n",
      "Epoch: [9][100/107]\tTotal Loss -2.3788e+00 (-2.3770e+00)\tConsistency Loss 1.7189e+00 (1.7186e+00)\tInconsistency Loss 1.9772e-01 (1.9764e-01)\tEntropy 2.0488e+00 (2.0478e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/107]\tTotal Loss -2.3794e+00 (-2.3794e+00)\tConsistency Loss 1.7158e+00 (1.7158e+00)\tInconsistency Loss 1.9831e-01 (1.9831e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [10][100/107]\tTotal Loss -2.3772e+00 (-2.3771e+00)\tConsistency Loss 1.7135e+00 (1.7183e+00)\tInconsistency Loss 1.9877e-01 (1.9771e-01)\tEntropy 2.0454e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/107]\tTotal Loss -2.3783e+00 (-2.3783e+00)\tConsistency Loss 1.7217e+00 (1.7217e+00)\tInconsistency Loss 1.9698e-01 (1.9698e-01)\tEntropy 2.0500e+00 (2.0500e+00)\n",
      "Epoch: [11][100/107]\tTotal Loss -2.3774e+00 (-2.3772e+00)\tConsistency Loss 1.7179e+00 (1.7181e+00)\tInconsistency Loss 1.9769e-01 (1.9775e-01)\tEntropy 2.0476e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/107]\tTotal Loss -2.3780e+00 (-2.3780e+00)\tConsistency Loss 1.7177e+00 (1.7177e+00)\tInconsistency Loss 1.9759e-01 (1.9759e-01)\tEntropy 2.0478e+00 (2.0478e+00)\n",
      "Epoch: [12][100/107]\tTotal Loss -2.3772e+00 (-2.3771e+00)\tConsistency Loss 1.7186e+00 (1.7181e+00)\tInconsistency Loss 1.9761e-01 (1.9773e-01)\tEntropy 2.0479e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7177e+00 (1.7177e+00)\tInconsistency Loss 1.9783e-01 (1.9783e-01)\tEntropy 2.0474e+00 (2.0474e+00)\n",
      "Epoch: [13][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "G-7\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2246/7829 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/G-7/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/44]\tLoss 1.0037e+00 (1.0037e+00)\n",
      "Epoch: [1][10/44]\tLoss 3.4853e-01 (6.2462e-01)\n",
      "Epoch: [1][20/44]\tLoss 1.2111e-01 (4.2433e-01)\n",
      "Epoch: [1][30/44]\tLoss 4.2243e-02 (3.1037e-01)\n",
      "Epoch: [1][40/44]\tLoss 1.4669e-02 (2.4069e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/44]\tLoss 1.0699e-02 (1.0699e-02)\n",
      "Epoch: [2][10/44]\tLoss 1.0060e-02 (1.0115e-02)\n",
      "Epoch: [2][20/44]\tLoss 1.0024e-02 (1.0083e-02)\n",
      "Epoch: [2][30/44]\tLoss 1.0045e-02 (1.0067e-02)\n",
      "Epoch: [2][40/44]\tLoss 1.0031e-02 (1.0059e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/44]\tLoss 1.0045e-02 (1.0045e-02)\n",
      "Epoch: [3][10/44]\tLoss 1.0009e-02 (1.0024e-02)\n",
      "Epoch: [3][20/44]\tLoss 1.0025e-02 (1.0024e-02)\n",
      "Epoch: [3][30/44]\tLoss 1.0012e-02 (1.0022e-02)\n",
      "Epoch: [3][40/44]\tLoss 1.0017e-02 (1.0021e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/44]\tLoss 1.0011e-02 (1.0011e-02)\n",
      "Epoch: [4][10/44]\tLoss 1.0010e-02 (1.0013e-02)\n",
      "Epoch: [4][20/44]\tLoss 1.0011e-02 (1.0014e-02)\n",
      "Epoch: [4][30/44]\tLoss 1.0017e-02 (1.0015e-02)\n",
      "Epoch: [4][40/44]\tLoss 1.0017e-02 (1.0015e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/44]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [5][10/44]\tLoss 1.0040e-02 (1.0015e-02)\n",
      "Epoch: [5][20/44]\tLoss 1.0014e-02 (1.0013e-02)\n",
      "Epoch: [5][30/44]\tLoss 1.0007e-02 (1.0013e-02)\n",
      "Epoch: [5][40/44]\tLoss 1.0007e-02 (1.0012e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/44]\tLoss 1.0013e-02 (1.0013e-02)\n",
      "Epoch: [6][10/44]\tLoss 1.0012e-02 (1.0010e-02)\n",
      "Epoch: [6][20/44]\tLoss 1.0008e-02 (1.0010e-02)\n",
      "Epoch: [6][30/44]\tLoss 1.0004e-02 (1.0009e-02)\n",
      "Epoch: [6][40/44]\tLoss 1.0005e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/44]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [7][10/44]\tLoss 1.0010e-02 (1.0008e-02)\n",
      "Epoch: [7][20/44]\tLoss 1.0003e-02 (1.0007e-02)\n",
      "Epoch: [7][30/44]\tLoss 1.0004e-02 (1.0007e-02)\n",
      "Epoch: [7][40/44]\tLoss 1.0024e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/44]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [8][10/44]\tLoss 1.0002e-02 (1.0006e-02)\n",
      "Epoch: [8][20/44]\tLoss 1.0005e-02 (1.0006e-02)\n",
      "Epoch: [8][30/44]\tLoss 1.0007e-02 (1.0006e-02)\n",
      "Epoch: [8][40/44]\tLoss 1.0011e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/44]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [9][10/44]\tLoss 1.0005e-02 (1.0006e-02)\n",
      "Epoch: [9][20/44]\tLoss 1.0004e-02 (1.0005e-02)\n",
      "Epoch: [9][30/44]\tLoss 1.0004e-02 (1.0005e-02)\n",
      "Epoch: [9][40/44]\tLoss 1.0004e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/44]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [10][10/44]\tLoss 1.0017e-02 (1.0006e-02)\n",
      "Epoch: [10][20/44]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [10][30/44]\tLoss 1.0006e-02 (1.0005e-02)\n",
      "Epoch: [10][40/44]\tLoss 1.0006e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/44]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [11][10/44]\tLoss 1.0006e-02 (1.0005e-02)\n",
      "Epoch: [11][20/44]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [11][30/44]\tLoss 1.0005e-02 (1.0004e-02)\n",
      "Epoch: [11][40/44]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/44]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [12][10/44]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [12][20/44]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [12][30/44]\tLoss 1.0011e-02 (1.0005e-02)\n",
      "Epoch: [12][40/44]\tLoss 1.0004e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/44]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [13][10/44]\tLoss 1.0005e-02 (1.0004e-02)\n",
      "Epoch: [13][20/44]\tLoss 1.0008e-02 (1.0004e-02)\n",
      "Epoch: [13][30/44]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [13][40/44]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [14][10/44]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [14][20/44]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [14][30/44]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [14][40/44]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/44]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [15][10/44]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [15][20/44]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [15][30/44]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [15][40/44]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [16][10/44]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [16][20/44]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [16][30/44]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [16][40/44]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [17][10/44]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [17][20/44]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [17][30/44]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [17][40/44]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/44]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [18][10/44]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [18][20/44]\tLoss 1.0006e-02 (1.0003e-02)\n",
      "Epoch: [18][30/44]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [18][40/44]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/44]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [19][20/44]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [19][30/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [19][40/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/44]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [20][10/44]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [20][20/44]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [20][30/44]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [20][40/44]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [21][20/44]\tLoss 1.0006e-02 (1.0002e-02)\n",
      "Epoch: [21][30/44]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [21][40/44]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/44]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [22][10/44]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [22][20/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [22][30/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [22][40/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/44]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [23][10/44]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [23][20/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [23][30/44]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [23][40/44]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][10/44]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [24][20/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [24][30/44]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [24][40/44]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/44]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/44]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [25][20/44]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [25][30/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [25][40/44]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [26][10/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [26][20/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [26][30/44]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [26][40/44]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/44]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [27][10/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [27][20/44]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [27][30/44]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [27][40/44]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/44]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [28][10/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [28][20/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [28][30/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [28][40/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/44]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [29][10/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [29][20/44]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [29][30/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [29][40/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/44]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][10/44]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [30][20/44]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [30][30/44]\tLoss 1.0005e-02 (1.0002e-02)\n",
      "Epoch: [30][40/44]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/45]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/157]\n",
      "Fill TS Repository [100/157]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries G-7\u001b[0m\n",
      "\u001b[32m-- Train samples size: 4492 - Test samples size: 7829\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/G-7/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][ 0/89]\tTotal Loss -2.3024e+00 (-2.3024e+00)\tConsistency Loss 2.2584e+00 (2.2584e+00)\tInconsistency Loss 1.1027e-01 (1.1027e-01)\tEntropy 2.2804e+00 (2.2804e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][ 0/89]\tTotal Loss -2.3699e+00 (-2.3699e+00)\tConsistency Loss 1.7313e+00 (1.7313e+00)\tInconsistency Loss 1.9818e-01 (1.9818e-01)\tEntropy 2.0506e+00 (2.0506e+00)\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][ 0/89]\tTotal Loss -2.3978e+00 (-2.3978e+00)\tConsistency Loss 1.7295e+00 (1.7295e+00)\tInconsistency Loss 1.9382e-01 (1.9382e-01)\tEntropy 2.0637e+00 (2.0637e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][ 0/89]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7142e+00 (1.7142e+00)\tInconsistency Loss 1.9864e-01 (1.9864e-01)\tEntropy 2.0457e+00 (2.0457e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][ 0/89]\tTotal Loss -2.3768e+00 (-2.3768e+00)\tConsistency Loss 1.7190e+00 (1.7190e+00)\tInconsistency Loss 1.9755e-01 (1.9755e-01)\tEntropy 2.0479e+00 (2.0479e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][ 0/89]\tTotal Loss -2.3774e+00 (-2.3774e+00)\tConsistency Loss 1.7208e+00 (1.7208e+00)\tInconsistency Loss 1.9721e-01 (1.9721e-01)\tEntropy 2.0491e+00 (2.0491e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][ 0/89]\tTotal Loss -2.3764e+00 (-2.3764e+00)\tConsistency Loss 1.7172e+00 (1.7172e+00)\tInconsistency Loss 1.9709e-01 (1.9709e-01)\tEntropy 2.0468e+00 (2.0468e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][ 0/89]\tTotal Loss -2.3778e+00 (-2.3778e+00)\tConsistency Loss 1.7210e+00 (1.7210e+00)\tInconsistency Loss 1.9754e-01 (1.9754e-01)\tEntropy 2.0494e+00 (2.0494e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][ 0/89]\tTotal Loss -2.3769e+00 (-2.3769e+00)\tConsistency Loss 1.7161e+00 (1.7161e+00)\tInconsistency Loss 1.9816e-01 (1.9816e-01)\tEntropy 2.0465e+00 (2.0465e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9776e-01 (1.9776e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7201e+00 (1.7201e+00)\tInconsistency Loss 1.9726e-01 (1.9726e-01)\tEntropy 2.0486e+00 (2.0486e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][ 0/89]\tTotal Loss -2.3774e+00 (-2.3774e+00)\tConsistency Loss 1.7185e+00 (1.7185e+00)\tInconsistency Loss 1.9760e-01 (1.9760e-01)\tEntropy 2.0480e+00 (2.0480e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7208e+00 (1.7208e+00)\tInconsistency Loss 1.9713e-01 (1.9713e-01)\tEntropy 2.0490e+00 (2.0490e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][ 0/89]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "P-7\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2653/7871 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/P-7/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0041e+00 (1.0041e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4876e-01 (6.2434e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2129e-01 (4.2427e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2239e-02 (3.1033e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4783e-02 (2.4068e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0093e-02 (1.9557e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0018e-02 (1.0018e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0011e-02 (1.0038e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0073e-02 (1.0067e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0012e-02 (1.0059e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0016e-02 (1.0051e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0040e-02 (1.0044e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0019e-02 (1.0019e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0017e-02 (1.0041e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0021e-02 (1.0029e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0022e-02 (1.0026e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0010e-02 (1.0025e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0002e-02 (1.0021e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0009e-02 (1.0006e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0033e-02 (1.0007e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0007e-02 (1.0008e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0004e-02 (1.0008e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0013e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0006e-02 (1.0009e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0050e-02 (1.0010e-02)\n",
      "Epoch: [5][30/53]\tLoss 9.9966e-03 (1.0009e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0003e-02 (1.0009e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0078e-02 (1.0011e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0052e-02 (1.0052e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0003e-02 (1.0009e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0004e-02 (1.0007e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0005e-02 (1.0007e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0002e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0007e-02 (1.0005e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0002e-02 (1.0007e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0002e-02 (1.0006e-02)\n",
      "Epoch: [7][50/53]\tLoss 9.9886e-03 (1.0006e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0044e-02 (1.0044e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0002e-02 (1.0007e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0000e-02 (1.0006e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0004e-02 (1.0005e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0004e-02 (1.0005e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0009e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0000e-02 (1.0005e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0008e-02 (1.0004e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0007e-02 (1.0004e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0006e-02 (1.0004e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0006e-02 (1.0004e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0005e-02 (1.0003e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0004e-02 (1.0003e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [12][20/53]\tLoss 9.9999e-03 (1.0003e-02)\n",
      "Epoch: [12][30/53]\tLoss 9.9927e-03 (1.0003e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0007e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [13][20/53]\tLoss 9.9993e-03 (1.0004e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0005e-02 (1.0003e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0003e-02 (1.0005e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0011e-02 (1.0003e-02)\n",
      "Epoch: [14][50/53]\tLoss 9.9989e-03 (1.0002e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0000e-02 (1.0003e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [16][50/53]\tLoss 1.0011e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/53]\tLoss 9.9997e-03 (1.0001e-02)\n",
      "Epoch: [17][20/53]\tLoss 9.9971e-03 (1.0001e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][40/53]\tLoss 9.9966e-03 (1.0001e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][40/53]\tLoss 9.9992e-03 (1.0001e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [20][40/53]\tLoss 9.9975e-03 (1.0002e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0005e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0005e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0010e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [24][10/53]\tLoss 9.9999e-03 (1.0002e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0005e-02 (1.0002e-02)\n",
      "Epoch: [24][50/53]\tLoss 9.9997e-03 (1.0002e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [25][50/53]\tLoss 9.9994e-03 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0010e-02 (1.0010e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][20/53]\tLoss 9.9989e-03 (1.0001e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0007e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 9.9998e-03 (9.9998e-03)\n",
      "Epoch: [29][10/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][40/53]\tLoss 9.9988e-03 (1.0001e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/53]\tLoss 9.9983e-03 (1.0001e-02)\n",
      "Epoch: [30][50/53]\tLoss 9.9999e-03 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/158]\n",
      "Fill TS Repository [100/158]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries P-7\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5306 - Test samples size: 7871\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/P-7/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/106]\tTotal Loss -2.3000e+00 (-2.3000e+00)\tConsistency Loss 2.2581e+00 (2.2581e+00)\tInconsistency Loss 1.1052e-01 (1.1052e-01)\tEntropy 2.2790e+00 (2.2790e+00)\n",
      "Epoch: [1][100/106]\tTotal Loss -2.3811e+00 (-2.3577e+00)\tConsistency Loss 1.7145e+00 (1.8457e+00)\tInconsistency Loss 1.9731e-01 (1.7650e-01)\tEntropy 2.0478e+00 (2.1017e+00)\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/106]\tTotal Loss -2.3803e+00 (-2.3803e+00)\tConsistency Loss 1.6776e+00 (1.6776e+00)\tInconsistency Loss 2.0520e-01 (2.0520e-01)\tEntropy 2.0289e+00 (2.0289e+00)\n",
      "Epoch: [2][100/106]\tTotal Loss -2.3658e+00 (-2.3759e+00)\tConsistency Loss 1.7325e+00 (1.7215e+00)\tInconsistency Loss 1.9701e-01 (1.9693e-01)\tEntropy 2.0492e+00 (2.0487e+00)\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/106]\tTotal Loss -2.3782e+00 (-2.3782e+00)\tConsistency Loss 1.7423e+00 (1.7423e+00)\tInconsistency Loss 1.9137e-01 (1.9137e-01)\tEntropy 2.0603e+00 (2.0603e+00)\n",
      "Epoch: [3][100/106]\tTotal Loss -2.3837e+00 (-2.3770e+00)\tConsistency Loss 1.7047e+00 (1.7197e+00)\tInconsistency Loss 2.0019e-01 (1.9754e-01)\tEntropy 2.0442e+00 (2.0484e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/106]\tTotal Loss -2.3788e+00 (-2.3788e+00)\tConsistency Loss 1.7292e+00 (1.7292e+00)\tInconsistency Loss 1.9644e-01 (1.9644e-01)\tEntropy 2.0540e+00 (2.0540e+00)\n",
      "Epoch: [4][100/106]\tTotal Loss -2.3709e+00 (-2.3769e+00)\tConsistency Loss 1.7239e+00 (1.7205e+00)\tInconsistency Loss 1.9698e-01 (1.9734e-01)\tEntropy 2.0474e+00 (2.0487e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/106]\tTotal Loss -2.3793e+00 (-2.3793e+00)\tConsistency Loss 1.7179e+00 (1.7179e+00)\tInconsistency Loss 1.9665e-01 (1.9665e-01)\tEntropy 2.0486e+00 (2.0486e+00)\n",
      "Epoch: [5][100/106]\tTotal Loss -2.3711e+00 (-2.3770e+00)\tConsistency Loss 1.7173e+00 (1.7187e+00)\tInconsistency Loss 1.9881e-01 (1.9780e-01)\tEntropy 2.0442e+00 (2.0478e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/106]\tTotal Loss -2.3793e+00 (-2.3793e+00)\tConsistency Loss 1.7257e+00 (1.7257e+00)\tInconsistency Loss 1.9534e-01 (1.9534e-01)\tEntropy 2.0525e+00 (2.0525e+00)\n",
      "Epoch: [6][100/106]\tTotal Loss -2.3766e+00 (-2.3772e+00)\tConsistency Loss 1.7173e+00 (1.7184e+00)\tInconsistency Loss 1.9802e-01 (1.9765e-01)\tEntropy 2.0470e+00 (2.0478e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/106]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7194e+00 (1.7194e+00)\tInconsistency Loss 1.9659e-01 (1.9659e-01)\tEntropy 2.0483e+00 (2.0483e+00)\n",
      "Epoch: [7][100/106]\tTotal Loss -2.3802e+00 (-2.3776e+00)\tConsistency Loss 1.7059e+00 (1.7175e+00)\tInconsistency Loss 2.0015e-01 (1.9787e-01)\tEntropy 2.0431e+00 (2.0475e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/106]\tTotal Loss -2.3806e+00 (-2.3806e+00)\tConsistency Loss 1.7174e+00 (1.7174e+00)\tInconsistency Loss 1.9784e-01 (1.9784e-01)\tEntropy 2.0490e+00 (2.0490e+00)\n",
      "Epoch: [8][100/106]\tTotal Loss -2.3716e+00 (-2.3767e+00)\tConsistency Loss 1.7134e+00 (1.7195e+00)\tInconsistency Loss 1.9881e-01 (1.9757e-01)\tEntropy 2.0425e+00 (2.0481e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/106]\tTotal Loss -2.3743e+00 (-2.3743e+00)\tConsistency Loss 1.7189e+00 (1.7189e+00)\tInconsistency Loss 1.9734e-01 (1.9734e-01)\tEntropy 2.0466e+00 (2.0466e+00)\n",
      "Epoch: [9][100/106]\tTotal Loss -2.3772e+00 (-2.3773e+00)\tConsistency Loss 1.7221e+00 (1.7181e+00)\tInconsistency Loss 1.9684e-01 (1.9771e-01)\tEntropy 2.0497e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7148e+00 (1.7148e+00)\tInconsistency Loss 1.9846e-01 (1.9846e-01)\tEntropy 2.0460e+00 (2.0460e+00)\n",
      "Epoch: [10][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [11][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [12][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "R-1\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2674/7044 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/R-1/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0000e+00 (1.0000e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4868e-01 (6.2381e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2158e-01 (4.2409e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2390e-02 (3.1027e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4781e-02 (2.4066e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0000e-02 (1.9555e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0000e-02 (1.0003e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0000e-02 (1.0013e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0000e-02 (1.0011e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0003e-02 (1.0000e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0007e-02 (1.0001e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][40/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][50/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/141]\n",
      "Fill TS Repository [100/141]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries R-1\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5348 - Test samples size: 7044\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/R-1/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/106]\tTotal Loss -2.3007e+00 (-2.3007e+00)\tConsistency Loss 2.2572e+00 (2.2572e+00)\tInconsistency Loss 1.1057e-01 (1.1057e-01)\tEntropy 2.2790e+00 (2.2790e+00)\n",
      "Epoch: [1][100/106]\tTotal Loss -2.3685e+00 (-2.3569e+00)\tConsistency Loss 1.7484e+00 (1.8579e+00)\tInconsistency Loss 1.9267e-01 (1.7467e-01)\tEntropy 2.0584e+00 (2.1074e+00)\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/106]\tTotal Loss -2.3829e+00 (-2.3829e+00)\tConsistency Loss 1.7120e+00 (1.7120e+00)\tInconsistency Loss 1.9709e-01 (1.9709e-01)\tEntropy 2.0474e+00 (2.0474e+00)\n",
      "Epoch: [2][100/106]\tTotal Loss -2.3807e+00 (-2.3767e+00)\tConsistency Loss 1.7146e+00 (1.7190e+00)\tInconsistency Loss 1.9827e-01 (1.9762e-01)\tEntropy 2.0477e+00 (2.0479e+00)\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/106]\tTotal Loss -2.3876e+00 (-2.3876e+00)\tConsistency Loss 1.7311e+00 (1.7311e+00)\tInconsistency Loss 1.9462e-01 (1.9462e-01)\tEntropy 2.0594e+00 (2.0594e+00)\n",
      "Epoch: [3][100/106]\tTotal Loss -2.3813e+00 (-2.3767e+00)\tConsistency Loss 1.7239e+00 (1.7202e+00)\tInconsistency Loss 1.9639e-01 (1.9730e-01)\tEntropy 2.0526e+00 (2.0485e+00)\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/106]\tTotal Loss -2.3753e+00 (-2.3753e+00)\tConsistency Loss 1.7128e+00 (1.7128e+00)\tInconsistency Loss 1.9893e-01 (1.9893e-01)\tEntropy 2.0441e+00 (2.0441e+00)\n",
      "Epoch: [4][100/106]\tTotal Loss -2.3791e+00 (-2.3774e+00)\tConsistency Loss 1.7427e+00 (1.7172e+00)\tInconsistency Loss 1.9194e-01 (1.9808e-01)\tEntropy 2.0609e+00 (2.0473e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7232e+00 (1.7232e+00)\tInconsistency Loss 1.9474e-01 (1.9474e-01)\tEntropy 2.0502e+00 (2.0502e+00)\n",
      "Epoch: [5][100/106]\tTotal Loss -2.3828e+00 (-2.3774e+00)\tConsistency Loss 1.7280e+00 (1.7173e+00)\tInconsistency Loss 1.9394e-01 (1.9792e-01)\tEntropy 2.0554e+00 (2.0473e+00)\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/106]\tTotal Loss -2.3787e+00 (-2.3787e+00)\tConsistency Loss 1.7195e+00 (1.7195e+00)\tInconsistency Loss 1.9716e-01 (1.9716e-01)\tEntropy 2.0491e+00 (2.0491e+00)\n",
      "Epoch: [6][100/106]\tTotal Loss -2.3782e+00 (-2.3772e+00)\tConsistency Loss 1.7225e+00 (1.7185e+00)\tInconsistency Loss 1.9669e-01 (1.9767e-01)\tEntropy 2.0504e+00 (2.0479e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/106]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7351e+00 (1.7351e+00)\tInconsistency Loss 1.9359e-01 (1.9359e-01)\tEntropy 2.0561e+00 (2.0561e+00)\n",
      "Epoch: [7][100/106]\tTotal Loss -2.3770e+00 (-2.3771e+00)\tConsistency Loss 1.7107e+00 (1.7187e+00)\tInconsistency Loss 1.9919e-01 (1.9763e-01)\tEntropy 2.0439e+00 (2.0479e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/106]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7262e+00 (1.7262e+00)\tInconsistency Loss 1.9597e-01 (1.9597e-01)\tEntropy 2.0517e+00 (2.0517e+00)\n",
      "Epoch: [8][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7182e+00)\tInconsistency Loss 1.9773e-01 (1.9772e-01)\tEntropy 2.0476e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [9][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9773e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [10][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [11][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [12][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/106]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "A-5\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 505/4493 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/A-5/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/10]\tLoss 1.0000e+00 (1.0000e+00)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/10]\tLoss 3.8376e-01 (3.8376e-01)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/10]\tLoss 1.4867e-01 (1.4867e-01)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/10]\tLoss 5.7590e-02 (5.7590e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/10]\tLoss 2.2308e-02 (2.2308e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/10]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/10]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/10]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/10]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/10]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/10]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/10]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/10]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/10]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/10]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/10]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/10]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/10]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/10]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/10]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/10]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/10]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/10]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/10]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/10]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/10]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/10]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/10]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/10]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/10]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/11]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/90]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries A-5\u001b[0m\n",
      "\u001b[32m-- Train samples size: 1010 - Test samples size: 4493\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/A-5/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][ 0/20]\tTotal Loss -2.3006e+00 (-2.3006e+00)\tConsistency Loss 2.2570e+00 (2.2570e+00)\tInconsistency Loss 1.1058e-01 (1.1058e-01)\tEntropy 2.2788e+00 (2.2788e+00)\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][ 0/20]\tTotal Loss -2.3117e+00 (-2.3117e+00)\tConsistency Loss 2.1730e+00 (2.1730e+00)\tInconsistency Loss 1.2236e-01 (1.2236e-01)\tEntropy 2.2423e+00 (2.2423e+00)\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][ 0/20]\tTotal Loss -2.3801e+00 (-2.3801e+00)\tConsistency Loss 1.5987e+00 (1.5987e+00)\tInconsistency Loss 2.1685e-01 (2.1685e-01)\tEntropy 1.9894e+00 (1.9894e+00)\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][ 0/20]\tTotal Loss -2.3869e+00 (-2.3869e+00)\tConsistency Loss 1.7159e+00 (1.7159e+00)\tInconsistency Loss 1.9556e-01 (1.9556e-01)\tEntropy 2.0514e+00 (2.0514e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][ 0/20]\tTotal Loss -2.3509e+00 (-2.3509e+00)\tConsistency Loss 1.7901e+00 (1.7901e+00)\tInconsistency Loss 1.9902e-01 (1.9902e-01)\tEntropy 2.0705e+00 (2.0705e+00)\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][ 0/20]\tTotal Loss -2.3801e+00 (-2.3801e+00)\tConsistency Loss 1.6877e+00 (1.6877e+00)\tInconsistency Loss 2.0252e-01 (2.0252e-01)\tEntropy 2.0339e+00 (2.0339e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][ 0/20]\tTotal Loss -2.4005e+00 (-2.4005e+00)\tConsistency Loss 1.7493e+00 (1.7493e+00)\tInconsistency Loss 1.8769e-01 (1.8769e-01)\tEntropy 2.0749e+00 (2.0749e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][ 0/20]\tTotal Loss -2.3783e+00 (-2.3783e+00)\tConsistency Loss 1.7468e+00 (1.7468e+00)\tInconsistency Loss 1.9145e-01 (1.9145e-01)\tEntropy 2.0626e+00 (2.0626e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][ 0/20]\tTotal Loss -2.4041e+00 (-2.4041e+00)\tConsistency Loss 1.7025e+00 (1.7025e+00)\tInconsistency Loss 1.9693e-01 (1.9693e-01)\tEntropy 2.0533e+00 (2.0533e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][ 0/20]\tTotal Loss -2.3670e+00 (-2.3670e+00)\tConsistency Loss 1.7404e+00 (1.7404e+00)\tInconsistency Loss 2.0046e-01 (2.0046e-01)\tEntropy 2.0537e+00 (2.0537e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][ 0/20]\tTotal Loss -2.3799e+00 (-2.3799e+00)\tConsistency Loss 1.7452e+00 (1.7452e+00)\tInconsistency Loss 1.9315e-01 (1.9315e-01)\tEntropy 2.0626e+00 (2.0626e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][ 0/20]\tTotal Loss -2.3744e+00 (-2.3744e+00)\tConsistency Loss 1.7260e+00 (1.7260e+00)\tInconsistency Loss 1.9650e-01 (1.9650e-01)\tEntropy 2.0502e+00 (2.0502e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][ 0/20]\tTotal Loss -2.3741e+00 (-2.3741e+00)\tConsistency Loss 1.7272e+00 (1.7272e+00)\tInconsistency Loss 1.9644e-01 (1.9644e-01)\tEntropy 2.0506e+00 (2.0506e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][ 0/20]\tTotal Loss -2.3823e+00 (-2.3823e+00)\tConsistency Loss 1.6998e+00 (1.6998e+00)\tInconsistency Loss 1.9980e-01 (1.9980e-01)\tEntropy 2.0410e+00 (2.0410e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][ 0/20]\tTotal Loss -2.3833e+00 (-2.3833e+00)\tConsistency Loss 1.7329e+00 (1.7329e+00)\tInconsistency Loss 1.9303e-01 (1.9303e-01)\tEntropy 2.0581e+00 (2.0581e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][ 0/20]\tTotal Loss -2.3836e+00 (-2.3836e+00)\tConsistency Loss 1.7371e+00 (1.7371e+00)\tInconsistency Loss 1.9372e-01 (1.9372e-01)\tEntropy 2.0603e+00 (2.0603e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][ 0/20]\tTotal Loss -2.3774e+00 (-2.3774e+00)\tConsistency Loss 1.7381e+00 (1.7381e+00)\tInconsistency Loss 1.9127e-01 (1.9127e-01)\tEntropy 2.0578e+00 (2.0578e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][ 0/20]\tTotal Loss -2.3688e+00 (-2.3688e+00)\tConsistency Loss 1.7430e+00 (1.7430e+00)\tInconsistency Loss 1.9481e-01 (1.9481e-01)\tEntropy 2.0559e+00 (2.0559e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][ 0/20]\tTotal Loss -2.3802e+00 (-2.3802e+00)\tConsistency Loss 1.7026e+00 (1.7026e+00)\tInconsistency Loss 1.9981e-01 (1.9981e-01)\tEntropy 2.0414e+00 (2.0414e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][ 0/20]\tTotal Loss -2.3872e+00 (-2.3872e+00)\tConsistency Loss 1.7147e+00 (1.7147e+00)\tInconsistency Loss 1.9543e-01 (1.9543e-01)\tEntropy 2.0510e+00 (2.0510e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][ 0/20]\tTotal Loss -2.3809e+00 (-2.3809e+00)\tConsistency Loss 1.7186e+00 (1.7186e+00)\tInconsistency Loss 1.9669e-01 (1.9669e-01)\tEntropy 2.0498e+00 (2.0498e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][ 0/20]\tTotal Loss -2.3883e+00 (-2.3883e+00)\tConsistency Loss 1.7220e+00 (1.7220e+00)\tInconsistency Loss 1.9462e-01 (1.9462e-01)\tEntropy 2.0551e+00 (2.0551e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][ 0/20]\tTotal Loss -2.3763e+00 (-2.3763e+00)\tConsistency Loss 1.7392e+00 (1.7392e+00)\tInconsistency Loss 1.9205e-01 (1.9205e-01)\tEntropy 2.0578e+00 (2.0578e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][ 0/20]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7234e+00 (1.7234e+00)\tInconsistency Loss 1.9645e-01 (1.9645e-01)\tEntropy 2.0504e+00 (2.0504e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][ 0/20]\tTotal Loss -2.3796e+00 (-2.3796e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9764e-01 (1.9764e-01)\tEntropy 2.0488e+00 (2.0488e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][ 0/20]\tTotal Loss -2.3784e+00 (-2.3784e+00)\tConsistency Loss 1.7412e+00 (1.7412e+00)\tInconsistency Loss 1.9236e-01 (1.9236e-01)\tEntropy 2.0598e+00 (2.0598e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][ 0/20]\tTotal Loss -2.3740e+00 (-2.3740e+00)\tConsistency Loss 1.7420e+00 (1.7420e+00)\tInconsistency Loss 1.9378e-01 (1.9378e-01)\tEntropy 2.0580e+00 (2.0580e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][ 0/20]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7222e+00 (1.7222e+00)\tInconsistency Loss 1.9741e-01 (1.9741e-01)\tEntropy 2.0496e+00 (2.0496e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][ 0/20]\tTotal Loss -2.3796e+00 (-2.3796e+00)\tConsistency Loss 1.7268e+00 (1.7268e+00)\tInconsistency Loss 1.9537e-01 (1.9537e-01)\tEntropy 2.0532e+00 (2.0532e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][ 0/20]\tTotal Loss -2.3808e+00 (-2.3808e+00)\tConsistency Loss 1.7173e+00 (1.7173e+00)\tInconsistency Loss 1.9750e-01 (1.9750e-01)\tEntropy 2.0490e+00 (2.0490e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][ 0/20]\tTotal Loss -2.3755e+00 (-2.3755e+00)\tConsistency Loss 1.7252e+00 (1.7252e+00)\tInconsistency Loss 1.9570e-01 (1.9570e-01)\tEntropy 2.0504e+00 (2.0504e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][ 0/20]\tTotal Loss -2.3808e+00 (-2.3808e+00)\tConsistency Loss 1.7254e+00 (1.7254e+00)\tInconsistency Loss 1.9567e-01 (1.9567e-01)\tEntropy 2.0531e+00 (2.0531e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][ 0/20]\tTotal Loss -2.3807e+00 (-2.3807e+00)\tConsistency Loss 1.7199e+00 (1.7199e+00)\tInconsistency Loss 1.9673e-01 (1.9673e-01)\tEntropy 2.0503e+00 (2.0503e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][ 0/20]\tTotal Loss -2.3778e+00 (-2.3778e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9670e-01 (1.9670e-01)\tEntropy 2.0479e+00 (2.0479e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][ 0/20]\tTotal Loss -2.3803e+00 (-2.3803e+00)\tConsistency Loss 1.7088e+00 (1.7088e+00)\tInconsistency Loss 1.9848e-01 (1.9848e-01)\tEntropy 2.0446e+00 (2.0446e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][ 0/20]\tTotal Loss -2.3763e+00 (-2.3763e+00)\tConsistency Loss 1.7217e+00 (1.7217e+00)\tInconsistency Loss 1.9692e-01 (1.9692e-01)\tEntropy 2.0490e+00 (2.0490e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][ 0/20]\tTotal Loss -2.3768e+00 (-2.3768e+00)\tConsistency Loss 1.7207e+00 (1.7207e+00)\tInconsistency Loss 1.9726e-01 (1.9726e-01)\tEntropy 2.0487e+00 (2.0487e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][ 0/20]\tTotal Loss -2.3767e+00 (-2.3767e+00)\tConsistency Loss 1.7212e+00 (1.7212e+00)\tInconsistency Loss 1.9738e-01 (1.9738e-01)\tEntropy 2.0490e+00 (2.0490e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][ 0/20]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7201e+00 (1.7201e+00)\tInconsistency Loss 1.9736e-01 (1.9736e-01)\tEntropy 2.0486e+00 (2.0486e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][ 0/20]\tTotal Loss -2.3775e+00 (-2.3775e+00)\tConsistency Loss 1.7204e+00 (1.7204e+00)\tInconsistency Loss 1.9724e-01 (1.9724e-01)\tEntropy 2.0489e+00 (2.0489e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][ 0/20]\tTotal Loss -2.3767e+00 (-2.3767e+00)\tConsistency Loss 1.7179e+00 (1.7179e+00)\tInconsistency Loss 1.9791e-01 (1.9791e-01)\tEntropy 2.0473e+00 (2.0473e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][ 0/20]\tTotal Loss -2.3785e+00 (-2.3785e+00)\tConsistency Loss 1.7171e+00 (1.7171e+00)\tInconsistency Loss 1.9752e-01 (1.9752e-01)\tEntropy 2.0478e+00 (2.0478e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][ 0/20]\tTotal Loss -2.3787e+00 (-2.3787e+00)\tConsistency Loss 1.7221e+00 (1.7221e+00)\tInconsistency Loss 1.9661e-01 (1.9661e-01)\tEntropy 2.0504e+00 (2.0504e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][ 0/20]\tTotal Loss -2.3750e+00 (-2.3750e+00)\tConsistency Loss 1.7286e+00 (1.7286e+00)\tInconsistency Loss 1.9559e-01 (1.9559e-01)\tEntropy 2.0518e+00 (2.0518e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][ 0/20]\tTotal Loss -2.3803e+00 (-2.3803e+00)\tConsistency Loss 1.7276e+00 (1.7276e+00)\tInconsistency Loss 1.9524e-01 (1.9524e-01)\tEntropy 2.0539e+00 (2.0539e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][ 0/20]\tTotal Loss -2.3765e+00 (-2.3765e+00)\tConsistency Loss 1.7214e+00 (1.7214e+00)\tInconsistency Loss 1.9715e-01 (1.9715e-01)\tEntropy 2.0489e+00 (2.0489e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][ 0/20]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7196e+00 (1.7196e+00)\tInconsistency Loss 1.9773e-01 (1.9773e-01)\tEntropy 2.0483e+00 (2.0483e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][ 0/20]\tTotal Loss -2.3799e+00 (-2.3799e+00)\tConsistency Loss 1.7175e+00 (1.7175e+00)\tInconsistency Loss 1.9808e-01 (1.9808e-01)\tEntropy 2.0487e+00 (2.0487e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][ 0/20]\tTotal Loss -2.3742e+00 (-2.3742e+00)\tConsistency Loss 1.7151e+00 (1.7151e+00)\tInconsistency Loss 1.9849e-01 (1.9849e-01)\tEntropy 2.0446e+00 (2.0446e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][ 0/20]\tTotal Loss -2.3786e+00 (-2.3786e+00)\tConsistency Loss 1.7163e+00 (1.7163e+00)\tInconsistency Loss 1.9775e-01 (1.9775e-01)\tEntropy 2.0475e+00 (2.0475e+00)\n",
      "A-6\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 482/4253 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/A-6/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][0/9]\tLoss 1.0081e+00 (1.0081e+00)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][0/9]\tLoss 4.2968e-01 (4.2968e-01)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][0/9]\tLoss 1.8480e-01 (1.8480e-01)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][0/9]\tLoss 7.9575e-02 (7.9575e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][0/9]\tLoss 3.4152e-02 (3.4152e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][0/9]\tLoss 1.4703e-02 (1.4703e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][0/9]\tLoss 1.0037e-02 (1.0037e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][0/9]\tLoss 1.0039e-02 (1.0039e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][0/9]\tLoss 1.0045e-02 (1.0045e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][0/9]\tLoss 1.0037e-02 (1.0037e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][0/9]\tLoss 1.0030e-02 (1.0030e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][0/9]\tLoss 1.0049e-02 (1.0049e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][0/9]\tLoss 1.0058e-02 (1.0058e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][0/9]\tLoss 1.0037e-02 (1.0037e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][0/9]\tLoss 9.9674e-03 (9.9674e-03)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][0/9]\tLoss 1.0053e-02 (1.0053e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][0/9]\tLoss 1.0039e-02 (1.0039e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][0/9]\tLoss 1.0022e-02 (1.0022e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][0/9]\tLoss 1.0234e-02 (1.0234e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][0/9]\tLoss 9.9761e-03 (9.9761e-03)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][0/9]\tLoss 1.0037e-02 (1.0037e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][0/9]\tLoss 1.0019e-02 (1.0019e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][0/9]\tLoss 1.0020e-02 (1.0020e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][0/9]\tLoss 1.0029e-02 (1.0029e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][0/9]\tLoss 1.0014e-02 (1.0014e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][0/9]\tLoss 1.0016e-02 (1.0016e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][0/9]\tLoss 1.0014e-02 (1.0014e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][0/9]\tLoss 1.0011e-02 (1.0011e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][0/9]\tLoss 1.0018e-02 (1.0018e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][0/9]\tLoss 1.0015e-02 (1.0015e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/10]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/86]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries A-6\u001b[0m\n",
      "\u001b[32m-- Train samples size: 964 - Test samples size: 4253\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/A-6/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][ 0/19]\tTotal Loss -2.3042e+00 (-2.3042e+00)\tConsistency Loss 2.2555e+00 (2.2555e+00)\tInconsistency Loss 1.1048e-01 (1.1048e-01)\tEntropy 2.2798e+00 (2.2798e+00)\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][ 0/19]\tTotal Loss -2.3661e+00 (-2.3661e+00)\tConsistency Loss 1.8252e+00 (1.8252e+00)\tInconsistency Loss 1.7309e-01 (1.7309e-01)\tEntropy 2.0957e+00 (2.0957e+00)\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][ 0/19]\tTotal Loss -2.3759e+00 (-2.3759e+00)\tConsistency Loss 1.7768e+00 (1.7768e+00)\tInconsistency Loss 1.8452e-01 (1.8452e-01)\tEntropy 2.0763e+00 (2.0763e+00)\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][ 0/19]\tTotal Loss -2.3693e+00 (-2.3693e+00)\tConsistency Loss 1.7323e+00 (1.7323e+00)\tInconsistency Loss 1.9568e-01 (1.9568e-01)\tEntropy 2.0508e+00 (2.0508e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][ 0/19]\tTotal Loss -2.3764e+00 (-2.3764e+00)\tConsistency Loss 1.7278e+00 (1.7278e+00)\tInconsistency Loss 1.9504e-01 (1.9504e-01)\tEntropy 2.0521e+00 (2.0521e+00)\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][ 0/19]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7178e+00 (1.7178e+00)\tInconsistency Loss 1.9711e-01 (1.9711e-01)\tEntropy 2.0474e+00 (2.0474e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][ 0/19]\tTotal Loss -2.4194e+00 (-2.4194e+00)\tConsistency Loss 1.8474e+00 (1.8474e+00)\tInconsistency Loss 1.5572e-01 (1.5572e-01)\tEntropy 2.1334e+00 (2.1334e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][ 0/19]\tTotal Loss -2.6930e+00 (-2.6930e+00)\tConsistency Loss 1.6231e+00 (1.6231e+00)\tInconsistency Loss 1.1812e-01 (1.1812e-01)\tEntropy 2.1581e+00 (2.1581e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][ 0/19]\tTotal Loss -2.6834e+00 (-2.6834e+00)\tConsistency Loss 1.6599e+00 (1.6599e+00)\tInconsistency Loss 1.0020e-01 (1.0020e-01)\tEntropy 2.1716e+00 (2.1716e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][ 0/19]\tTotal Loss -2.7060e+00 (-2.7060e+00)\tConsistency Loss 1.6415e+00 (1.6415e+00)\tInconsistency Loss 1.1116e-01 (1.1116e-01)\tEntropy 2.1737e+00 (2.1737e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][ 0/19]\tTotal Loss -2.6854e+00 (-2.6854e+00)\tConsistency Loss 1.6153e+00 (1.6153e+00)\tInconsistency Loss 1.1748e-01 (1.1748e-01)\tEntropy 2.1503e+00 (2.1503e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][ 0/19]\tTotal Loss -2.5684e+00 (-2.5684e+00)\tConsistency Loss 1.7657e+00 (1.7657e+00)\tInconsistency Loss 1.1393e-01 (1.1393e-01)\tEntropy 2.1671e+00 (2.1671e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][ 0/19]\tTotal Loss -2.5826e+00 (-2.5826e+00)\tConsistency Loss 1.7309e+00 (1.7309e+00)\tInconsistency Loss 1.2083e-01 (1.2083e-01)\tEntropy 2.1568e+00 (2.1568e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][ 0/19]\tTotal Loss -2.7094e+00 (-2.7094e+00)\tConsistency Loss 1.6560e+00 (1.6560e+00)\tInconsistency Loss 1.1030e-01 (1.1030e-01)\tEntropy 2.1827e+00 (2.1827e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][ 0/19]\tTotal Loss -2.7707e+00 (-2.7707e+00)\tConsistency Loss 1.6245e+00 (1.6245e+00)\tInconsistency Loss 1.0180e-01 (1.0180e-01)\tEntropy 2.1976e+00 (2.1976e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][ 0/19]\tTotal Loss -2.6670e+00 (-2.6670e+00)\tConsistency Loss 1.7445e+00 (1.7445e+00)\tInconsistency Loss 1.1217e-01 (1.1217e-01)\tEntropy 2.2058e+00 (2.2058e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][ 0/19]\tTotal Loss -2.6906e+00 (-2.6906e+00)\tConsistency Loss 1.6863e+00 (1.6863e+00)\tInconsistency Loss 1.1229e-01 (1.1229e-01)\tEntropy 2.1884e+00 (2.1884e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][ 0/19]\tTotal Loss -2.7719e+00 (-2.7719e+00)\tConsistency Loss 1.5867e+00 (1.5867e+00)\tInconsistency Loss 1.1070e-01 (1.1070e-01)\tEntropy 2.1793e+00 (2.1793e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][ 0/19]\tTotal Loss -2.7243e+00 (-2.7243e+00)\tConsistency Loss 1.6527e+00 (1.6527e+00)\tInconsistency Loss 1.2632e-01 (1.2632e-01)\tEntropy 2.1885e+00 (2.1885e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][ 0/19]\tTotal Loss -2.7159e+00 (-2.7159e+00)\tConsistency Loss 1.6439e+00 (1.6439e+00)\tInconsistency Loss 1.0281e-01 (1.0281e-01)\tEntropy 2.1799e+00 (2.1799e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][ 0/19]\tTotal Loss -2.7838e+00 (-2.7838e+00)\tConsistency Loss 1.5790e+00 (1.5790e+00)\tInconsistency Loss 1.0876e-01 (1.0876e-01)\tEntropy 2.1814e+00 (2.1814e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][ 0/19]\tTotal Loss -2.7707e+00 (-2.7707e+00)\tConsistency Loss 1.5522e+00 (1.5522e+00)\tInconsistency Loss 1.3208e-01 (1.3208e-01)\tEntropy 2.1614e+00 (2.1614e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][ 0/19]\tTotal Loss -2.7298e+00 (-2.7298e+00)\tConsistency Loss 1.5774e+00 (1.5774e+00)\tInconsistency Loss 1.0324e-01 (1.0324e-01)\tEntropy 2.1536e+00 (2.1536e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][ 0/19]\tTotal Loss -2.7396e+00 (-2.7396e+00)\tConsistency Loss 1.6600e+00 (1.6600e+00)\tInconsistency Loss 1.2492e-01 (1.2492e-01)\tEntropy 2.1998e+00 (2.1998e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][ 0/19]\tTotal Loss -2.7189e+00 (-2.7189e+00)\tConsistency Loss 1.5822e+00 (1.5822e+00)\tInconsistency Loss 1.1335e-01 (1.1335e-01)\tEntropy 2.1506e+00 (2.1506e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][ 0/19]\tTotal Loss -2.8112e+00 (-2.8112e+00)\tConsistency Loss 1.5237e+00 (1.5237e+00)\tInconsistency Loss 1.2738e-01 (1.2738e-01)\tEntropy 2.1675e+00 (2.1675e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][ 0/19]\tTotal Loss -2.8027e+00 (-2.8027e+00)\tConsistency Loss 1.4942e+00 (1.4942e+00)\tInconsistency Loss 1.1303e-01 (1.1303e-01)\tEntropy 2.1485e+00 (2.1485e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][ 0/19]\tTotal Loss -2.8395e+00 (-2.8395e+00)\tConsistency Loss 1.5087e+00 (1.5087e+00)\tInconsistency Loss 1.1915e-01 (1.1915e-01)\tEntropy 2.1741e+00 (2.1741e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][ 0/19]\tTotal Loss -2.8220e+00 (-2.8220e+00)\tConsistency Loss 1.5676e+00 (1.5676e+00)\tInconsistency Loss 1.1703e-01 (1.1703e-01)\tEntropy 2.1948e+00 (2.1948e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][ 0/19]\tTotal Loss -2.7514e+00 (-2.7514e+00)\tConsistency Loss 1.6206e+00 (1.6206e+00)\tInconsistency Loss 1.0348e-01 (1.0348e-01)\tEntropy 2.1860e+00 (2.1860e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][ 0/19]\tTotal Loss -2.8077e+00 (-2.8077e+00)\tConsistency Loss 1.5993e+00 (1.5993e+00)\tInconsistency Loss 1.0524e-01 (1.0524e-01)\tEntropy 2.2035e+00 (2.2035e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][ 0/19]\tTotal Loss -2.6942e+00 (-2.6942e+00)\tConsistency Loss 1.6188e+00 (1.6188e+00)\tInconsistency Loss 1.2384e-01 (1.2384e-01)\tEntropy 2.1565e+00 (2.1565e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][ 0/19]\tTotal Loss -2.7633e+00 (-2.7633e+00)\tConsistency Loss 1.6246e+00 (1.6246e+00)\tInconsistency Loss 9.3989e-02 (9.3989e-02)\tEntropy 2.1940e+00 (2.1940e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][ 0/19]\tTotal Loss -2.7172e+00 (-2.7172e+00)\tConsistency Loss 1.6248e+00 (1.6248e+00)\tInconsistency Loss 1.1429e-01 (1.1429e-01)\tEntropy 2.1710e+00 (2.1710e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][ 0/19]\tTotal Loss -2.7190e+00 (-2.7190e+00)\tConsistency Loss 1.6239e+00 (1.6239e+00)\tInconsistency Loss 1.1894e-01 (1.1894e-01)\tEntropy 2.1714e+00 (2.1714e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][ 0/19]\tTotal Loss -2.7011e+00 (-2.7011e+00)\tConsistency Loss 1.6035e+00 (1.6035e+00)\tInconsistency Loss 1.0936e-01 (1.0936e-01)\tEntropy 2.1523e+00 (2.1523e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][ 0/19]\tTotal Loss -2.7089e+00 (-2.7089e+00)\tConsistency Loss 1.5779e+00 (1.5779e+00)\tInconsistency Loss 1.1527e-01 (1.1527e-01)\tEntropy 2.1434e+00 (2.1434e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][ 0/19]\tTotal Loss -2.7592e+00 (-2.7592e+00)\tConsistency Loss 1.4977e+00 (1.4977e+00)\tInconsistency Loss 1.2833e-01 (1.2833e-01)\tEntropy 2.1285e+00 (2.1285e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][ 0/19]\tTotal Loss -2.7614e+00 (-2.7614e+00)\tConsistency Loss 1.5240e+00 (1.5240e+00)\tInconsistency Loss 1.3747e-01 (1.3747e-01)\tEntropy 2.1427e+00 (2.1427e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][ 0/19]\tTotal Loss -2.7611e+00 (-2.7611e+00)\tConsistency Loss 1.6051e+00 (1.6051e+00)\tInconsistency Loss 1.1708e-01 (1.1708e-01)\tEntropy 2.1831e+00 (2.1831e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][ 0/19]\tTotal Loss -2.7861e+00 (-2.7861e+00)\tConsistency Loss 1.6500e+00 (1.6500e+00)\tInconsistency Loss 8.4018e-02 (8.4018e-02)\tEntropy 2.2180e+00 (2.2180e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][ 0/19]\tTotal Loss -2.6912e+00 (-2.6912e+00)\tConsistency Loss 1.6866e+00 (1.6866e+00)\tInconsistency Loss 1.1861e-01 (1.1861e-01)\tEntropy 2.1889e+00 (2.1889e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][ 0/19]\tTotal Loss -2.8221e+00 (-2.8221e+00)\tConsistency Loss 1.5915e+00 (1.5915e+00)\tInconsistency Loss 9.5009e-02 (9.5009e-02)\tEntropy 2.2068e+00 (2.2068e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][ 0/19]\tTotal Loss -2.7831e+00 (-2.7831e+00)\tConsistency Loss 1.5507e+00 (1.5507e+00)\tInconsistency Loss 1.2034e-01 (1.2034e-01)\tEntropy 2.1669e+00 (2.1669e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][ 0/19]\tTotal Loss -2.7950e+00 (-2.7950e+00)\tConsistency Loss 1.5953e+00 (1.5953e+00)\tInconsistency Loss 1.0371e-01 (1.0371e-01)\tEntropy 2.1952e+00 (2.1952e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][ 0/19]\tTotal Loss -2.7732e+00 (-2.7732e+00)\tConsistency Loss 1.5261e+00 (1.5261e+00)\tInconsistency Loss 1.2118e-01 (1.2118e-01)\tEntropy 2.1496e+00 (2.1496e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][ 0/19]\tTotal Loss -2.7934e+00 (-2.7934e+00)\tConsistency Loss 1.5170e+00 (1.5170e+00)\tInconsistency Loss 1.3672e-01 (1.3672e-01)\tEntropy 2.1552e+00 (2.1552e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][ 0/19]\tTotal Loss -2.4700e+00 (-2.4700e+00)\tConsistency Loss 1.9340e+00 (1.9340e+00)\tInconsistency Loss 1.3091e-01 (1.3091e-01)\tEntropy 2.2020e+00 (2.2020e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][ 0/19]\tTotal Loss -2.7134e+00 (-2.7134e+00)\tConsistency Loss 1.6699e+00 (1.6699e+00)\tInconsistency Loss 1.1001e-01 (1.1001e-01)\tEntropy 2.1917e+00 (2.1917e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][ 0/19]\tTotal Loss -2.7696e+00 (-2.7696e+00)\tConsistency Loss 1.5698e+00 (1.5698e+00)\tInconsistency Loss 1.1342e-01 (1.1342e-01)\tEntropy 2.1697e+00 (2.1697e+00)\n",
      "A-7\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2679/8431 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/A-7/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0067e+00 (1.0067e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4728e-01 (6.2516e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2108e-01 (4.2456e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2014e-02 (3.1045e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4654e-02 (2.4073e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0079e-02 (1.9561e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0047e-02 (1.0047e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0029e-02 (1.0062e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0001e-02 (1.0053e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0085e-02 (1.0059e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0025e-02 (1.0056e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0049e-02 (1.0049e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0048e-02 (1.0048e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0047e-02 (1.0031e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0009e-02 (1.0006e-02)\n",
      "Epoch: [3][30/53]\tLoss 9.9520e-03 (1.0006e-02)\n",
      "Epoch: [3][40/53]\tLoss 9.9951e-03 (1.0013e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0017e-02 (1.0014e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0008e-02 (1.0008e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0114e-02 (1.0019e-02)\n",
      "Epoch: [4][20/53]\tLoss 9.9855e-03 (1.0016e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0174e-02 (1.0022e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0012e-02 (1.0014e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0065e-02 (1.0017e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0028e-02 (1.0028e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0014e-02 (1.0011e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0009e-02 (1.0005e-02)\n",
      "Epoch: [5][40/53]\tLoss 9.9612e-03 (1.0006e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0002e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0022e-02 (1.0022e-02)\n",
      "Epoch: [6][10/53]\tLoss 9.9960e-03 (1.0011e-02)\n",
      "Epoch: [6][20/53]\tLoss 9.9913e-03 (1.0009e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0014e-02 (1.0013e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0025e-02 (1.0012e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0009e-02 (1.0011e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 9.9964e-03 (9.9964e-03)\n",
      "Epoch: [7][10/53]\tLoss 1.0012e-02 (1.0009e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0035e-02 (9.9995e-03)\n",
      "Epoch: [7][30/53]\tLoss 1.0007e-02 (9.9995e-03)\n",
      "Epoch: [7][40/53]\tLoss 1.0039e-02 (1.0003e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0008e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0010e-02 (1.0010e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0021e-02 (1.0003e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0007e-02 (1.0001e-02)\n",
      "Epoch: [8][40/53]\tLoss 9.9738e-03 (1.0001e-02)\n",
      "Epoch: [8][50/53]\tLoss 9.9954e-03 (1.0003e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0011e-02 (1.0008e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0006e-02 (9.9981e-03)\n",
      "Epoch: [9][30/53]\tLoss 1.0009e-02 (1.0002e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0042e-02 (1.0003e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][10/53]\tLoss 9.9964e-03 (1.0007e-02)\n",
      "Epoch: [10][20/53]\tLoss 9.9972e-03 (1.0005e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0005e-02 (1.0003e-02)\n",
      "Epoch: [10][40/53]\tLoss 9.9688e-03 (1.0005e-02)\n",
      "Epoch: [10][50/53]\tLoss 9.9981e-03 (1.0005e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [11][10/53]\tLoss 9.9838e-03 (1.0005e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0012e-02 (1.0004e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0023e-02 (1.0004e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0017e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 9.9740e-03 (9.9740e-03)\n",
      "Epoch: [12][10/53]\tLoss 9.9964e-03 (9.9970e-03)\n",
      "Epoch: [12][20/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [12][30/53]\tLoss 9.9912e-03 (1.0001e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0013e-02 (9.9994e-03)\n",
      "Epoch: [12][50/53]\tLoss 1.0001e-02 (9.9968e-03)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [13][10/53]\tLoss 9.9944e-03 (9.9880e-03)\n",
      "Epoch: [13][20/53]\tLoss 9.9956e-03 (9.9976e-03)\n",
      "Epoch: [13][30/53]\tLoss 9.9995e-03 (9.9963e-03)\n",
      "Epoch: [13][40/53]\tLoss 1.0004e-02 (9.9969e-03)\n",
      "Epoch: [13][50/53]\tLoss 1.0002e-02 (9.9987e-03)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 9.9909e-03 (9.9909e-03)\n",
      "Epoch: [14][10/53]\tLoss 9.9897e-03 (1.0000e-02)\n",
      "Epoch: [14][20/53]\tLoss 9.9865e-03 (1.0001e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0007e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0010e-02 (1.0004e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [15][30/53]\tLoss 9.9989e-03 (1.0005e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0007e-02 (1.0001e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 9.9905e-03 (9.9905e-03)\n",
      "Epoch: [16][10/53]\tLoss 1.0007e-02 (1.0002e-02)\n",
      "Epoch: [16][20/53]\tLoss 9.9898e-03 (1.0002e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0006e-02 (9.9998e-03)\n",
      "Epoch: [16][50/53]\tLoss 1.0006e-02 (9.9992e-03)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0011e-02 (1.0002e-02)\n",
      "Epoch: [17][20/53]\tLoss 9.9949e-03 (1.0004e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0040e-02 (1.0001e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0031e-02 (1.0003e-02)\n",
      "Epoch: [17][50/53]\tLoss 9.9947e-03 (1.0003e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 9.9642e-03 (9.9642e-03)\n",
      "Epoch: [18][10/53]\tLoss 1.0017e-02 (9.9950e-03)\n",
      "Epoch: [18][20/53]\tLoss 1.0011e-02 (9.9962e-03)\n",
      "Epoch: [18][30/53]\tLoss 9.9870e-03 (9.9968e-03)\n",
      "Epoch: [18][40/53]\tLoss 9.9735e-03 (9.9962e-03)\n",
      "Epoch: [18][50/53]\tLoss 9.9919e-03 (9.9982e-03)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0008e-02 (1.0008e-02)\n",
      "Epoch: [19][10/53]\tLoss 9.9798e-03 (9.9983e-03)\n",
      "Epoch: [19][20/53]\tLoss 1.0003e-02 (9.9972e-03)\n",
      "Epoch: [19][30/53]\tLoss 9.9598e-03 (9.9966e-03)\n",
      "Epoch: [19][40/53]\tLoss 9.9918e-03 (9.9982e-03)\n",
      "Epoch: [19][50/53]\tLoss 1.0002e-02 (9.9990e-03)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0008e-02 (1.0008e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0008e-02 (9.9999e-03)\n",
      "Epoch: [20][20/53]\tLoss 1.0028e-02 (9.9972e-03)\n",
      "Epoch: [20][30/53]\tLoss 9.9948e-03 (9.9983e-03)\n",
      "Epoch: [20][40/53]\tLoss 9.9893e-03 (9.9947e-03)\n",
      "Epoch: [20][50/53]\tLoss 9.9938e-03 (9.9951e-03)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [21][10/53]\tLoss 9.9963e-03 (9.9893e-03)\n",
      "Epoch: [21][20/53]\tLoss 1.0020e-02 (9.9991e-03)\n",
      "Epoch: [21][30/53]\tLoss 1.0004e-02 (1.0000e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0015e-02 (9.9992e-03)\n",
      "Epoch: [21][50/53]\tLoss 9.9927e-03 (9.9985e-03)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0014e-02 (1.0014e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0005e-02 (9.9973e-03)\n",
      "Epoch: [22][20/53]\tLoss 9.9917e-03 (1.0000e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0002e-02 (9.9976e-03)\n",
      "Epoch: [22][40/53]\tLoss 9.9774e-03 (9.9960e-03)\n",
      "Epoch: [22][50/53]\tLoss 9.9930e-03 (9.9953e-03)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0036e-02 (1.0036e-02)\n",
      "Epoch: [23][10/53]\tLoss 9.9756e-03 (9.9974e-03)\n",
      "Epoch: [23][20/53]\tLoss 9.9998e-03 (9.9996e-03)\n",
      "Epoch: [23][30/53]\tLoss 9.9926e-03 (9.9960e-03)\n",
      "Epoch: [23][40/53]\tLoss 1.0002e-02 (9.9970e-03)\n",
      "Epoch: [23][50/53]\tLoss 1.0004e-02 (9.9989e-03)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0018e-02 (1.0018e-02)\n",
      "Epoch: [24][10/53]\tLoss 9.9848e-03 (9.9881e-03)\n",
      "Epoch: [24][20/53]\tLoss 1.0021e-02 (9.9897e-03)\n",
      "Epoch: [24][30/53]\tLoss 1.0003e-02 (9.9920e-03)\n",
      "Epoch: [24][40/53]\tLoss 1.0000e-02 (9.9947e-03)\n",
      "Epoch: [24][50/53]\tLoss 9.9604e-03 (9.9958e-03)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 9.9988e-03 (9.9988e-03)\n",
      "Epoch: [25][10/53]\tLoss 9.9969e-03 (9.9881e-03)\n",
      "Epoch: [25][20/53]\tLoss 9.9839e-03 (9.9963e-03)\n",
      "Epoch: [25][30/53]\tLoss 1.0007e-02 (9.9953e-03)\n",
      "Epoch: [25][40/53]\tLoss 9.9781e-03 (9.9946e-03)\n",
      "Epoch: [25][50/53]\tLoss 9.9910e-03 (9.9948e-03)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0003e-02 (1.0004e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0000e-02 (9.9952e-03)\n",
      "Epoch: [26][30/53]\tLoss 9.9955e-03 (9.9977e-03)\n",
      "Epoch: [26][40/53]\tLoss 9.9553e-03 (9.9911e-03)\n",
      "Epoch: [26][50/53]\tLoss 9.9898e-03 (9.9926e-03)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 9.9006e-03 (9.9006e-03)\n",
      "Epoch: [27][10/53]\tLoss 9.9673e-03 (9.9813e-03)\n",
      "Epoch: [27][20/53]\tLoss 9.9847e-03 (9.9894e-03)\n",
      "Epoch: [27][30/53]\tLoss 1.0003e-02 (9.9911e-03)\n",
      "Epoch: [27][40/53]\tLoss 9.9985e-03 (9.9919e-03)\n",
      "Epoch: [27][50/53]\tLoss 1.0001e-02 (9.9944e-03)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 9.9819e-03 (9.9819e-03)\n",
      "Epoch: [28][10/53]\tLoss 9.8774e-03 (9.9718e-03)\n",
      "Epoch: [28][20/53]\tLoss 1.0004e-02 (9.9861e-03)\n",
      "Epoch: [28][30/53]\tLoss 9.9829e-03 (9.9905e-03)\n",
      "Epoch: [28][40/53]\tLoss 1.0014e-02 (9.9901e-03)\n",
      "Epoch: [28][50/53]\tLoss 9.9547e-03 (9.9899e-03)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [29][10/53]\tLoss 9.9038e-03 (9.9605e-03)\n",
      "Epoch: [29][20/53]\tLoss 1.0035e-02 (9.9762e-03)\n",
      "Epoch: [29][30/53]\tLoss 1.0007e-02 (9.9843e-03)\n",
      "Epoch: [29][40/53]\tLoss 9.9804e-03 (9.9862e-03)\n",
      "Epoch: [29][50/53]\tLoss 1.0017e-02 (9.9890e-03)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 9.9814e-03 (9.9814e-03)\n",
      "Epoch: [30][10/53]\tLoss 1.0012e-02 (9.9979e-03)\n",
      "Epoch: [30][20/53]\tLoss 9.9212e-03 (9.9855e-03)\n",
      "Epoch: [30][30/53]\tLoss 9.9938e-03 (9.9890e-03)\n",
      "Epoch: [30][40/53]\tLoss 1.0003e-02 (9.9878e-03)\n",
      "Epoch: [30][50/53]\tLoss 1.0043e-02 (9.9901e-03)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/169]\n",
      "Fill TS Repository [100/169]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries A-7\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5358 - Test samples size: 8431\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/A-7/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/107]\tTotal Loss -2.3007e+00 (-2.3007e+00)\tConsistency Loss 2.2579e+00 (2.2579e+00)\tInconsistency Loss 1.1066e-01 (1.1066e-01)\tEntropy 2.2793e+00 (2.2793e+00)\n",
      "Epoch: [1][100/107]\tTotal Loss -2.3412e+00 (-2.3643e+00)\tConsistency Loss 1.7642e+00 (1.8217e+00)\tInconsistency Loss 1.9496e-01 (1.8101e-01)\tEntropy 2.0527e+00 (2.0930e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/107]\tTotal Loss -2.3392e+00 (-2.3392e+00)\tConsistency Loss 1.5899e+00 (1.5899e+00)\tInconsistency Loss 2.2966e-01 (2.2966e-01)\tEntropy 1.9646e+00 (1.9646e+00)\n",
      "Epoch: [2][100/107]\tTotal Loss -2.3602e+00 (-2.3753e+00)\tConsistency Loss 1.7176e+00 (1.7256e+00)\tInconsistency Loss 2.0180e-01 (1.9750e-01)\tEntropy 2.0389e+00 (2.0505e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/107]\tTotal Loss -2.3835e+00 (-2.3835e+00)\tConsistency Loss 1.7765e+00 (1.7765e+00)\tInconsistency Loss 1.8705e-01 (1.8705e-01)\tEntropy 2.0800e+00 (2.0800e+00)\n",
      "Epoch: [3][100/107]\tTotal Loss -2.3594e+00 (-2.3771e+00)\tConsistency Loss 1.7615e+00 (1.7302e+00)\tInconsistency Loss 1.9254e-01 (1.9632e-01)\tEntropy 2.0605e+00 (2.0536e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/107]\tTotal Loss -2.4034e+00 (-2.4034e+00)\tConsistency Loss 1.7434e+00 (1.7434e+00)\tInconsistency Loss 1.9769e-01 (1.9769e-01)\tEntropy 2.0734e+00 (2.0734e+00)\n",
      "Epoch: [4][100/107]\tTotal Loss -2.3658e+00 (-2.3826e+00)\tConsistency Loss 1.6758e+00 (1.7273e+00)\tInconsistency Loss 2.1317e-01 (1.9690e-01)\tEntropy 2.0208e+00 (2.0550e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/107]\tTotal Loss -2.3839e+00 (-2.3839e+00)\tConsistency Loss 1.7453e+00 (1.7453e+00)\tInconsistency Loss 1.9346e-01 (1.9346e-01)\tEntropy 2.0646e+00 (2.0646e+00)\n",
      "Epoch: [5][100/107]\tTotal Loss -2.3966e+00 (-2.3805e+00)\tConsistency Loss 1.7001e+00 (1.7293e+00)\tInconsistency Loss 1.9674e-01 (1.9641e-01)\tEntropy 2.0483e+00 (2.0549e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7085e+00 (1.7085e+00)\tInconsistency Loss 2.0068e-01 (2.0068e-01)\tEntropy 2.0428e+00 (2.0428e+00)\n",
      "Epoch: [6][100/107]\tTotal Loss -2.3705e+00 (-2.3835e+00)\tConsistency Loss 1.7163e+00 (1.7273e+00)\tInconsistency Loss 1.9976e-01 (1.9700e-01)\tEntropy 2.0434e+00 (2.0554e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/107]\tTotal Loss -2.3737e+00 (-2.3737e+00)\tConsistency Loss 1.7441e+00 (1.7441e+00)\tInconsistency Loss 1.9848e-01 (1.9848e-01)\tEntropy 2.0589e+00 (2.0589e+00)\n",
      "Epoch: [7][100/107]\tTotal Loss -2.4324e+00 (-2.3844e+00)\tConsistency Loss 1.6878e+00 (1.7298e+00)\tInconsistency Loss 1.9366e-01 (1.9584e-01)\tEntropy 2.0601e+00 (2.0571e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/107]\tTotal Loss -2.3986e+00 (-2.3986e+00)\tConsistency Loss 1.7410e+00 (1.7410e+00)\tInconsistency Loss 1.9395e-01 (1.9395e-01)\tEntropy 2.0698e+00 (2.0698e+00)\n",
      "Epoch: [8][100/107]\tTotal Loss -2.3567e+00 (-2.3857e+00)\tConsistency Loss 1.7866e+00 (1.7227e+00)\tInconsistency Loss 1.8975e-01 (1.9932e-01)\tEntropy 2.0716e+00 (2.0542e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/107]\tTotal Loss -2.4153e+00 (-2.4153e+00)\tConsistency Loss 1.7508e+00 (1.7508e+00)\tInconsistency Loss 1.8634e-01 (1.8634e-01)\tEntropy 2.0831e+00 (2.0831e+00)\n",
      "Epoch: [9][100/107]\tTotal Loss -2.3704e+00 (-2.3848e+00)\tConsistency Loss 1.7240e+00 (1.7299e+00)\tInconsistency Loss 2.0943e-01 (1.9664e-01)\tEntropy 2.0472e+00 (2.0573e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/107]\tTotal Loss -2.3847e+00 (-2.3847e+00)\tConsistency Loss 1.7116e+00 (1.7116e+00)\tInconsistency Loss 1.9368e-01 (1.9368e-01)\tEntropy 2.0481e+00 (2.0481e+00)\n",
      "Epoch: [10][100/107]\tTotal Loss -2.3788e+00 (-2.3798e+00)\tConsistency Loss 1.7428e+00 (1.7276e+00)\tInconsistency Loss 1.9438e-01 (1.9679e-01)\tEntropy 2.0608e+00 (2.0537e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/107]\tTotal Loss -2.3667e+00 (-2.3667e+00)\tConsistency Loss 1.7414e+00 (1.7414e+00)\tInconsistency Loss 1.9622e-01 (1.9622e-01)\tEntropy 2.0540e+00 (2.0540e+00)\n",
      "Epoch: [11][100/107]\tTotal Loss -2.3977e+00 (-2.3856e+00)\tConsistency Loss 1.7294e+00 (1.7245e+00)\tInconsistency Loss 1.9067e-01 (1.9784e-01)\tEntropy 2.0635e+00 (2.0551e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/107]\tTotal Loss -2.3670e+00 (-2.3670e+00)\tConsistency Loss 1.7707e+00 (1.7707e+00)\tInconsistency Loss 1.8772e-01 (1.8772e-01)\tEntropy 2.0689e+00 (2.0689e+00)\n",
      "Epoch: [12][100/107]\tTotal Loss -2.3968e+00 (-2.3878e+00)\tConsistency Loss 1.7142e+00 (1.7277e+00)\tInconsistency Loss 1.9220e-01 (1.9704e-01)\tEntropy 2.0555e+00 (2.0578e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/107]\tTotal Loss -2.3778e+00 (-2.3778e+00)\tConsistency Loss 1.7443e+00 (1.7443e+00)\tInconsistency Loss 1.8932e-01 (1.8932e-01)\tEntropy 2.0610e+00 (2.0610e+00)\n",
      "Epoch: [13][100/107]\tTotal Loss -2.3790e+00 (-2.3889e+00)\tConsistency Loss 1.7069e+00 (1.7267e+00)\tInconsistency Loss 2.0494e-01 (1.9691e-01)\tEntropy 2.0429e+00 (2.0578e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/107]\tTotal Loss -2.4126e+00 (-2.4126e+00)\tConsistency Loss 1.7040e+00 (1.7040e+00)\tInconsistency Loss 1.9508e-01 (1.9508e-01)\tEntropy 2.0583e+00 (2.0583e+00)\n",
      "Epoch: [14][100/107]\tTotal Loss -2.3417e+00 (-2.3886e+00)\tConsistency Loss 1.8117e+00 (1.7284e+00)\tInconsistency Loss 1.8428e-01 (1.9672e-01)\tEntropy 2.0767e+00 (2.0585e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/107]\tTotal Loss -2.3452e+00 (-2.3452e+00)\tConsistency Loss 1.7491e+00 (1.7491e+00)\tInconsistency Loss 1.9397e-01 (1.9397e-01)\tEntropy 2.0471e+00 (2.0471e+00)\n",
      "Epoch: [15][100/107]\tTotal Loss -2.4113e+00 (-2.3864e+00)\tConsistency Loss 1.6913e+00 (1.7273e+00)\tInconsistency Loss 1.9081e-01 (1.9676e-01)\tEntropy 2.0513e+00 (2.0568e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/107]\tTotal Loss -2.4249e+00 (-2.4249e+00)\tConsistency Loss 1.7241e+00 (1.7241e+00)\tInconsistency Loss 1.8681e-01 (1.8681e-01)\tEntropy 2.0745e+00 (2.0745e+00)\n",
      "Epoch: [16][100/107]\tTotal Loss -2.3905e+00 (-2.3888e+00)\tConsistency Loss 1.7043e+00 (1.7276e+00)\tInconsistency Loss 2.0091e-01 (1.9599e-01)\tEntropy 2.0474e+00 (2.0582e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/107]\tTotal Loss -2.3673e+00 (-2.3673e+00)\tConsistency Loss 1.7727e+00 (1.7727e+00)\tInconsistency Loss 1.9350e-01 (1.9350e-01)\tEntropy 2.0700e+00 (2.0700e+00)\n",
      "Epoch: [17][100/107]\tTotal Loss -2.3697e+00 (-2.3867e+00)\tConsistency Loss 1.7679e+00 (1.7280e+00)\tInconsistency Loss 1.8888e-01 (1.9626e-01)\tEntropy 2.0688e+00 (2.0573e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/107]\tTotal Loss -2.4226e+00 (-2.4226e+00)\tConsistency Loss 1.6704e+00 (1.6704e+00)\tInconsistency Loss 1.9729e-01 (1.9729e-01)\tEntropy 2.0465e+00 (2.0465e+00)\n",
      "Epoch: [18][100/107]\tTotal Loss -2.3946e+00 (-2.3932e+00)\tConsistency Loss 1.7236e+00 (1.7237e+00)\tInconsistency Loss 1.9204e-01 (1.9720e-01)\tEntropy 2.0591e+00 (2.0584e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/107]\tTotal Loss -2.4078e+00 (-2.4078e+00)\tConsistency Loss 1.7004e+00 (1.7004e+00)\tInconsistency Loss 1.9873e-01 (1.9873e-01)\tEntropy 2.0541e+00 (2.0541e+00)\n",
      "Epoch: [19][100/107]\tTotal Loss -2.3852e+00 (-2.3899e+00)\tConsistency Loss 1.7088e+00 (1.7285e+00)\tInconsistency Loss 1.9927e-01 (1.9644e-01)\tEntropy 2.0470e+00 (2.0592e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/107]\tTotal Loss -2.4233e+00 (-2.4233e+00)\tConsistency Loss 1.7118e+00 (1.7118e+00)\tInconsistency Loss 1.8997e-01 (1.8997e-01)\tEntropy 2.0676e+00 (2.0676e+00)\n",
      "Epoch: [20][100/107]\tTotal Loss -2.3767e+00 (-2.3880e+00)\tConsistency Loss 1.7330e+00 (1.7303e+00)\tInconsistency Loss 1.9420e-01 (1.9522e-01)\tEntropy 2.0548e+00 (2.0592e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/107]\tTotal Loss -2.4298e+00 (-2.4298e+00)\tConsistency Loss 1.7083e+00 (1.7083e+00)\tInconsistency Loss 1.9437e-01 (1.9437e-01)\tEntropy 2.0690e+00 (2.0690e+00)\n",
      "Epoch: [21][100/107]\tTotal Loss -2.4201e+00 (-2.3910e+00)\tConsistency Loss 1.7236e+00 (1.7270e+00)\tInconsistency Loss 1.8639e-01 (1.9675e-01)\tEntropy 2.0719e+00 (2.0590e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/107]\tTotal Loss -2.3918e+00 (-2.3918e+00)\tConsistency Loss 1.7317e+00 (1.7317e+00)\tInconsistency Loss 1.9180e-01 (1.9180e-01)\tEntropy 2.0617e+00 (2.0617e+00)\n",
      "Epoch: [22][100/107]\tTotal Loss -2.4107e+00 (-2.3891e+00)\tConsistency Loss 1.7207e+00 (1.7271e+00)\tInconsistency Loss 1.9675e-01 (1.9641e-01)\tEntropy 2.0657e+00 (2.0581e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/107]\tTotal Loss -2.3577e+00 (-2.3577e+00)\tConsistency Loss 1.7852e+00 (1.7852e+00)\tInconsistency Loss 1.9045e-01 (1.9045e-01)\tEntropy 2.0715e+00 (2.0715e+00)\n",
      "Epoch: [23][100/107]\tTotal Loss -2.4127e+00 (-2.3906e+00)\tConsistency Loss 1.7218e+00 (1.7268e+00)\tInconsistency Loss 1.8630e-01 (1.9757e-01)\tEntropy 2.0673e+00 (2.0587e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/107]\tTotal Loss -2.3743e+00 (-2.3743e+00)\tConsistency Loss 1.6895e+00 (1.6895e+00)\tInconsistency Loss 2.0434e-01 (2.0434e-01)\tEntropy 2.0319e+00 (2.0319e+00)\n",
      "Epoch: [24][100/107]\tTotal Loss -2.3546e+00 (-2.3906e+00)\tConsistency Loss 1.7624e+00 (1.7259e+00)\tInconsistency Loss 1.9552e-01 (1.9675e-01)\tEntropy 2.0585e+00 (2.0583e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/107]\tTotal Loss -2.4191e+00 (-2.4191e+00)\tConsistency Loss 1.7178e+00 (1.7178e+00)\tInconsistency Loss 1.8866e-01 (1.8866e-01)\tEntropy 2.0684e+00 (2.0684e+00)\n",
      "Epoch: [25][100/107]\tTotal Loss -2.3792e+00 (-2.3892e+00)\tConsistency Loss 1.7176e+00 (1.7299e+00)\tInconsistency Loss 2.1111e-01 (1.9504e-01)\tEntropy 2.0484e+00 (2.0596e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/107]\tTotal Loss -2.3615e+00 (-2.3615e+00)\tConsistency Loss 1.7556e+00 (1.7556e+00)\tInconsistency Loss 1.9118e-01 (1.9118e-01)\tEntropy 2.0585e+00 (2.0585e+00)\n",
      "Epoch: [26][100/107]\tTotal Loss -2.3774e+00 (-2.3856e+00)\tConsistency Loss 1.7412e+00 (1.7266e+00)\tInconsistency Loss 1.9242e-01 (1.9673e-01)\tEntropy 2.0593e+00 (2.0561e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/107]\tTotal Loss -2.4036e+00 (-2.4036e+00)\tConsistency Loss 1.7201e+00 (1.7201e+00)\tInconsistency Loss 1.9630e-01 (1.9630e-01)\tEntropy 2.0618e+00 (2.0618e+00)\n",
      "Epoch: [27][100/107]\tTotal Loss -2.3474e+00 (-2.3937e+00)\tConsistency Loss 1.7883e+00 (1.7242e+00)\tInconsistency Loss 1.9419e-01 (1.9656e-01)\tEntropy 2.0679e+00 (2.0590e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/107]\tTotal Loss -2.3630e+00 (-2.3630e+00)\tConsistency Loss 1.7094e+00 (1.7094e+00)\tInconsistency Loss 2.0281e-01 (2.0281e-01)\tEntropy 2.0362e+00 (2.0362e+00)\n",
      "Epoch: [28][100/107]\tTotal Loss -2.4028e+00 (-2.3862e+00)\tConsistency Loss 1.7127e+00 (1.7319e+00)\tInconsistency Loss 1.9673e-01 (1.9645e-01)\tEntropy 2.0577e+00 (2.0590e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/107]\tTotal Loss -2.3618e+00 (-2.3618e+00)\tConsistency Loss 1.7358e+00 (1.7358e+00)\tInconsistency Loss 2.0331e-01 (2.0331e-01)\tEntropy 2.0488e+00 (2.0488e+00)\n",
      "Epoch: [29][100/107]\tTotal Loss -2.4044e+00 (-2.3901e+00)\tConsistency Loss 1.7274e+00 (1.7288e+00)\tInconsistency Loss 1.8888e-01 (1.9663e-01)\tEntropy 2.0659e+00 (2.0595e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/107]\tTotal Loss -2.3727e+00 (-2.3727e+00)\tConsistency Loss 1.7484e+00 (1.7484e+00)\tInconsistency Loss 2.0009e-01 (2.0009e-01)\tEntropy 2.0606e+00 (2.0606e+00)\n",
      "Epoch: [30][100/107]\tTotal Loss -2.4254e+00 (-2.3918e+00)\tConsistency Loss 1.7262e+00 (1.7233e+00)\tInconsistency Loss 1.8683e-01 (1.9751e-01)\tEntropy 2.0758e+00 (2.0575e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/107]\tTotal Loss -2.3740e+00 (-2.3740e+00)\tConsistency Loss 1.6418e+00 (1.6418e+00)\tInconsistency Loss 2.1576e-01 (2.1576e-01)\tEntropy 2.0079e+00 (2.0079e+00)\n",
      "Epoch: [31][100/107]\tTotal Loss -2.3756e+00 (-2.3765e+00)\tConsistency Loss 1.7175e+00 (1.7200e+00)\tInconsistency Loss 1.9805e-01 (1.9752e-01)\tEntropy 2.0465e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/107]\tTotal Loss -2.3776e+00 (-2.3776e+00)\tConsistency Loss 1.7204e+00 (1.7204e+00)\tInconsistency Loss 1.9733e-01 (1.9733e-01)\tEntropy 2.0490e+00 (2.0490e+00)\n",
      "Epoch: [32][100/107]\tTotal Loss -2.3769e+00 (-2.3773e+00)\tConsistency Loss 1.7214e+00 (1.7190e+00)\tInconsistency Loss 1.9694e-01 (1.9756e-01)\tEntropy 2.0491e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/107]\tTotal Loss -2.3769e+00 (-2.3769e+00)\tConsistency Loss 1.7185e+00 (1.7185e+00)\tInconsistency Loss 1.9777e-01 (1.9777e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "Epoch: [33][100/107]\tTotal Loss -2.3778e+00 (-2.3774e+00)\tConsistency Loss 1.7188e+00 (1.7191e+00)\tInconsistency Loss 1.9749e-01 (1.9756e-01)\tEntropy 2.0483e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/107]\tTotal Loss -2.3776e+00 (-2.3776e+00)\tConsistency Loss 1.7185e+00 (1.7185e+00)\tInconsistency Loss 1.9776e-01 (1.9776e-01)\tEntropy 2.0480e+00 (2.0480e+00)\n",
      "Epoch: [34][100/107]\tTotal Loss -2.3771e+00 (-2.3775e+00)\tConsistency Loss 1.7164e+00 (1.7189e+00)\tInconsistency Loss 1.9828e-01 (1.9759e-01)\tEntropy 2.0467e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/107]\tTotal Loss -2.3776e+00 (-2.3776e+00)\tConsistency Loss 1.7211e+00 (1.7211e+00)\tInconsistency Loss 1.9727e-01 (1.9727e-01)\tEntropy 2.0493e+00 (2.0493e+00)\n",
      "Epoch: [35][100/107]\tTotal Loss -2.3765e+00 (-2.3775e+00)\tConsistency Loss 1.7207e+00 (1.7189e+00)\tInconsistency Loss 1.9736e-01 (1.9761e-01)\tEntropy 2.0486e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/107]\tTotal Loss -2.3778e+00 (-2.3778e+00)\tConsistency Loss 1.7186e+00 (1.7186e+00)\tInconsistency Loss 1.9784e-01 (1.9784e-01)\tEntropy 2.0482e+00 (2.0482e+00)\n",
      "Epoch: [36][100/107]\tTotal Loss -2.3767e+00 (-2.3774e+00)\tConsistency Loss 1.7179e+00 (1.7188e+00)\tInconsistency Loss 1.9790e-01 (1.9766e-01)\tEntropy 2.0473e+00 (2.0481e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/107]\tTotal Loss -2.3776e+00 (-2.3776e+00)\tConsistency Loss 1.7200e+00 (1.7200e+00)\tInconsistency Loss 1.9724e-01 (1.9724e-01)\tEntropy 2.0488e+00 (2.0488e+00)\n",
      "Epoch: [37][100/107]\tTotal Loss -2.3789e+00 (-2.3773e+00)\tConsistency Loss 1.7189e+00 (1.7196e+00)\tInconsistency Loss 1.9752e-01 (1.9756e-01)\tEntropy 2.0489e+00 (2.0485e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/107]\tTotal Loss -2.3776e+00 (-2.3776e+00)\tConsistency Loss 1.7217e+00 (1.7217e+00)\tInconsistency Loss 1.9692e-01 (1.9692e-01)\tEntropy 2.0496e+00 (2.0496e+00)\n",
      "Epoch: [38][100/107]\tTotal Loss -2.3767e+00 (-2.3775e+00)\tConsistency Loss 1.7267e+00 (1.7199e+00)\tInconsistency Loss 1.9645e-01 (1.9748e-01)\tEntropy 2.0517e+00 (2.0487e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/107]\tTotal Loss -2.3702e+00 (-2.3702e+00)\tConsistency Loss 1.7091e+00 (1.7091e+00)\tInconsistency Loss 2.0156e-01 (2.0156e-01)\tEntropy 2.0397e+00 (2.0397e+00)\n",
      "Epoch: [39][100/107]\tTotal Loss -2.3797e+00 (-2.3780e+00)\tConsistency Loss 1.7074e+00 (1.7216e+00)\tInconsistency Loss 1.9944e-01 (1.9737e-01)\tEntropy 2.0435e+00 (2.0498e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/107]\tTotal Loss -2.3706e+00 (-2.3706e+00)\tConsistency Loss 1.7446e+00 (1.7446e+00)\tInconsistency Loss 1.9161e-01 (1.9161e-01)\tEntropy 2.0576e+00 (2.0576e+00)\n",
      "Epoch: [40][100/107]\tTotal Loss -2.3801e+00 (-2.3791e+00)\tConsistency Loss 1.7106e+00 (1.7250e+00)\tInconsistency Loss 2.0265e-01 (1.9715e-01)\tEntropy 2.0454e+00 (2.0521e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/107]\tTotal Loss -2.3814e+00 (-2.3814e+00)\tConsistency Loss 1.7330e+00 (1.7330e+00)\tInconsistency Loss 1.9561e-01 (1.9561e-01)\tEntropy 2.0572e+00 (2.0572e+00)\n",
      "Epoch: [41][100/107]\tTotal Loss -2.3706e+00 (-2.3795e+00)\tConsistency Loss 1.7048e+00 (1.7262e+00)\tInconsistency Loss 2.0245e-01 (1.9774e-01)\tEntropy 2.0377e+00 (2.0528e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/107]\tTotal Loss -2.3751e+00 (-2.3751e+00)\tConsistency Loss 1.7171e+00 (1.7171e+00)\tInconsistency Loss 2.0021e-01 (2.0021e-01)\tEntropy 2.0461e+00 (2.0461e+00)\n",
      "Epoch: [42][100/107]\tTotal Loss -2.3770e+00 (-2.3801e+00)\tConsistency Loss 1.7065e+00 (1.7234e+00)\tInconsistency Loss 2.0035e-01 (1.9865e-01)\tEntropy 2.0417e+00 (2.0517e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/107]\tTotal Loss -2.3760e+00 (-2.3760e+00)\tConsistency Loss 1.7232e+00 (1.7232e+00)\tInconsistency Loss 1.9696e-01 (1.9696e-01)\tEntropy 2.0496e+00 (2.0496e+00)\n",
      "Epoch: [43][100/107]\tTotal Loss -2.3809e+00 (-2.3775e+00)\tConsistency Loss 1.7145e+00 (1.7193e+00)\tInconsistency Loss 1.9820e-01 (1.9768e-01)\tEntropy 2.0477e+00 (2.0484e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/107]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7220e+00 (1.7220e+00)\tInconsistency Loss 1.9712e-01 (1.9712e-01)\tEntropy 2.0497e+00 (2.0497e+00)\n",
      "Epoch: [44][100/107]\tTotal Loss -2.3778e+00 (-2.3778e+00)\tConsistency Loss 1.7151e+00 (1.7189e+00)\tInconsistency Loss 1.9913e-01 (1.9779e-01)\tEntropy 2.0464e+00 (2.0484e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/107]\tTotal Loss -2.3791e+00 (-2.3791e+00)\tConsistency Loss 1.7250e+00 (1.7250e+00)\tInconsistency Loss 1.9605e-01 (1.9605e-01)\tEntropy 2.0520e+00 (2.0520e+00)\n",
      "Epoch: [45][100/107]\tTotal Loss -2.3777e+00 (-2.3773e+00)\tConsistency Loss 1.7127e+00 (1.7190e+00)\tInconsistency Loss 1.9920e-01 (1.9768e-01)\tEntropy 2.0452e+00 (2.0481e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/107]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7221e+00 (1.7221e+00)\tInconsistency Loss 1.9698e-01 (1.9698e-01)\tEntropy 2.0496e+00 (2.0496e+00)\n",
      "Epoch: [46][100/107]\tTotal Loss -2.3769e+00 (-2.3774e+00)\tConsistency Loss 1.7209e+00 (1.7190e+00)\tInconsistency Loss 1.9750e-01 (1.9763e-01)\tEntropy 2.0489e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/107]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7171e+00 (1.7171e+00)\tInconsistency Loss 1.9831e-01 (1.9831e-01)\tEntropy 2.0470e+00 (2.0470e+00)\n",
      "Epoch: [47][100/107]\tTotal Loss -2.3768e+00 (-2.3775e+00)\tConsistency Loss 1.7214e+00 (1.7190e+00)\tInconsistency Loss 1.9718e-01 (1.9763e-01)\tEntropy 2.0491e+00 (2.0483e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/107]\tTotal Loss -2.3776e+00 (-2.3776e+00)\tConsistency Loss 1.7090e+00 (1.7090e+00)\tInconsistency Loss 2.0002e-01 (2.0002e-01)\tEntropy 2.0433e+00 (2.0433e+00)\n",
      "Epoch: [48][100/107]\tTotal Loss -2.3740e+00 (-2.3785e+00)\tConsistency Loss 1.7325e+00 (1.7200e+00)\tInconsistency Loss 1.9645e-01 (1.9779e-01)\tEntropy 2.0532e+00 (2.0493e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/107]\tTotal Loss -2.3774e+00 (-2.3774e+00)\tConsistency Loss 1.7209e+00 (1.7209e+00)\tInconsistency Loss 1.9719e-01 (1.9719e-01)\tEntropy 2.0491e+00 (2.0491e+00)\n",
      "Epoch: [49][100/107]\tTotal Loss -2.3723e+00 (-2.3786e+00)\tConsistency Loss 1.6925e+00 (1.7238e+00)\tInconsistency Loss 2.0469e-01 (1.9765e-01)\tEntropy 2.0324e+00 (2.0512e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/107]\tTotal Loss -2.3865e+00 (-2.3865e+00)\tConsistency Loss 1.7365e+00 (1.7365e+00)\tInconsistency Loss 1.9430e-01 (1.9430e-01)\tEntropy 2.0615e+00 (2.0615e+00)\n",
      "Epoch: [50][100/107]\tTotal Loss -2.3704e+00 (-2.3809e+00)\tConsistency Loss 1.6776e+00 (1.7246e+00)\tInconsistency Loss 2.0936e-01 (1.9815e-01)\tEntropy 2.0240e+00 (2.0527e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "D-13\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 1290/7463 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/D-13/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/25]\tLoss 1.0004e+00 (1.0004e+00)\n",
      "Epoch: [1][10/25]\tLoss 3.4866e-01 (6.2383e-01)\n",
      "Epoch: [1][20/25]\tLoss 1.2157e-01 (4.2409e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/25]\tLoss 7.9762e-02 (7.9762e-02)\n",
      "Epoch: [2][10/25]\tLoss 2.7811e-02 (4.9756e-02)\n",
      "Epoch: [2][20/25]\tLoss 1.0000e-02 (3.3840e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [3][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [3][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [4][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [4][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [5][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [6][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [7][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [8][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [8][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [9][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [10][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [11][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [12][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [13][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [15][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [17][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [19][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [20][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [21][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [22][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [24][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [28][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][20/25]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/26]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/150]\n",
      "Fill TS Repository [100/150]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries D-13\u001b[0m\n",
      "\u001b[32m-- Train samples size: 2580 - Test samples size: 7463\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/D-13/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][ 0/51]\tTotal Loss -2.3093e+00 (-2.3093e+00)\tConsistency Loss 2.2511e+00 (2.2511e+00)\tInconsistency Loss 1.1004e-01 (1.1004e-01)\tEntropy 2.2802e+00 (2.2802e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][ 0/51]\tTotal Loss -2.4093e+00 (-2.4093e+00)\tConsistency Loss 1.8168e+00 (1.8168e+00)\tInconsistency Loss 1.7276e-01 (1.7276e-01)\tEntropy 2.1130e+00 (2.1130e+00)\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][ 0/51]\tTotal Loss -2.3642e+00 (-2.3642e+00)\tConsistency Loss 1.7521e+00 (1.7521e+00)\tInconsistency Loss 1.8654e-01 (1.8654e-01)\tEntropy 2.0581e+00 (2.0581e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][ 0/51]\tTotal Loss -2.3845e+00 (-2.3845e+00)\tConsistency Loss 1.7511e+00 (1.7511e+00)\tInconsistency Loss 1.8574e-01 (1.8574e-01)\tEntropy 2.0678e+00 (2.0678e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][ 0/51]\tTotal Loss -2.3697e+00 (-2.3697e+00)\tConsistency Loss 1.6615e+00 (1.6615e+00)\tInconsistency Loss 2.1355e-01 (2.1355e-01)\tEntropy 2.0156e+00 (2.0156e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][ 0/51]\tTotal Loss -2.3822e+00 (-2.3822e+00)\tConsistency Loss 1.7107e+00 (1.7107e+00)\tInconsistency Loss 1.9947e-01 (1.9947e-01)\tEntropy 2.0464e+00 (2.0464e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][ 0/51]\tTotal Loss -2.3752e+00 (-2.3752e+00)\tConsistency Loss 1.7391e+00 (1.7391e+00)\tInconsistency Loss 1.9055e-01 (1.9055e-01)\tEntropy 2.0572e+00 (2.0572e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][ 0/51]\tTotal Loss -2.3810e+00 (-2.3810e+00)\tConsistency Loss 1.7118e+00 (1.7118e+00)\tInconsistency Loss 1.9887e-01 (1.9887e-01)\tEntropy 2.0464e+00 (2.0464e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][ 0/51]\tTotal Loss -2.3758e+00 (-2.3758e+00)\tConsistency Loss 1.6992e+00 (1.6992e+00)\tInconsistency Loss 2.0156e-01 (2.0156e-01)\tEntropy 2.0375e+00 (2.0375e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][ 0/51]\tTotal Loss -2.3701e+00 (-2.3701e+00)\tConsistency Loss 1.7284e+00 (1.7284e+00)\tInconsistency Loss 1.9629e-01 (1.9629e-01)\tEntropy 2.0492e+00 (2.0492e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][ 0/51]\tTotal Loss -2.3725e+00 (-2.3725e+00)\tConsistency Loss 1.7247e+00 (1.7247e+00)\tInconsistency Loss 1.9703e-01 (1.9703e-01)\tEntropy 2.0486e+00 (2.0486e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][ 0/51]\tTotal Loss -2.3760e+00 (-2.3760e+00)\tConsistency Loss 1.7345e+00 (1.7345e+00)\tInconsistency Loss 1.9426e-01 (1.9426e-01)\tEntropy 2.0553e+00 (2.0553e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][ 0/51]\tTotal Loss -2.3786e+00 (-2.3786e+00)\tConsistency Loss 1.7222e+00 (1.7222e+00)\tInconsistency Loss 1.9646e-01 (1.9646e-01)\tEntropy 2.0504e+00 (2.0504e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][ 0/51]\tTotal Loss -2.3776e+00 (-2.3776e+00)\tConsistency Loss 1.7218e+00 (1.7218e+00)\tInconsistency Loss 1.9623e-01 (1.9623e-01)\tEntropy 2.0497e+00 (2.0497e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][ 0/51]\tTotal Loss -2.3780e+00 (-2.3780e+00)\tConsistency Loss 1.7183e+00 (1.7183e+00)\tInconsistency Loss 1.9730e-01 (1.9730e-01)\tEntropy 2.0481e+00 (2.0481e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][ 0/51]\tTotal Loss -2.3780e+00 (-2.3780e+00)\tConsistency Loss 1.7202e+00 (1.7202e+00)\tInconsistency Loss 1.9718e-01 (1.9718e-01)\tEntropy 2.0491e+00 (2.0491e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][ 0/51]\tTotal Loss -2.3766e+00 (-2.3766e+00)\tConsistency Loss 1.7164e+00 (1.7164e+00)\tInconsistency Loss 1.9805e-01 (1.9805e-01)\tEntropy 2.0465e+00 (2.0465e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][ 0/51]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7197e+00 (1.7197e+00)\tInconsistency Loss 1.9742e-01 (1.9742e-01)\tEntropy 2.0484e+00 (2.0484e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][ 0/51]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7175e+00 (1.7175e+00)\tInconsistency Loss 1.9781e-01 (1.9781e-01)\tEntropy 2.0474e+00 (2.0474e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7192e+00 (1.7192e+00)\tInconsistency Loss 1.9751e-01 (1.9751e-01)\tEntropy 2.0482e+00 (2.0482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][ 0/51]\tTotal Loss -2.3774e+00 (-2.3774e+00)\tConsistency Loss 1.7194e+00 (1.7194e+00)\tInconsistency Loss 1.9736e-01 (1.9736e-01)\tEntropy 2.0484e+00 (2.0484e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7183e+00 (1.7183e+00)\tInconsistency Loss 1.9768e-01 (1.9768e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][ 0/51]\tTotal Loss -2.3776e+00 (-2.3776e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9755e-01 (1.9755e-01)\tEntropy 2.0478e+00 (2.0478e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][ 0/51]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7187e+00 (1.7187e+00)\tInconsistency Loss 1.9764e-01 (1.9764e-01)\tEntropy 2.0478e+00 (2.0478e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][ 0/51]\tTotal Loss -2.3771e+00 (-2.3771e+00)\tConsistency Loss 1.7171e+00 (1.7171e+00)\tInconsistency Loss 1.9799e-01 (1.9799e-01)\tEntropy 2.0471e+00 (2.0471e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][ 0/51]\tTotal Loss -2.3769e+00 (-2.3769e+00)\tConsistency Loss 1.7176e+00 (1.7176e+00)\tInconsistency Loss 1.9788e-01 (1.9788e-01)\tEntropy 2.0473e+00 (2.0473e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][ 0/51]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9771e-01 (1.9771e-01)\tEntropy 2.0475e+00 (2.0475e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7182e+00)\tInconsistency Loss 1.9770e-01 (1.9770e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][ 0/51]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "P-2\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2621/8009 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/P-2/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/52]\tLoss 1.0057e+00 (1.0057e+00)\n",
      "Epoch: [1][10/52]\tLoss 3.4840e-01 (6.2440e-01)\n",
      "Epoch: [1][20/52]\tLoss 1.2127e-01 (4.2425e-01)\n",
      "Epoch: [1][30/52]\tLoss 4.2288e-02 (3.1033e-01)\n",
      "Epoch: [1][40/52]\tLoss 1.4683e-02 (2.4068e-01)\n",
      "Epoch: [1][50/52]\tLoss 1.0042e-02 (1.9557e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/52]\tLoss 1.0026e-02 (1.0026e-02)\n",
      "Epoch: [2][10/52]\tLoss 1.0014e-02 (1.0034e-02)\n",
      "Epoch: [2][20/52]\tLoss 1.0043e-02 (1.0031e-02)\n",
      "Epoch: [2][30/52]\tLoss 1.0014e-02 (1.0031e-02)\n",
      "Epoch: [2][40/52]\tLoss 1.0010e-02 (1.0027e-02)\n",
      "Epoch: [2][50/52]\tLoss 1.0006e-02 (1.0024e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/52]\tLoss 1.0027e-02 (1.0027e-02)\n",
      "Epoch: [3][10/52]\tLoss 1.0007e-02 (1.0017e-02)\n",
      "Epoch: [3][20/52]\tLoss 1.0030e-02 (1.0016e-02)\n",
      "Epoch: [3][30/52]\tLoss 1.0003e-02 (1.0015e-02)\n",
      "Epoch: [3][40/52]\tLoss 1.0010e-02 (1.0013e-02)\n",
      "Epoch: [3][50/52]\tLoss 1.0009e-02 (1.0015e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/52]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [4][10/52]\tLoss 1.0005e-02 (1.0011e-02)\n",
      "Epoch: [4][20/52]\tLoss 1.0003e-02 (1.0009e-02)\n",
      "Epoch: [4][30/52]\tLoss 1.0019e-02 (1.0008e-02)\n",
      "Epoch: [4][40/52]\tLoss 1.0007e-02 (1.0008e-02)\n",
      "Epoch: [4][50/52]\tLoss 1.0009e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [5][10/52]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [5][20/52]\tLoss 1.0007e-02 (1.0006e-02)\n",
      "Epoch: [5][30/52]\tLoss 1.0002e-02 (1.0006e-02)\n",
      "Epoch: [5][40/52]\tLoss 1.0003e-02 (1.0006e-02)\n",
      "Epoch: [5][50/52]\tLoss 1.0005e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/52]\tLoss 1.0027e-02 (1.0027e-02)\n",
      "Epoch: [6][10/52]\tLoss 1.0001e-02 (1.0005e-02)\n",
      "Epoch: [6][20/52]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [6][30/52]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [6][40/52]\tLoss 1.0003e-02 (1.0006e-02)\n",
      "Epoch: [6][50/52]\tLoss 1.0003e-02 (1.0006e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/52]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [7][10/52]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [7][20/52]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [7][30/52]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [7][40/52]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [7][50/52]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [8][10/52]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [8][20/52]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][30/52]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][40/52]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [8][50/52]\tLoss 1.0007e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [9][10/52]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [9][20/52]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [9][30/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][40/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][50/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][10/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][20/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][30/52]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [10][40/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][50/52]\tLoss 1.0004e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][10/52]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [11][20/52]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [11][30/52]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [11][40/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][50/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/52]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [12][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][20/52]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [12][30/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][40/52]\tLoss 1.0005e-02 (1.0002e-02)\n",
      "Epoch: [12][50/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/52]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [13][10/52]\tLoss 1.0012e-02 (1.0003e-02)\n",
      "Epoch: [13][20/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][30/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][40/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [13][50/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][10/52]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [14][20/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [14][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][30/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [15][40/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [15][50/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [16][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][30/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [16][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][20/52]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [17][30/52]\tLoss 1.0008e-02 (1.0001e-02)\n",
      "Epoch: [17][40/52]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [17][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [19][40/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [19][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/52]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [20][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][20/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [20][30/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][50/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][20/52]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [23][30/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][10/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][30/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][10/52]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [25][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][10/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [26][20/52]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [26][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][30/52]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [27][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][10/52]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [28][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][30/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/52]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][10/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][20/52]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [29][30/52]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [29][40/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][50/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][10/52]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][20/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][30/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][50/52]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/53]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/161]\n",
      "Fill TS Repository [100/161]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries P-2\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5242 - Test samples size: 8009\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/P-2/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/104]\tTotal Loss -2.3004e+00 (-2.3004e+00)\tConsistency Loss 2.2589e+00 (2.2589e+00)\tInconsistency Loss 1.1038e-01 (1.1038e-01)\tEntropy 2.2796e+00 (2.2796e+00)\n",
      "Epoch: [1][100/104]\tTotal Loss -2.3735e+00 (-2.3623e+00)\tConsistency Loss 1.6627e+00 (1.8182e+00)\tInconsistency Loss 2.0592e-01 (1.8056e-01)\tEntropy 2.0181e+00 (2.0902e+00)\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/104]\tTotal Loss -2.4246e+00 (-2.4246e+00)\tConsistency Loss 1.6465e+00 (1.6465e+00)\tInconsistency Loss 2.0936e-01 (2.0936e-01)\tEntropy 2.0356e+00 (2.0356e+00)\n",
      "Epoch: [2][100/104]\tTotal Loss -2.3796e+00 (-2.3757e+00)\tConsistency Loss 1.6868e+00 (1.7224e+00)\tInconsistency Loss 2.0309e-01 (1.9694e-01)\tEntropy 2.0332e+00 (2.0490e+00)\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/104]\tTotal Loss -2.3780e+00 (-2.3780e+00)\tConsistency Loss 1.6974e+00 (1.6974e+00)\tInconsistency Loss 2.0186e-01 (2.0186e-01)\tEntropy 2.0377e+00 (2.0377e+00)\n",
      "Epoch: [3][100/104]\tTotal Loss -2.3716e+00 (-2.3768e+00)\tConsistency Loss 1.7146e+00 (1.7204e+00)\tInconsistency Loss 1.9960e-01 (1.9730e-01)\tEntropy 2.0431e+00 (2.0486e+00)\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/104]\tTotal Loss -2.3819e+00 (-2.3819e+00)\tConsistency Loss 1.7193e+00 (1.7193e+00)\tInconsistency Loss 1.9589e-01 (1.9589e-01)\tEntropy 2.0506e+00 (2.0506e+00)\n",
      "Epoch: [4][100/104]\tTotal Loss -2.3770e+00 (-2.3767e+00)\tConsistency Loss 1.7192e+00 (1.7193e+00)\tInconsistency Loss 1.9514e-01 (1.9756e-01)\tEntropy 2.0481e+00 (2.0480e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/104]\tTotal Loss -2.3763e+00 (-2.3763e+00)\tConsistency Loss 1.7505e+00 (1.7505e+00)\tInconsistency Loss 1.9054e-01 (1.9054e-01)\tEntropy 2.0634e+00 (2.0634e+00)\n",
      "Epoch: [5][100/104]\tTotal Loss -2.3789e+00 (-2.3767e+00)\tConsistency Loss 1.7093e+00 (1.7203e+00)\tInconsistency Loss 1.9917e-01 (1.9735e-01)\tEntropy 2.0441e+00 (2.0485e+00)\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/104]\tTotal Loss -2.3789e+00 (-2.3789e+00)\tConsistency Loss 1.7085e+00 (1.7085e+00)\tInconsistency Loss 1.9957e-01 (1.9957e-01)\tEntropy 2.0437e+00 (2.0437e+00)\n",
      "Epoch: [6][100/104]\tTotal Loss -2.3755e+00 (-2.3770e+00)\tConsistency Loss 1.7179e+00 (1.7193e+00)\tInconsistency Loss 1.9907e-01 (1.9763e-01)\tEntropy 2.0467e+00 (2.0482e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/104]\tTotal Loss -2.3747e+00 (-2.3747e+00)\tConsistency Loss 1.7130e+00 (1.7130e+00)\tInconsistency Loss 1.9897e-01 (1.9897e-01)\tEntropy 2.0439e+00 (2.0439e+00)\n",
      "Epoch: [7][100/104]\tTotal Loss -2.3827e+00 (-2.3772e+00)\tConsistency Loss 1.7145e+00 (1.7185e+00)\tInconsistency Loss 1.9764e-01 (1.9764e-01)\tEntropy 2.0486e+00 (2.0478e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/104]\tTotal Loss -2.3742e+00 (-2.3742e+00)\tConsistency Loss 1.7253e+00 (1.7253e+00)\tInconsistency Loss 1.9662e-01 (1.9662e-01)\tEntropy 2.0497e+00 (2.0497e+00)\n",
      "Epoch: [8][100/104]\tTotal Loss -2.3769e+00 (-2.3770e+00)\tConsistency Loss 1.7165e+00 (1.7188e+00)\tInconsistency Loss 1.9812e-01 (1.9761e-01)\tEntropy 2.0467e+00 (2.0479e+00)\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/104]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7170e+00 (1.7170e+00)\tInconsistency Loss 1.9799e-01 (1.9799e-01)\tEntropy 2.0470e+00 (2.0470e+00)\n",
      "Epoch: [9][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7177e+00 (1.7182e+00)\tInconsistency Loss 1.9781e-01 (1.9773e-01)\tEntropy 2.0474e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/104]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7173e+00 (1.7173e+00)\tInconsistency Loss 1.9791e-01 (1.9791e-01)\tEntropy 2.0472e+00 (2.0472e+00)\n",
      "Epoch: [10][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7186e+00 (1.7181e+00)\tInconsistency Loss 1.9762e-01 (1.9773e-01)\tEntropy 2.0479e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7185e+00 (1.7185e+00)\tInconsistency Loss 1.9765e-01 (1.9765e-01)\tEntropy 2.0478e+00 (2.0478e+00)\n",
      "Epoch: [11][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9775e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [12][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/104]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "A-8\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 562/8175 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/A-8/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/11]\tLoss 1.0025e+00 (1.0025e+00)\n",
      "Epoch: [1][10/11]\tLoss 3.4808e-01 (6.2447e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/11]\tLoss 3.4820e-01 (3.4820e-01)\n",
      "Epoch: [2][10/11]\tLoss 1.2125e-01 (2.1711e-01)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/11]\tLoss 1.2126e-01 (1.2126e-01)\n",
      "Epoch: [3][10/11]\tLoss 4.2296e-02 (7.5653e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/11]\tLoss 4.2264e-02 (4.2264e-02)\n",
      "Epoch: [4][10/11]\tLoss 1.4728e-02 (2.6356e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/11]\tLoss 1.4743e-02 (1.4743e-02)\n",
      "Epoch: [5][10/11]\tLoss 1.0023e-02 (1.0991e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/11]\tLoss 1.0051e-02 (1.0051e-02)\n",
      "Epoch: [6][10/11]\tLoss 1.0022e-02 (1.0029e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/11]\tLoss 1.0025e-02 (1.0025e-02)\n",
      "Epoch: [7][10/11]\tLoss 1.0012e-02 (1.0019e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/11]\tLoss 1.0018e-02 (1.0018e-02)\n",
      "Epoch: [8][10/11]\tLoss 1.0007e-02 (1.0017e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/11]\tLoss 1.0016e-02 (1.0016e-02)\n",
      "Epoch: [9][10/11]\tLoss 1.0024e-02 (1.0015e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/11]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [10][10/11]\tLoss 1.0016e-02 (1.0013e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/11]\tLoss 1.0014e-02 (1.0014e-02)\n",
      "Epoch: [11][10/11]\tLoss 1.0007e-02 (1.0011e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/11]\tLoss 1.0022e-02 (1.0022e-02)\n",
      "Epoch: [12][10/11]\tLoss 1.0014e-02 (1.0019e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/11]\tLoss 1.0018e-02 (1.0018e-02)\n",
      "Epoch: [13][10/11]\tLoss 1.0043e-02 (1.0014e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/11]\tLoss 1.0012e-02 (1.0012e-02)\n",
      "Epoch: [14][10/11]\tLoss 1.0008e-02 (1.0010e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/11]\tLoss 1.0010e-02 (1.0010e-02)\n",
      "Epoch: [15][10/11]\tLoss 1.0009e-02 (1.0010e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/11]\tLoss 1.0012e-02 (1.0012e-02)\n",
      "Epoch: [16][10/11]\tLoss 1.0014e-02 (1.0012e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/11]\tLoss 1.0010e-02 (1.0010e-02)\n",
      "Epoch: [17][10/11]\tLoss 1.0014e-02 (1.0010e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/11]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [18][10/11]\tLoss 1.0012e-02 (1.0010e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/11]\tLoss 1.0008e-02 (1.0008e-02)\n",
      "Epoch: [19][10/11]\tLoss 1.0012e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/11]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [20][10/11]\tLoss 1.0004e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/11]\tLoss 1.0011e-02 (1.0011e-02)\n",
      "Epoch: [21][10/11]\tLoss 1.0005e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/11]\tLoss 1.0027e-02 (1.0027e-02)\n",
      "Epoch: [22][10/11]\tLoss 1.0013e-02 (1.0011e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/11]\tLoss 1.0016e-02 (1.0016e-02)\n",
      "Epoch: [23][10/11]\tLoss 1.0002e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/11]\tLoss 1.0017e-02 (1.0017e-02)\n",
      "Epoch: [24][10/11]\tLoss 1.0011e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/11]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [25][10/11]\tLoss 1.0007e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/11]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [26][10/11]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/11]\tLoss 1.0005e-02 (1.0005e-02)\n",
      "Epoch: [27][10/11]\tLoss 1.0015e-02 (1.0010e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/11]\tLoss 1.0008e-02 (1.0008e-02)\n",
      "Epoch: [28][10/11]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/11]\tLoss 1.0009e-02 (1.0009e-02)\n",
      "Epoch: [29][10/11]\tLoss 1.0002e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/11]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [30][10/11]\tLoss 1.0008e-02 (1.0008e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/12]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/164]\n",
      "Fill TS Repository [100/164]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries A-8\u001b[0m\n",
      "\u001b[32m-- Train samples size: 1124 - Test samples size: 8175\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/A-8/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][ 0/22]\tTotal Loss -2.3053e+00 (-2.3053e+00)\tConsistency Loss 2.2533e+00 (2.2533e+00)\tInconsistency Loss 1.1036e-01 (1.1036e-01)\tEntropy 2.2793e+00 (2.2793e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][ 0/22]\tTotal Loss -2.5029e+00 (-2.5029e+00)\tConsistency Loss 1.8351e+00 (1.8351e+00)\tInconsistency Loss 8.8321e-02 (8.8321e-02)\tEntropy 2.1690e+00 (2.1690e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][ 0/22]\tTotal Loss -2.4630e+00 (-2.4630e+00)\tConsistency Loss 1.8438e+00 (1.8438e+00)\tInconsistency Loss 7.6725e-02 (7.6725e-02)\tEntropy 2.1534e+00 (2.1534e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][ 0/22]\tTotal Loss -2.5649e+00 (-2.5649e+00)\tConsistency Loss 1.8654e+00 (1.8654e+00)\tInconsistency Loss 6.2983e-02 (6.2983e-02)\tEntropy 2.2151e+00 (2.2151e+00)\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][ 0/22]\tTotal Loss -2.7983e+00 (-2.7983e+00)\tConsistency Loss 1.6080e+00 (1.6080e+00)\tInconsistency Loss 4.3192e-02 (4.3192e-02)\tEntropy 2.2032e+00 (2.2032e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][ 0/22]\tTotal Loss -2.7770e+00 (-2.7770e+00)\tConsistency Loss 1.6135e+00 (1.6135e+00)\tInconsistency Loss 5.4020e-02 (5.4020e-02)\tEntropy 2.1953e+00 (2.1953e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][ 0/22]\tTotal Loss -2.5293e+00 (-2.5293e+00)\tConsistency Loss 1.8845e+00 (1.8845e+00)\tInconsistency Loss 6.2288e-02 (6.2288e-02)\tEntropy 2.2069e+00 (2.2069e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][ 0/22]\tTotal Loss -2.6140e+00 (-2.6140e+00)\tConsistency Loss 1.8225e+00 (1.8225e+00)\tInconsistency Loss 6.4381e-02 (6.4381e-02)\tEntropy 2.2182e+00 (2.2182e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][ 0/22]\tTotal Loss -2.6729e+00 (-2.6729e+00)\tConsistency Loss 1.7328e+00 (1.7328e+00)\tInconsistency Loss 4.7725e-02 (4.7725e-02)\tEntropy 2.2029e+00 (2.2029e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][ 0/22]\tTotal Loss -2.7328e+00 (-2.7328e+00)\tConsistency Loss 1.6448e+00 (1.6448e+00)\tInconsistency Loss 4.7326e-02 (4.7326e-02)\tEntropy 2.1888e+00 (2.1888e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][ 0/22]\tTotal Loss -2.7311e+00 (-2.7311e+00)\tConsistency Loss 1.6633e+00 (1.6633e+00)\tInconsistency Loss 5.6521e-02 (5.6521e-02)\tEntropy 2.1972e+00 (2.1972e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][ 0/22]\tTotal Loss -2.7158e+00 (-2.7158e+00)\tConsistency Loss 1.6712e+00 (1.6712e+00)\tInconsistency Loss 4.8113e-02 (4.8113e-02)\tEntropy 2.1935e+00 (2.1935e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][ 0/22]\tTotal Loss -2.6749e+00 (-2.6749e+00)\tConsistency Loss 1.7873e+00 (1.7873e+00)\tInconsistency Loss 5.6088e-02 (5.6088e-02)\tEntropy 2.2311e+00 (2.2311e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][ 0/22]\tTotal Loss -2.7321e+00 (-2.7321e+00)\tConsistency Loss 1.7602e+00 (1.7602e+00)\tInconsistency Loss 3.9130e-02 (3.9130e-02)\tEntropy 2.2461e+00 (2.2461e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][ 0/22]\tTotal Loss -2.8248e+00 (-2.8248e+00)\tConsistency Loss 1.6758e+00 (1.6758e+00)\tInconsistency Loss 4.4982e-02 (4.4982e-02)\tEntropy 2.2503e+00 (2.2503e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][ 0/22]\tTotal Loss -2.6642e+00 (-2.6642e+00)\tConsistency Loss 1.7544e+00 (1.7544e+00)\tInconsistency Loss 5.5844e-02 (5.5844e-02)\tEntropy 2.2093e+00 (2.2093e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][ 0/22]\tTotal Loss -2.9078e+00 (-2.9078e+00)\tConsistency Loss 1.5590e+00 (1.5590e+00)\tInconsistency Loss 3.7381e-02 (3.7381e-02)\tEntropy 2.2334e+00 (2.2334e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][ 0/22]\tTotal Loss -2.8061e+00 (-2.8061e+00)\tConsistency Loss 1.6528e+00 (1.6528e+00)\tInconsistency Loss 6.0228e-02 (6.0228e-02)\tEntropy 2.2294e+00 (2.2294e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][ 0/22]\tTotal Loss -2.8259e+00 (-2.8259e+00)\tConsistency Loss 1.6576e+00 (1.6576e+00)\tInconsistency Loss 5.4660e-02 (5.4660e-02)\tEntropy 2.2417e+00 (2.2417e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][ 0/22]\tTotal Loss -2.5798e+00 (-2.5798e+00)\tConsistency Loss 1.8085e+00 (1.8085e+00)\tInconsistency Loss 5.8296e-02 (5.8296e-02)\tEntropy 2.1942e+00 (2.1942e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][ 0/22]\tTotal Loss -2.7043e+00 (-2.7043e+00)\tConsistency Loss 1.7599e+00 (1.7599e+00)\tInconsistency Loss 5.8024e-02 (5.8024e-02)\tEntropy 2.2321e+00 (2.2321e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][ 0/22]\tTotal Loss -2.8280e+00 (-2.8280e+00)\tConsistency Loss 1.6188e+00 (1.6188e+00)\tInconsistency Loss 5.0311e-02 (5.0311e-02)\tEntropy 2.2234e+00 (2.2234e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][ 0/22]\tTotal Loss -2.5950e+00 (-2.5950e+00)\tConsistency Loss 1.8628e+00 (1.8628e+00)\tInconsistency Loss 4.9662e-02 (4.9662e-02)\tEntropy 2.2289e+00 (2.2289e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][ 0/22]\tTotal Loss -2.7776e+00 (-2.7776e+00)\tConsistency Loss 1.6790e+00 (1.6790e+00)\tInconsistency Loss 7.5787e-02 (7.5787e-02)\tEntropy 2.2283e+00 (2.2283e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][ 0/22]\tTotal Loss -2.7706e+00 (-2.7706e+00)\tConsistency Loss 1.6492e+00 (1.6492e+00)\tInconsistency Loss 4.1318e-02 (4.1318e-02)\tEntropy 2.2099e+00 (2.2099e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][ 0/22]\tTotal Loss -2.7768e+00 (-2.7768e+00)\tConsistency Loss 1.6799e+00 (1.6799e+00)\tInconsistency Loss 5.7980e-02 (5.7980e-02)\tEntropy 2.2283e+00 (2.2283e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][ 0/22]\tTotal Loss -2.7869e+00 (-2.7869e+00)\tConsistency Loss 1.6529e+00 (1.6529e+00)\tInconsistency Loss 4.3829e-02 (4.3829e-02)\tEntropy 2.2199e+00 (2.2199e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][ 0/22]\tTotal Loss -2.8802e+00 (-2.8802e+00)\tConsistency Loss 1.5519e+00 (1.5519e+00)\tInconsistency Loss 4.6864e-02 (4.6864e-02)\tEntropy 2.2160e+00 (2.2160e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][ 0/22]\tTotal Loss -2.6349e+00 (-2.6349e+00)\tConsistency Loss 1.7891e+00 (1.7891e+00)\tInconsistency Loss 7.5841e-02 (7.5841e-02)\tEntropy 2.2120e+00 (2.2120e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][ 0/22]\tTotal Loss -2.8475e+00 (-2.8475e+00)\tConsistency Loss 1.5877e+00 (1.5877e+00)\tInconsistency Loss 4.4616e-02 (4.4616e-02)\tEntropy 2.2176e+00 (2.2176e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][ 0/22]\tTotal Loss -2.8062e+00 (-2.8062e+00)\tConsistency Loss 1.6766e+00 (1.6766e+00)\tInconsistency Loss 6.2655e-02 (6.2655e-02)\tEntropy 2.2414e+00 (2.2414e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][ 0/22]\tTotal Loss -2.7687e+00 (-2.7687e+00)\tConsistency Loss 1.6724e+00 (1.6724e+00)\tInconsistency Loss 5.4462e-02 (5.4462e-02)\tEntropy 2.2205e+00 (2.2205e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][ 0/22]\tTotal Loss -2.8955e+00 (-2.8955e+00)\tConsistency Loss 1.5912e+00 (1.5912e+00)\tInconsistency Loss 6.6249e-02 (6.6249e-02)\tEntropy 2.2434e+00 (2.2434e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][ 0/22]\tTotal Loss -2.7207e+00 (-2.7207e+00)\tConsistency Loss 1.7454e+00 (1.7454e+00)\tInconsistency Loss 6.2241e-02 (6.2241e-02)\tEntropy 2.2331e+00 (2.2331e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][ 0/22]\tTotal Loss -2.6549e+00 (-2.6549e+00)\tConsistency Loss 1.7823e+00 (1.7823e+00)\tInconsistency Loss 5.2819e-02 (5.2819e-02)\tEntropy 2.2186e+00 (2.2186e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][ 0/22]\tTotal Loss -2.8372e+00 (-2.8372e+00)\tConsistency Loss 1.6122e+00 (1.6122e+00)\tInconsistency Loss 5.3267e-02 (5.3267e-02)\tEntropy 2.2247e+00 (2.2247e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][ 0/22]\tTotal Loss -2.8005e+00 (-2.8005e+00)\tConsistency Loss 1.6548e+00 (1.6548e+00)\tInconsistency Loss 5.7361e-02 (5.7361e-02)\tEntropy 2.2276e+00 (2.2276e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][ 0/22]\tTotal Loss -2.8572e+00 (-2.8572e+00)\tConsistency Loss 1.5385e+00 (1.5385e+00)\tInconsistency Loss 6.5499e-02 (6.5499e-02)\tEntropy 2.1979e+00 (2.1979e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][ 0/22]\tTotal Loss -2.7678e+00 (-2.7678e+00)\tConsistency Loss 1.6619e+00 (1.6619e+00)\tInconsistency Loss 6.4865e-02 (6.4865e-02)\tEntropy 2.2148e+00 (2.2148e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][ 0/22]\tTotal Loss -2.8282e+00 (-2.8282e+00)\tConsistency Loss 1.5790e+00 (1.5790e+00)\tInconsistency Loss 5.9003e-02 (5.9003e-02)\tEntropy 2.2036e+00 (2.2036e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][ 0/22]\tTotal Loss -2.9002e+00 (-2.9002e+00)\tConsistency Loss 1.5053e+00 (1.5053e+00)\tInconsistency Loss 5.9820e-02 (5.9820e-02)\tEntropy 2.2027e+00 (2.2027e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][ 0/22]\tTotal Loss -2.7909e+00 (-2.7909e+00)\tConsistency Loss 1.5828e+00 (1.5828e+00)\tInconsistency Loss 4.7368e-02 (4.7368e-02)\tEntropy 2.1868e+00 (2.1868e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][ 0/22]\tTotal Loss -2.8503e+00 (-2.8503e+00)\tConsistency Loss 1.5635e+00 (1.5635e+00)\tInconsistency Loss 5.8590e-02 (5.8590e-02)\tEntropy 2.2069e+00 (2.2069e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][ 0/22]\tTotal Loss -2.8690e+00 (-2.8690e+00)\tConsistency Loss 1.6060e+00 (1.6060e+00)\tInconsistency Loss 4.7196e-02 (4.7196e-02)\tEntropy 2.2375e+00 (2.2375e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][ 0/22]\tTotal Loss -2.8178e+00 (-2.8178e+00)\tConsistency Loss 1.6786e+00 (1.6786e+00)\tInconsistency Loss 5.2051e-02 (5.2051e-02)\tEntropy 2.2482e+00 (2.2482e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][ 0/22]\tTotal Loss -2.7764e+00 (-2.7764e+00)\tConsistency Loss 1.6607e+00 (1.6607e+00)\tInconsistency Loss 5.5474e-02 (5.5474e-02)\tEntropy 2.2185e+00 (2.2185e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][ 0/22]\tTotal Loss -2.7315e+00 (-2.7315e+00)\tConsistency Loss 1.6552e+00 (1.6552e+00)\tInconsistency Loss 7.3505e-02 (7.3505e-02)\tEntropy 2.1933e+00 (2.1933e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][ 0/22]\tTotal Loss -2.8208e+00 (-2.8208e+00)\tConsistency Loss 1.5878e+00 (1.5878e+00)\tInconsistency Loss 7.6606e-02 (7.6606e-02)\tEntropy 2.2043e+00 (2.2043e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][ 0/22]\tTotal Loss -2.8106e+00 (-2.8106e+00)\tConsistency Loss 1.6158e+00 (1.6158e+00)\tInconsistency Loss 6.9251e-02 (6.9251e-02)\tEntropy 2.2132e+00 (2.2132e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][ 0/22]\tTotal Loss -2.9415e+00 (-2.9415e+00)\tConsistency Loss 1.4926e+00 (1.4926e+00)\tInconsistency Loss 3.3311e-02 (3.3311e-02)\tEntropy 2.2171e+00 (2.2171e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "A-9\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 562/8234 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/A-9/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/11]\tLoss 1.0024e+00 (1.0024e+00)\n",
      "Epoch: [1][10/11]\tLoss 3.4823e-01 (6.2436e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/11]\tLoss 3.4824e-01 (3.4824e-01)\n",
      "Epoch: [2][10/11]\tLoss 1.2131e-01 (2.1719e-01)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/11]\tLoss 1.2132e-01 (1.2132e-01)\n",
      "Epoch: [3][10/11]\tLoss 4.2293e-02 (7.5689e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/11]\tLoss 4.2258e-02 (4.2258e-02)\n",
      "Epoch: [4][10/11]\tLoss 1.4739e-02 (2.6365e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/11]\tLoss 1.4734e-02 (1.4734e-02)\n",
      "Epoch: [5][10/11]\tLoss 1.0033e-02 (1.0980e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/11]\tLoss 1.0043e-02 (1.0043e-02)\n",
      "Epoch: [6][10/11]\tLoss 1.0015e-02 (1.0025e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/11]\tLoss 1.0035e-02 (1.0035e-02)\n",
      "Epoch: [7][10/11]\tLoss 1.0016e-02 (1.0016e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/11]\tLoss 1.0018e-02 (1.0018e-02)\n",
      "Epoch: [8][10/11]\tLoss 1.0008e-02 (1.0015e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/11]\tLoss 1.0010e-02 (1.0010e-02)\n",
      "Epoch: [9][10/11]\tLoss 1.0019e-02 (1.0013e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/11]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [10][10/11]\tLoss 1.0015e-02 (1.0012e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/11]\tLoss 1.0011e-02 (1.0011e-02)\n",
      "Epoch: [11][10/11]\tLoss 1.0000e-02 (1.0011e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/11]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [12][10/11]\tLoss 1.0012e-02 (1.0016e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/11]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [13][10/11]\tLoss 1.0034e-02 (1.0012e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/11]\tLoss 1.0010e-02 (1.0010e-02)\n",
      "Epoch: [14][10/11]\tLoss 1.0009e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/11]\tLoss 1.0008e-02 (1.0008e-02)\n",
      "Epoch: [15][10/11]\tLoss 1.0012e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/11]\tLoss 1.0014e-02 (1.0014e-02)\n",
      "Epoch: [16][10/11]\tLoss 1.0013e-02 (1.0013e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/11]\tLoss 1.0014e-02 (1.0014e-02)\n",
      "Epoch: [17][10/11]\tLoss 1.0011e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/11]\tLoss 1.0010e-02 (1.0010e-02)\n",
      "Epoch: [18][10/11]\tLoss 1.0004e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/11]\tLoss 1.0015e-02 (1.0015e-02)\n",
      "Epoch: [19][10/11]\tLoss 1.0015e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/11]\tLoss 1.0009e-02 (1.0009e-02)\n",
      "Epoch: [20][10/11]\tLoss 1.0007e-02 (1.0010e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/11]\tLoss 1.0011e-02 (1.0011e-02)\n",
      "Epoch: [21][10/11]\tLoss 1.0011e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/11]\tLoss 1.0012e-02 (1.0012e-02)\n",
      "Epoch: [22][10/11]\tLoss 1.0010e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/11]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [23][10/11]\tLoss 1.0005e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/11]\tLoss 1.0014e-02 (1.0014e-02)\n",
      "Epoch: [24][10/11]\tLoss 1.0009e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/11]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [25][10/11]\tLoss 1.0010e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/11]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [26][10/11]\tLoss 1.0005e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/11]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [27][10/11]\tLoss 1.0019e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/11]\tLoss 1.0010e-02 (1.0010e-02)\n",
      "Epoch: [28][10/11]\tLoss 1.0005e-02 (1.0007e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/11]\tLoss 1.0004e-02 (1.0004e-02)\n",
      "Epoch: [29][10/11]\tLoss 1.0003e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/11]\tLoss 1.0006e-02 (1.0006e-02)\n",
      "Epoch: [30][10/11]\tLoss 1.0006e-02 (1.0007e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/12]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/165]\n",
      "Fill TS Repository [100/165]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries A-9\u001b[0m\n",
      "\u001b[32m-- Train samples size: 1124 - Test samples size: 8234\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/A-9/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][ 0/22]\tTotal Loss -2.3017e+00 (-2.3017e+00)\tConsistency Loss 2.2562e+00 (2.2562e+00)\tInconsistency Loss 1.1049e-01 (1.1049e-01)\tEntropy 2.2789e+00 (2.2789e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][ 0/22]\tTotal Loss -2.3604e+00 (-2.3604e+00)\tConsistency Loss 1.9414e+00 (1.9414e+00)\tInconsistency Loss 1.5565e-01 (1.5565e-01)\tEntropy 2.1509e+00 (2.1509e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][ 0/22]\tTotal Loss -2.4004e+00 (-2.4004e+00)\tConsistency Loss 1.7780e+00 (1.7780e+00)\tInconsistency Loss 1.8354e-01 (1.8354e-01)\tEntropy 2.0892e+00 (2.0892e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][ 0/22]\tTotal Loss -2.3570e+00 (-2.3570e+00)\tConsistency Loss 1.7747e+00 (1.7747e+00)\tInconsistency Loss 1.8702e-01 (1.8702e-01)\tEntropy 2.0659e+00 (2.0659e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][ 0/22]\tTotal Loss -2.3775e+00 (-2.3775e+00)\tConsistency Loss 1.6900e+00 (1.6900e+00)\tInconsistency Loss 2.0147e-01 (2.0147e-01)\tEntropy 2.0338e+00 (2.0338e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][ 0/22]\tTotal Loss -2.3893e+00 (-2.3893e+00)\tConsistency Loss 1.6871e+00 (1.6871e+00)\tInconsistency Loss 2.0096e-01 (2.0096e-01)\tEntropy 2.0382e+00 (2.0382e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][ 0/22]\tTotal Loss -2.4095e+00 (-2.4095e+00)\tConsistency Loss 1.7210e+00 (1.7210e+00)\tInconsistency Loss 1.9717e-01 (1.9717e-01)\tEntropy 2.0653e+00 (2.0653e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][ 0/22]\tTotal Loss -2.3683e+00 (-2.3683e+00)\tConsistency Loss 1.6983e+00 (1.6983e+00)\tInconsistency Loss 2.0055e-01 (2.0055e-01)\tEntropy 2.0333e+00 (2.0333e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][ 0/22]\tTotal Loss -2.3483e+00 (-2.3483e+00)\tConsistency Loss 1.7785e+00 (1.7785e+00)\tInconsistency Loss 1.8938e-01 (1.8938e-01)\tEntropy 2.0634e+00 (2.0634e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][ 0/22]\tTotal Loss -2.3533e+00 (-2.3533e+00)\tConsistency Loss 1.7613e+00 (1.7613e+00)\tInconsistency Loss 1.9150e-01 (1.9150e-01)\tEntropy 2.0573e+00 (2.0573e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][ 0/22]\tTotal Loss -2.3596e+00 (-2.3596e+00)\tConsistency Loss 1.7646e+00 (1.7646e+00)\tInconsistency Loss 1.9120e-01 (1.9120e-01)\tEntropy 2.0621e+00 (2.0621e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][ 0/22]\tTotal Loss -2.3618e+00 (-2.3618e+00)\tConsistency Loss 1.6916e+00 (1.6916e+00)\tInconsistency Loss 2.0495e-01 (2.0495e-01)\tEntropy 2.0267e+00 (2.0267e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][ 0/22]\tTotal Loss -2.3883e+00 (-2.3883e+00)\tConsistency Loss 1.7107e+00 (1.7107e+00)\tInconsistency Loss 1.9691e-01 (1.9691e-01)\tEntropy 2.0495e+00 (2.0495e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][ 0/22]\tTotal Loss -2.3775e+00 (-2.3775e+00)\tConsistency Loss 1.6439e+00 (1.6439e+00)\tInconsistency Loss 2.1425e-01 (2.1425e-01)\tEntropy 2.0107e+00 (2.0107e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][ 0/22]\tTotal Loss -2.3677e+00 (-2.3677e+00)\tConsistency Loss 1.6469e+00 (1.6469e+00)\tInconsistency Loss 2.1314e-01 (2.1314e-01)\tEntropy 2.0073e+00 (2.0073e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][ 0/22]\tTotal Loss -2.3894e+00 (-2.3894e+00)\tConsistency Loss 1.6904e+00 (1.6904e+00)\tInconsistency Loss 2.0257e-01 (2.0257e-01)\tEntropy 2.0399e+00 (2.0399e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][ 0/22]\tTotal Loss -2.3785e+00 (-2.3785e+00)\tConsistency Loss 1.7018e+00 (1.7018e+00)\tInconsistency Loss 1.9853e-01 (1.9853e-01)\tEntropy 2.0402e+00 (2.0402e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][ 0/22]\tTotal Loss -2.3807e+00 (-2.3807e+00)\tConsistency Loss 1.6888e+00 (1.6888e+00)\tInconsistency Loss 2.0241e-01 (2.0241e-01)\tEntropy 2.0348e+00 (2.0348e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][ 0/22]\tTotal Loss -2.3739e+00 (-2.3739e+00)\tConsistency Loss 1.7270e+00 (1.7270e+00)\tInconsistency Loss 1.9608e-01 (1.9608e-01)\tEntropy 2.0505e+00 (2.0505e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][ 0/22]\tTotal Loss -2.3662e+00 (-2.3662e+00)\tConsistency Loss 1.7563e+00 (1.7563e+00)\tInconsistency Loss 1.9111e-01 (1.9111e-01)\tEntropy 2.0613e+00 (2.0613e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][ 0/22]\tTotal Loss -2.3809e+00 (-2.3809e+00)\tConsistency Loss 1.7311e+00 (1.7311e+00)\tInconsistency Loss 1.9525e-01 (1.9525e-01)\tEntropy 2.0560e+00 (2.0560e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][ 0/22]\tTotal Loss -2.3795e+00 (-2.3795e+00)\tConsistency Loss 1.7456e+00 (1.7456e+00)\tInconsistency Loss 1.9230e-01 (1.9230e-01)\tEntropy 2.0625e+00 (2.0625e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][ 0/22]\tTotal Loss -2.3730e+00 (-2.3730e+00)\tConsistency Loss 1.7274e+00 (1.7274e+00)\tInconsistency Loss 1.9903e-01 (1.9903e-01)\tEntropy 2.0502e+00 (2.0502e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][ 0/22]\tTotal Loss -2.3724e+00 (-2.3724e+00)\tConsistency Loss 1.7033e+00 (1.7033e+00)\tInconsistency Loss 2.0102e-01 (2.0102e-01)\tEntropy 2.0378e+00 (2.0378e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][ 0/22]\tTotal Loss -2.3775e+00 (-2.3775e+00)\tConsistency Loss 1.7011e+00 (1.7011e+00)\tInconsistency Loss 2.0016e-01 (2.0016e-01)\tEntropy 2.0393e+00 (2.0393e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][ 0/22]\tTotal Loss -2.3803e+00 (-2.3803e+00)\tConsistency Loss 1.7129e+00 (1.7129e+00)\tInconsistency Loss 1.9638e-01 (1.9638e-01)\tEntropy 2.0466e+00 (2.0466e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][ 0/22]\tTotal Loss -2.3765e+00 (-2.3765e+00)\tConsistency Loss 1.7210e+00 (1.7210e+00)\tInconsistency Loss 1.9721e-01 (1.9721e-01)\tEntropy 2.0488e+00 (2.0488e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][ 0/22]\tTotal Loss -2.3749e+00 (-2.3749e+00)\tConsistency Loss 1.7157e+00 (1.7157e+00)\tInconsistency Loss 1.9753e-01 (1.9753e-01)\tEntropy 2.0453e+00 (2.0453e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][ 0/22]\tTotal Loss -2.3757e+00 (-2.3757e+00)\tConsistency Loss 1.7186e+00 (1.7186e+00)\tInconsistency Loss 1.9814e-01 (1.9814e-01)\tEntropy 2.0471e+00 (2.0471e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][ 0/22]\tTotal Loss -2.3797e+00 (-2.3797e+00)\tConsistency Loss 1.7280e+00 (1.7280e+00)\tInconsistency Loss 1.9394e-01 (1.9394e-01)\tEntropy 2.0539e+00 (2.0539e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][ 0/22]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7124e+00 (1.7124e+00)\tInconsistency Loss 1.9831e-01 (1.9831e-01)\tEntropy 2.0448e+00 (2.0448e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][ 0/22]\tTotal Loss -2.3767e+00 (-2.3767e+00)\tConsistency Loss 1.7188e+00 (1.7188e+00)\tInconsistency Loss 1.9763e-01 (1.9763e-01)\tEntropy 2.0477e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][ 0/22]\tTotal Loss -2.3723e+00 (-2.3723e+00)\tConsistency Loss 1.7254e+00 (1.7254e+00)\tInconsistency Loss 1.9697e-01 (1.9697e-01)\tEntropy 2.0489e+00 (2.0489e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][ 0/22]\tTotal Loss -2.3765e+00 (-2.3765e+00)\tConsistency Loss 1.7259e+00 (1.7259e+00)\tInconsistency Loss 1.9696e-01 (1.9696e-01)\tEntropy 2.0512e+00 (2.0512e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][ 0/22]\tTotal Loss -2.3713e+00 (-2.3713e+00)\tConsistency Loss 1.7017e+00 (1.7017e+00)\tInconsistency Loss 2.0151e-01 (2.0151e-01)\tEntropy 2.0365e+00 (2.0365e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][ 0/22]\tTotal Loss -2.3815e+00 (-2.3815e+00)\tConsistency Loss 1.7169e+00 (1.7169e+00)\tInconsistency Loss 1.9783e-01 (1.9783e-01)\tEntropy 2.0492e+00 (2.0492e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][ 0/22]\tTotal Loss -2.3780e+00 (-2.3780e+00)\tConsistency Loss 1.7223e+00 (1.7223e+00)\tInconsistency Loss 1.9626e-01 (1.9626e-01)\tEntropy 2.0501e+00 (2.0501e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][ 0/22]\tTotal Loss -2.3778e+00 (-2.3778e+00)\tConsistency Loss 1.7042e+00 (1.7042e+00)\tInconsistency Loss 2.0038e-01 (2.0038e-01)\tEntropy 2.0410e+00 (2.0410e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][ 0/22]\tTotal Loss -2.3802e+00 (-2.3802e+00)\tConsistency Loss 1.7241e+00 (1.7241e+00)\tInconsistency Loss 1.9588e-01 (1.9588e-01)\tEntropy 2.0522e+00 (2.0522e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][ 0/22]\tTotal Loss -2.3754e+00 (-2.3754e+00)\tConsistency Loss 1.7259e+00 (1.7259e+00)\tInconsistency Loss 1.9608e-01 (1.9608e-01)\tEntropy 2.0507e+00 (2.0507e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][ 0/22]\tTotal Loss -2.3802e+00 (-2.3802e+00)\tConsistency Loss 1.7436e+00 (1.7436e+00)\tInconsistency Loss 1.9147e-01 (1.9147e-01)\tEntropy 2.0619e+00 (2.0619e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][ 0/22]\tTotal Loss -2.3705e+00 (-2.3705e+00)\tConsistency Loss 1.7428e+00 (1.7428e+00)\tInconsistency Loss 1.9255e-01 (1.9255e-01)\tEntropy 2.0566e+00 (2.0566e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][ 0/22]\tTotal Loss -2.3805e+00 (-2.3805e+00)\tConsistency Loss 1.7045e+00 (1.7045e+00)\tInconsistency Loss 2.0054e-01 (2.0054e-01)\tEntropy 2.0425e+00 (2.0425e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][ 0/22]\tTotal Loss -2.3738e+00 (-2.3738e+00)\tConsistency Loss 1.7190e+00 (1.7190e+00)\tInconsistency Loss 1.9763e-01 (1.9763e-01)\tEntropy 2.0464e+00 (2.0464e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][ 0/22]\tTotal Loss -2.3794e+00 (-2.3794e+00)\tConsistency Loss 1.7217e+00 (1.7217e+00)\tInconsistency Loss 1.9742e-01 (1.9742e-01)\tEntropy 2.0506e+00 (2.0506e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][ 0/22]\tTotal Loss -2.3766e+00 (-2.3766e+00)\tConsistency Loss 1.7304e+00 (1.7304e+00)\tInconsistency Loss 1.9448e-01 (1.9448e-01)\tEntropy 2.0535e+00 (2.0535e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][ 0/22]\tTotal Loss -2.3726e+00 (-2.3726e+00)\tConsistency Loss 1.7168e+00 (1.7168e+00)\tInconsistency Loss 1.9829e-01 (1.9829e-01)\tEntropy 2.0447e+00 (2.0447e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][ 0/22]\tTotal Loss -2.3784e+00 (-2.3784e+00)\tConsistency Loss 1.7279e+00 (1.7279e+00)\tInconsistency Loss 1.9550e-01 (1.9550e-01)\tEntropy 2.0531e+00 (2.0531e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][ 0/22]\tTotal Loss -2.3748e+00 (-2.3748e+00)\tConsistency Loss 1.7197e+00 (1.7197e+00)\tInconsistency Loss 1.9754e-01 (1.9754e-01)\tEntropy 2.0473e+00 (2.0473e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][ 0/22]\tTotal Loss -2.3760e+00 (-2.3760e+00)\tConsistency Loss 1.7287e+00 (1.7287e+00)\tInconsistency Loss 1.9596e-01 (1.9596e-01)\tEntropy 2.0523e+00 (2.0523e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "F-3\n",
      "\u001b[33mCARLA Pretext stage --> \u001b[0m\n",
      "Dataset contains 2680/8176 train/val samples\n",
      "\u001b[34mNo checkpoint file at results/SMAP/F-3/pretext/checkpoint.pth.tar\u001b[0m\n",
      "\u001b[33mEpoch 1/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [1][ 0/53]\tLoss 1.0039e+00 (1.0039e+00)\n",
      "Epoch: [1][10/53]\tLoss 3.4824e-01 (6.2439e-01)\n",
      "Epoch: [1][20/53]\tLoss 1.2146e-01 (4.2424e-01)\n",
      "Epoch: [1][30/53]\tLoss 4.2264e-02 (3.1033e-01)\n",
      "Epoch: [1][40/53]\tLoss 1.4811e-02 (2.4068e-01)\n",
      "Epoch: [1][50/53]\tLoss 1.0028e-02 (1.9557e-01)\n",
      "\u001b[33mEpoch 2/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00100\n",
      "Epoch: [2][ 0/53]\tLoss 1.0019e-02 (1.0019e-02)\n",
      "Epoch: [2][10/53]\tLoss 1.0017e-02 (1.0032e-02)\n",
      "Epoch: [2][20/53]\tLoss 1.0048e-02 (1.0029e-02)\n",
      "Epoch: [2][30/53]\tLoss 1.0016e-02 (1.0027e-02)\n",
      "Epoch: [2][40/53]\tLoss 1.0007e-02 (1.0025e-02)\n",
      "Epoch: [2][50/53]\tLoss 1.0006e-02 (1.0025e-02)\n",
      "\u001b[33mEpoch 3/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00099\n",
      "Epoch: [3][ 0/53]\tLoss 1.0008e-02 (1.0008e-02)\n",
      "Epoch: [3][10/53]\tLoss 1.0028e-02 (1.0020e-02)\n",
      "Epoch: [3][20/53]\tLoss 1.0009e-02 (1.0014e-02)\n",
      "Epoch: [3][30/53]\tLoss 1.0008e-02 (1.0012e-02)\n",
      "Epoch: [3][40/53]\tLoss 1.0004e-02 (1.0012e-02)\n",
      "Epoch: [3][50/53]\tLoss 1.0016e-02 (1.0011e-02)\n",
      "\u001b[33mEpoch 4/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00098\n",
      "Epoch: [4][ 0/53]\tLoss 1.0022e-02 (1.0022e-02)\n",
      "Epoch: [4][10/53]\tLoss 1.0003e-02 (1.0009e-02)\n",
      "Epoch: [4][20/53]\tLoss 1.0012e-02 (1.0008e-02)\n",
      "Epoch: [4][30/53]\tLoss 1.0003e-02 (1.0007e-02)\n",
      "Epoch: [4][40/53]\tLoss 1.0001e-02 (1.0007e-02)\n",
      "Epoch: [4][50/53]\tLoss 1.0005e-02 (1.0009e-02)\n",
      "\u001b[33mEpoch 5/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00096\n",
      "Epoch: [5][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [5][10/53]\tLoss 1.0003e-02 (1.0014e-02)\n",
      "Epoch: [5][20/53]\tLoss 1.0003e-02 (1.0009e-02)\n",
      "Epoch: [5][30/53]\tLoss 1.0002e-02 (1.0009e-02)\n",
      "Epoch: [5][40/53]\tLoss 1.0002e-02 (1.0009e-02)\n",
      "Epoch: [5][50/53]\tLoss 1.0003e-02 (1.0008e-02)\n",
      "\u001b[33mEpoch 6/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00093\n",
      "Epoch: [6][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [6][10/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [6][20/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [6][30/53]\tLoss 1.0002e-02 (1.0004e-02)\n",
      "Epoch: [6][40/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [6][50/53]\tLoss 1.0046e-02 (1.0004e-02)\n",
      "\u001b[33mEpoch 7/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00090\n",
      "Epoch: [7][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [7][10/53]\tLoss 1.0001e-02 (1.0003e-02)\n",
      "Epoch: [7][20/53]\tLoss 1.0006e-02 (1.0004e-02)\n",
      "Epoch: [7][30/53]\tLoss 1.0010e-02 (1.0004e-02)\n",
      "Epoch: [7][40/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [7][50/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 8/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00087\n",
      "Epoch: [8][ 0/53]\tLoss 1.0003e-02 (1.0003e-02)\n",
      "Epoch: [8][10/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [8][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [8][30/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [8][40/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [8][50/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "\u001b[33mEpoch 9/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00083\n",
      "Epoch: [9][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [9][10/53]\tLoss 1.0002e-02 (1.0003e-02)\n",
      "Epoch: [9][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [9][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [9][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 10/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00079\n",
      "Epoch: [10][ 0/53]\tLoss 1.0007e-02 (1.0007e-02)\n",
      "Epoch: [10][10/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [10][20/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][30/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][40/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [10][50/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 11/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00075\n",
      "Epoch: [11][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [11][10/53]\tLoss 1.0005e-02 (1.0002e-02)\n",
      "Epoch: [11][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [11][50/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 12/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00070\n",
      "Epoch: [12][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [12][20/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][30/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [12][40/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [12][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 13/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00065\n",
      "Epoch: [13][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [13][10/53]\tLoss 1.0001e-02 (1.0004e-02)\n",
      "Epoch: [13][20/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [13][30/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [13][40/53]\tLoss 1.0003e-02 (1.0002e-02)\n",
      "Epoch: [13][50/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 14/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00060\n",
      "Epoch: [14][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [14][10/53]\tLoss 1.0000e-02 (1.0002e-02)\n",
      "Epoch: [14][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [14][40/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [14][50/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 15/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00055\n",
      "Epoch: [15][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [15][50/53]\tLoss 1.0013e-02 (1.0002e-02)\n",
      "\u001b[33mEpoch 16/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00050\n",
      "Epoch: [16][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [16][20/53]\tLoss 1.0010e-02 (1.0001e-02)\n",
      "Epoch: [16][30/53]\tLoss 1.0001e-02 (1.0002e-02)\n",
      "Epoch: [16][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [16][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 17/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00045\n",
      "Epoch: [17][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [17][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [17][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 18/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00040\n",
      "Epoch: [18][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [18][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [18][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [18][50/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 19/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00035\n",
      "Epoch: [19][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [19][40/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [19][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 20/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00030\n",
      "Epoch: [20][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [20][30/53]\tLoss 1.0011e-02 (1.0001e-02)\n",
      "Epoch: [20][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [20][50/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 21/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00025\n",
      "Epoch: [21][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [21][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [21][40/53]\tLoss 1.0004e-02 (1.0001e-02)\n",
      "Epoch: [21][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 22/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00021\n",
      "Epoch: [22][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [22][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [22][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 23/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00017\n",
      "Epoch: [23][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [23][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [23][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [23][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [23][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 24/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00013\n",
      "Epoch: [24][ 0/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [24][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [24][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 25/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00010\n",
      "Epoch: [25][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [25][10/53]\tLoss 1.0001e-02 (1.0000e-02)\n",
      "Epoch: [25][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [25][30/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [25][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [25][50/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 26/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00007\n",
      "Epoch: [26][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [26][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [26][30/53]\tLoss 1.0002e-02 (1.0001e-02)\n",
      "Epoch: [26][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [26][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 27/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00004\n",
      "Epoch: [27][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][10/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [27][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][30/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [27][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [27][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 28/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00002\n",
      "Epoch: [28][ 0/53]\tLoss 1.0002e-02 (1.0002e-02)\n",
      "Epoch: [28][10/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [28][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [28][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 29/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00001\n",
      "Epoch: [29][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [29][10/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][20/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [29][40/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [29][50/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "\u001b[33mEpoch 30/30\u001b[0m\n",
      "\u001b[33m---------------\u001b[0m\n",
      "Adjusted learning rate to 0.00000\n",
      "Epoch: [30][ 0/53]\tLoss 1.0000e-02 (1.0000e-02)\n",
      "Epoch: [30][10/53]\tLoss 1.0003e-02 (1.0001e-02)\n",
      "Epoch: [30][20/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "Epoch: [30][30/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][40/53]\tLoss 1.0001e-02 (1.0001e-02)\n",
      "Epoch: [30][50/53]\tLoss 1.0000e-02 (1.0001e-02)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (train) ...\u001b[0m\n",
      "Fill TS Repository [0/54]\n",
      "Mine the nearest neighbors (Top-10)\n",
      "\u001b[34mFill TS Repository for mining the nearest/furthest neighbors (val) ...\u001b[0m\n",
      "Fill TS Repository [0/164]\n",
      "Fill TS Repository [100/164]\n",
      "Mine the nearest and furthest neighbors (Top-10)\n",
      "\u001b[33mCARLA Self-supervised Classification stage --> \u001b[0m\n",
      "\u001b[32m\n",
      "- Get dataset and dataloaders for SMAP dataset - timeseries F-3\u001b[0m\n",
      "\u001b[32m-- Train samples size: 5360 - Test samples size: 8176\u001b[0m\n",
      "\u001b[32m\n",
      "- Model initialisation\u001b[0m\n",
      "\u001b[32m-- No checkpoint file at results/SMAP/F-3/classification/checkpoint.pth.tar -- new model initialised\u001b[0m\n",
      "\u001b[34m\n",
      "- Training:\u001b[0m\n",
      "\u001b[34m-- Epoch 1/50\u001b[0m\n",
      "Epoch: [1][  0/107]\tTotal Loss -2.3039e+00 (-2.3039e+00)\tConsistency Loss 2.2561e+00 (2.2561e+00)\tInconsistency Loss 1.1046e-01 (1.1046e-01)\tEntropy 2.2800e+00 (2.2800e+00)\n",
      "Epoch: [1][100/107]\tTotal Loss -2.3774e+00 (-2.3594e+00)\tConsistency Loss 1.7409e+00 (1.8447e+00)\tInconsistency Loss 1.9345e-01 (1.7691e-01)\tEntropy 2.0591e+00 (2.1021e+00)\n",
      "\u001b[34m-- Epoch 2/50\u001b[0m\n",
      "Epoch: [2][  0/107]\tTotal Loss -2.3713e+00 (-2.3713e+00)\tConsistency Loss 1.7022e+00 (1.7022e+00)\tInconsistency Loss 2.0021e-01 (2.0021e-01)\tEntropy 2.0367e+00 (2.0367e+00)\n",
      "Epoch: [2][100/107]\tTotal Loss -2.3749e+00 (-2.3760e+00)\tConsistency Loss 1.6992e+00 (1.7219e+00)\tInconsistency Loss 2.0136e-01 (1.9703e-01)\tEntropy 2.0370e+00 (2.0489e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 3/50\u001b[0m\n",
      "Epoch: [3][  0/107]\tTotal Loss -2.3837e+00 (-2.3837e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9841e-01 (1.9841e-01)\tEntropy 2.0508e+00 (2.0508e+00)\n",
      "Epoch: [3][100/107]\tTotal Loss -2.3877e+00 (-2.3779e+00)\tConsistency Loss 1.7173e+00 (1.7166e+00)\tInconsistency Loss 1.9736e-01 (1.9816e-01)\tEntropy 2.0525e+00 (2.0473e+00)\n",
      "\u001b[34m-- Epoch 4/50\u001b[0m\n",
      "Epoch: [4][  0/107]\tTotal Loss -2.3757e+00 (-2.3757e+00)\tConsistency Loss 1.7169e+00 (1.7169e+00)\tInconsistency Loss 1.9730e-01 (1.9730e-01)\tEntropy 2.0463e+00 (2.0463e+00)\n",
      "Epoch: [4][100/107]\tTotal Loss -2.3755e+00 (-2.3763e+00)\tConsistency Loss 1.7235e+00 (1.7210e+00)\tInconsistency Loss 1.9566e-01 (1.9712e-01)\tEntropy 2.0495e+00 (2.0487e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 5/50\u001b[0m\n",
      "Epoch: [5][  0/107]\tTotal Loss -2.3763e+00 (-2.3763e+00)\tConsistency Loss 1.7218e+00 (1.7218e+00)\tInconsistency Loss 1.9771e-01 (1.9771e-01)\tEntropy 2.0490e+00 (2.0490e+00)\n",
      "Epoch: [5][100/107]\tTotal Loss -2.3773e+00 (-2.3771e+00)\tConsistency Loss 1.7311e+00 (1.7188e+00)\tInconsistency Loss 1.9512e-01 (1.9767e-01)\tEntropy 2.0542e+00 (2.0480e+00)\n",
      "\u001b[34m-- Epoch 6/50\u001b[0m\n",
      "Epoch: [6][  0/107]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7195e+00 (1.7195e+00)\tInconsistency Loss 1.9779e-01 (1.9779e-01)\tEntropy 2.0482e+00 (2.0482e+00)\n",
      "Epoch: [6][100/107]\tTotal Loss -2.3781e+00 (-2.3769e+00)\tConsistency Loss 1.7161e+00 (1.7189e+00)\tInconsistency Loss 1.9817e-01 (1.9757e-01)\tEntropy 2.0471e+00 (2.0479e+00)\n",
      "\u001b[34m-- Epoch 7/50\u001b[0m\n",
      "Epoch: [7][  0/107]\tTotal Loss -2.3768e+00 (-2.3768e+00)\tConsistency Loss 1.7201e+00 (1.7201e+00)\tInconsistency Loss 1.9744e-01 (1.9744e-01)\tEntropy 2.0484e+00 (2.0484e+00)\n",
      "Epoch: [7][100/107]\tTotal Loss -2.3774e+00 (-2.3771e+00)\tConsistency Loss 1.7206e+00 (1.7184e+00)\tInconsistency Loss 1.9720e-01 (1.9767e-01)\tEntropy 2.0490e+00 (2.0478e+00)\n",
      "\u001b[34m-- Epoch 8/50\u001b[0m\n",
      "Epoch: [8][  0/107]\tTotal Loss -2.3773e+00 (-2.3773e+00)\tConsistency Loss 1.7167e+00 (1.7167e+00)\tInconsistency Loss 1.9798e-01 (1.9798e-01)\tEntropy 2.0470e+00 (2.0470e+00)\n",
      "Epoch: [8][100/107]\tTotal Loss -2.3772e+00 (-2.3771e+00)\tConsistency Loss 1.7199e+00 (1.7183e+00)\tInconsistency Loss 1.9733e-01 (1.9771e-01)\tEntropy 2.0486e+00 (2.0477e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fs03/hz18/zahraz/Published/CARLA/utils/evaluate_utils.py:188: RuntimeWarning: invalid value encountered in divide\n",
      "  f1_score = 2*precision*recall / (precision+recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: Nan --> 0\n",
      "\u001b[34m-- Epoch 9/50\u001b[0m\n",
      "Epoch: [9][  0/107]\tTotal Loss -2.3770e+00 (-2.3770e+00)\tConsistency Loss 1.7173e+00 (1.7173e+00)\tInconsistency Loss 1.9795e-01 (1.9795e-01)\tEntropy 2.0472e+00 (2.0472e+00)\n",
      "Epoch: [9][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7182e+00 (1.7181e+00)\tInconsistency Loss 1.9761e-01 (1.9774e-01)\tEntropy 2.0477e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 10/50\u001b[0m\n",
      "Epoch: [10][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7187e+00 (1.7187e+00)\tInconsistency Loss 1.9757e-01 (1.9757e-01)\tEntropy 2.0480e+00 (2.0480e+00)\n",
      "Epoch: [10][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7181e+00 (1.7181e+00)\tInconsistency Loss 1.9772e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 11/50\u001b[0m\n",
      "Epoch: [11][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9775e-01 (1.9775e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [11][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 12/50\u001b[0m\n",
      "Epoch: [12][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [12][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 13/50\u001b[0m\n",
      "Epoch: [13][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [13][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 14/50\u001b[0m\n",
      "Epoch: [14][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [14][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 15/50\u001b[0m\n",
      "Epoch: [15][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [15][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 16/50\u001b[0m\n",
      "Epoch: [16][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [16][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 17/50\u001b[0m\n",
      "Epoch: [17][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [17][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 18/50\u001b[0m\n",
      "Epoch: [18][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [18][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 19/50\u001b[0m\n",
      "Epoch: [19][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [19][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 20/50\u001b[0m\n",
      "Epoch: [20][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [20][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 21/50\u001b[0m\n",
      "Epoch: [21][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [21][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 22/50\u001b[0m\n",
      "Epoch: [22][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [22][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 23/50\u001b[0m\n",
      "Epoch: [23][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [23][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 24/50\u001b[0m\n",
      "Epoch: [24][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [24][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 25/50\u001b[0m\n",
      "Epoch: [25][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [25][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 26/50\u001b[0m\n",
      "Epoch: [26][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [26][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 27/50\u001b[0m\n",
      "Epoch: [27][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [27][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 28/50\u001b[0m\n",
      "Epoch: [28][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [28][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 29/50\u001b[0m\n",
      "Epoch: [29][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [29][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 30/50\u001b[0m\n",
      "Epoch: [30][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [30][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 31/50\u001b[0m\n",
      "Epoch: [31][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [31][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 32/50\u001b[0m\n",
      "Epoch: [32][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [32][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 33/50\u001b[0m\n",
      "Epoch: [33][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [33][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 34/50\u001b[0m\n",
      "Epoch: [34][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [34][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 35/50\u001b[0m\n",
      "Epoch: [35][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [35][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 36/50\u001b[0m\n",
      "Epoch: [36][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [36][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 37/50\u001b[0m\n",
      "Epoch: [37][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [37][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 38/50\u001b[0m\n",
      "Epoch: [38][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [38][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 39/50\u001b[0m\n",
      "Epoch: [39][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [39][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 40/50\u001b[0m\n",
      "Epoch: [40][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [40][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 41/50\u001b[0m\n",
      "Epoch: [41][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [41][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 42/50\u001b[0m\n",
      "Epoch: [42][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [42][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 43/50\u001b[0m\n",
      "Epoch: [43][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [43][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 44/50\u001b[0m\n",
      "Epoch: [44][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [44][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 45/50\u001b[0m\n",
      "Epoch: [45][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [45][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 46/50\u001b[0m\n",
      "Epoch: [46][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [46][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 47/50\u001b[0m\n",
      "Epoch: [47][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [47][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 48/50\u001b[0m\n",
      "Epoch: [48][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [48][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 49/50\u001b[0m\n",
      "Epoch: [49][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [49][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "\u001b[34m-- Epoch 50/50\u001b[0m\n",
      "Epoch: [50][  0/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n",
      "Epoch: [50][100/107]\tTotal Loss -2.3772e+00 (-2.3772e+00)\tConsistency Loss 1.7180e+00 (1.7180e+00)\tInconsistency Loss 1.9774e-01 (1.9774e-01)\tEntropy 2.0476e+00 (2.0476e+00)\n"
     ]
    }
   ],
   "source": [
    "all_files = os.listdir(os.path.join('/home/zahraz/hz18_scratch/zahraz/datasets', 'MSL_SMAP/train'))\n",
    "with open('/home/zahraz/hz18_scratch/zahraz/datasets/MSL_SMAP/labeled_anomalies.csv', 'r') as file:\n",
    "    csv_reader = pd.read_csv(file, delimiter=',')\n",
    "data_info = csv_reader[csv_reader['spacecraft'] == 'SMAP']\n",
    "file_list = np.asanyarray(data_info['chan_id'])\n",
    "\n",
    "# index = np.flatnonzero(file_list == \"D-3\")[0]\n",
    "\n",
    "for filename in file_list: #['GECCO']: #file_list: #[index:]:\n",
    "# for filename in data_info['chan_id']:\n",
    "    print(filename)\n",
    "    %run -i 'carla_pretext.py' --config_env configs/env.yml --config_exp configs/pretext/carla_pretext_smap.yml --fname $filename\n",
    "    %run -i 'carla_classification.py' --config_env configs/env.yml --config_exp configs/classification/carla_classification_smap.yml --fname $filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55b860d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_files = os.listdir(os.path.join('../../datasets', 'SMD/train'))\n",
    "# files = [file for file in all_files if file.startswith('machine-')]\n",
    "# files = sorted(files)\n",
    "# print(files)\n",
    "# # exclude_files = ['machine-1-1.txt', 'machine-1-2.txt', 'machine-1-3.txt', 'machine-1-4.txt', \n",
    "# #                  'machine-1-5.txt', 'machine-1-6.txt', 'machine-1-7.txt']\n",
    "# # files_to_process = [f for f in files if f not in exclude_files]\n",
    "\n",
    "# index = files.index(\"machine-3-10.txt\")\n",
    "# # print(index)\n",
    "# for filename in files[index:]:\n",
    "#     %run -i 'carla_pretext.py' --config_env configs/env.yml --config_exp configs/pretext/carla_pretext_smd.yml --fname $filename\n",
    "#     %run -i 'carla_classification.py' --config_env configs/env.yml --config_exp configs/classification/carla_classification_smd.yml --fname $filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88cb0ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filename in ['gecco']: #['swan']: #['swat']: #files:#[index+1:]:\n",
    "#     %run -i 'carla_pretext.py' --config_env configs/env.yml --config_exp configs/pretext/carla_pretext_gecco.yml --fname $filename\n",
    "#     %run -i 'carla_classification.py' --config_env configs/env.yml --config_exp configs/classification/carla_classification_gecco.yml --fname $filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e6cfff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_list = os.listdir(os.path.join('/home/zahraz/hz18_scratch/zahraz/datasets/KPI/', 'train'))                        \n",
    "# file_list = sorted(file_list)\n",
    "# # index = np.flatnonzero(np.array(file_list) == 'e0747cad-8dc8-38a9-a9ab-855b61f5551d.txt')[0]\n",
    "\n",
    "# for filename in file_list[index:]:\n",
    "#     # if 'real_' in filename:\n",
    "#         print(filename)\n",
    "#         %run -i 'carla_pretext.py' --config_env configs/env.yml --config_exp configs/pretext/carla_pretext_kpi.yml --fname $filename\n",
    "#         %run -i 'carla_classification.py' --config_env configs/env.yml --config_exp configs/classification/carla_classification_kpi.yml --fname $filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "532c694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_list = os.listdir(os.path.join('/home/zahraz/hz18_scratch/zahraz/datasets/', 'UCR'))                        \n",
    "# file_list = sorted(file_list)\n",
    "# index = np.flatnonzero(np.array(file_list) == '226_UCR_Anomaly_mit14046longtermecg_96123_123000_123300.txt')[0]\n",
    "\n",
    "# for filename in file_list[index:]:\n",
    "#     print(filename)\n",
    "#     %run -i 'carla_pretext.py' --config_env configs/env.yml --config_exp configs/pretext/carla_pretext_ucr.yml --fname $filename\n",
    "#     %run -i 'carla_classification.py' --config_env configs/env.yml --config_exp configs/classification/carla_classification_ucr.yml --fname $filename"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsenv",
   "language": "python",
   "name": "tsenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
